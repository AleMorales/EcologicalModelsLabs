---
title: "Lab 7  Optimisation and all that (including Bayesian approaches)"
author: "Ben Bolker, Bob Douma and Alejandro Morales. Bayesian part by AM"
date: "16 November 2022"
cache: true
visual: false
params:
  solution: false
filters:
  - custom-numbered-blocks
custom-numbered-blocks:
  groups:
    question:
      collapse: false
      colors: [00ff00, d9d9d9]
      label: "Exercise 6."
      boxstyle: foldbox.simple
    answer:
      collapse: false
      colors: [80bfff, d9d9d9]
      label: "Solution"
      boxstyle: foldbox.simple
  classes:
    Exercise:
      group: question
    Solution:
      group: answer
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Learning goals

You will learn how to:

1. Deal with optimization problems and assess confidence limits

2. Estimate parameters in a Bayesian framework and how parameter uncertainty can be assessed


## Optimisation problems and assessing the confidence limits of parameter estimates

Fitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.

If you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question 5.2 and practise a bit more.

This exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.

1. Take the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function $a(1-e^{(-bx)}$. Fit this model with normally distributed errors through this data with `mle2` and optim method `Nelder-Mead`. Choose four different starting points of the optimisation: `start_a = c(5,10,20,30)`, `start_b = c(0.001,0.005,0.01,0.1)` and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the $\sigma$ to 3.


2. To understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of $a$ and $b$. For $a$ choose a number of parameter values in the range of 0-40 and for $b$ choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using `contour` plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use `points()` for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for $b$ in the range of 0.001-0.03 and check again the NLL surface.

    _hint_: See Bolker Lab 6 for inspiration on coding.

    _hint_: You can use a for a double for-loop to run over all parameters

    _hint_: Store the NLL results in a matrix (you can make a 100x100 matrix by `matrix(NA,nrow=100,ncol=100)`).


3.  Calculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:
    a. Adapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter $a$.
    b. Vary $a$ of a range and optimise the other parameteters.
    c. Plot the NLL as a function of parameter $a$.
    d. Find the values of $a$ that enclose $-L + \chi^2(1-\alpha)/2$. In `R` this can be done through `qchisq(0.95,1)/2`.
    e. Compare your results with the results from the `R` function `confint()`. `confint()` uses the profiling method along with interpolation methods.

4. *(time permitting)* Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:
    a. Get the standard error of the parameter estimates through `vcov`. Note that `vcov` return the variance/covariance matrix
    b. Calculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.

5. *(time permitting)* Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?

6. To assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps
    a. Simulate a number of parameter values taken the uncertainty in the parameter estimates into account.

          _hint_: If the fitted mle object is called `mle2.obj`, then you can extract the variance-covariance matrix by using `vcov(mle2.obj)`. You can extract the mean parameter estimates by using `coef(mle2.obj)`. Now you are ready to simulate 1000 combinations of parameter values through `z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj))`. `mvrnorm` is a function to randomly draw values from a multivariate normal distribution.

    b. Predict the mean response based on the simulated parameter values and the values of $x$

          _hint_: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus `mu = z[i,1]*(1-exp(-z[i,2]*x))`

    c. Draw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.

          _hint_: `pred = rnorm(length(mu),mean=mu,sd=z[i,3])`. Store pred in a matrix with each simulated dataset in a seperate row.

    d. Calculate for each value of $x$ the 2.5% and the 97.5% quantiles

          _hint_: If the predictions are stored in a matrix `mat`, you can use `apply(mat,2,quantile,0.975)` to get the upper limit.


```{block2, echo = params$solution}
::: Solution

The solution is given below in the big chunk of code

        shapes1= read.csv("shapes1.csv")
        plot(shapes1)
        nll.mle = function(a,b,sd){
          # this calculates the mean y for a given value of x: the deterministic function
          mu = a*(1-exp(-b*shapes1$x))
          # this calculates the likelihood of the function given the probability
          # distribution, the data and mu and sd
          nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))
          return(nll)
        }
        library(bbmle)
        # Try 4 different starting points
        mle2.1 = vector("list", 4)
        start_a = c(5,10,20,30)
        start_b = c(0.001,0.005,0.01,0.1)
        for(i in 1:4) {
          mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method="Nelder-Mead")
        }
        # Check the best fit (in this case it is 3rd starting point)
        for(i in 1:4) {
          print(logLik(mle2.1[[i]]))
        }
        # Extract the best fit for the rest of the analysis
        best_mle2.1 = mle2.1[[3]]
        summary(best_mle2.1)
        logLik(best_mle2.1)
        confint(best_mle2.1)
        coef(best_mle2.1)
        plot(shapes1)
        curve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)
        curve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)
        # likelihood surface
        a1 = seq(0,40,length.out = 100)
        b1.1 = seq(0.001,0.03,length.out=100)
        b1.2 = seq(0.1,10,length.out=100)
        nll.grid = expand.grid(a1,b1.1)
        nll.grid$NLL = NA
        no = 0
        # Construct first contour
        for (i in 1:length(a1)){
          for (j in 1:length(b1.1)){
            no = no + 1
            nll.grid[no,1] = a1[i]
            nll.grid[no,2] = b1.1[j]
            nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)
          }
        }
        library(reshape2)
        z1.1 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])
        # Construct second contour
        no = 0
        for (i in 1:length(a1)){
          for (j in 1:length(b1.2)){
            no = no + 1
            nll.grid[no,1] = a1[i]
            nll.grid[no,2] = b1.2[j]
            nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)
          }
        }
        z1.2 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])
        # Plot the two contours
        par(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)
        contour(a1,b1.2,z1.2,nlevels = 20, xaxt = "n", yaxt = "n", ylim = c(0,9))
        axis(2, seq(1,9,2))
        points(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)
        points(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)
        points(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)
        points(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)
        contour(a1,b1.2,z1.2,levels=120,col=2,add=T)
        par(mar = c(3.5,4,0.5,1))
        contour(a1,b1.1,z1.1,nlevels = 20)
        points(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)
        points(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)
        points(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)
        points(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)
        contour(a1,b1.1,z1.1,levels=120,col=2,add=T)
        # profile
        nll.mle1 = function(a,sd){
          # this calculates the mean y for a given value of x: the deterministic function
          mu = a*(1-exp(-b*x))
          # this calculates the likelihood of the function given the probability
          # distribution, the data and mu and sd
          nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
          return(nll)
        }
        nll = numeric(length(b1.1))
        for (i in 1:length(b1.1)){
          b = b1.1[i]
          mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead")
          nll[i] = -logLik(mle.21)
        }
        par(mfrow = c(1,1))
        plot(nll~ b1.1,type="l",xlim=c(0.008,0.012), ylim = c(117,125))
        which.min(nll)
        # cutoff
        -logLik(best_mle2.1) + qchisq(0.95, 1)/2
        which(nll < 119.852)
        b1.1[c(23,35)]
        plot(nll~ b1.1,type="l",xlim=c(0.0070,0.012),ylim=c(116,125))
        abline(v=c(0.00744,0.01096),lty=2)
        abline(v=0.008968,lty=1,lwd=2)
        abline(v=c(0.00738,0.01103),lty=2,col="red")
        se.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])
        b + c(-1,1)*qnorm(0.975) * se.mu
        confint(best_mle2.1)
        abline(v=c(0.007177,0.0107589),col="blue")

:::
```



## Bayesian parameter estimation: negative binomial

In this section we will practice parameter estimation using the Bayesian method on the same negative binomial example as before (Section 6.1). The purpose of this exercise is to gain intuition of how Markov Chain Monte Carlo (MCMC) algorithms work and better understand the differences (and similarities) between maximum likelihood and Bayesian parameter estimation. The MCMC algorithm implemented below can be useful for relatively simple models such as the ones covered in this course. For more complex data analysis we recommend to use dedicated R packages that implement more powerful (and automated) algorithms. A list of such packages can be found in the task view on Bayesian Inference (https://cran.r-project.org/web/views/Bayesian.html).

### From Bayes rule to log posterior

The aim of Bayesian analysis is to estimate the parameters of a model conditional on observed data ($P(\theta | D)$, known as *posterior distribution*) given the likelihood ($L(\theta|D) = P(D|\theta)$) and *prior distributions* of the parameters ($P(\theta)$), according to Bayes rule:

$$
P(\theta | D) = \frac{P(D|\theta) P(\theta)}{P(D)}
$$

Details on Bayes rule are given in section 4.3 and 6.2.2 of the book. Note that the only unknown in the right hand side of Bayes rule is $P(D)$. However, we know that $P(D) = \int P(D|\theta)P(\theta)d\theta$. Therefore, in order to calculate the posterior distribution, we could calculate this integral. Any integration method would work, but integration will not be feasible for a large number of parameters. In practice, a more popular approach is to generate samples from the posterior distribution, while avoiding the integral. This is achieved by so-called Markov Chain Monte Carlo (MCMC) algorithms. These algorithms will provide a random sample from the posterior distribution given a formulation of the problem as:

$$
\log(P(\theta | D)) \propto \log(P(D|\theta)) + \log(P(\theta)) = \mathcal{L} + \log(P(\theta))
$$

Where $\mathcal{L}$ is the positive log-likelihood and $\propto$ means "proportional to". These algorithms work with logarithms for the same reason as in maximum likelihood estimation (i.e., to avoid numerical instability due to very large or very small numbers that would result from multiplication).

The first step of Bayesian parameter estimation is to build a function that calculates the log-posterior density for every parameter value. We will use the example of the negative binomial from section 6.1. This example fits a negative binomial distribution parameterized by its mean (`mu`) and size (`k`) both of which have to be positive.

In a Bayesian approach, we need to assign prior probabilities to each of the parameters, which means choosing a distribution and its parameters, based on prior knowledge. Of course, without a context, it is not possible to specify meaningful prior distributions (and this is arguably the hardest step in any Bayesian analysis), but for the sake of this exercise let's assume that we can represent our prior beliefs with Normal distributions centered around 0 and with a standard deviation of 2 (in practice only half of these prior distributions are being used as `mu` and `k` are positive, but that is fine). This essentially means that were are 99% certain that `mu` and `k` will be lower than 4.6, prior to seeing any data.

We have to construct a function that can return the sum of the log-likelihood and log-prior densities for a given combination of `mu` and `k` in order to use MCMC (remember, this is not the exact log-posterior because of the unknown normalizing constant):

```{r,results='hide'}
LPfun1 = function(p, dat = x) {
  # Mean and size of the negative binomial (use exp to force them to be positive)
  mu = exp(p[1])
  k  = exp(p[2])
  # Logarithm of the prior distributions on mu and k
  # (0 and 2 are parameters chosen by the user, they represent prior beliefs)
  lp_mu = dnorm(mu, 0, 2, log = TRUE)
  lp_k = dnorm(k, 0, 2, log = TRUE)
  log_prior = lp_mu + lp_k
  # Log-likelihood of the data under the model
  LL = sum(dnbinom(dat,mu=mu,size=k,log=TRUE))
  # Sum of the log-likelihood and the log-prior
  LL + log_prior
}
```

The main difference between `LPfun1` and `NLLfun1` created in section 6.1 is that the new function includes the log-prior densities of `mu` and `k` (`lp_mu` and `lp_k`, respectively) and that it returns the sum of log-likelihood + log-prior densities.

### Sampling from posterior: Metropolis-Hastings algorithm

Below is a simple version of the Metropolis-Hastings algorithm (section 7.3.1 of the book), with a multivariate Normal proposal distribution (**you need to install package `mvtnorm` first!**). Note that this function is written in a generic fashion, that is, it will work with any user-defined function that is assigned to the first argument (`model`) and any data required by said function is passed through the `...` argument (this is the strategy is used in many R functions, including `optim`).

The inputs of the `MH` function (see below for code) are:

- `model`: Function that calculates the non-normalized log-posterior (i.e. `LPfun1`).
- `init`: Initial values for the parameters. The closer to the "true" values the faster the MCMC algorithm will converge to the posterior distribution.
- `sigma`: Variance-covariance matrix of the proposal distribution used to calculate jumps in parameter space.
- `niter`: Number of iterations the algorithm will run for.
- `burn`: Fraction of iterations that will be used as burn-in (check section 7.3.2). These iterations will not be used for analysis but are required for convergence of the MCMC algorithm.
- `seed`: Seed for pseudo-random number generator that allows reproducing results.

The algorithm keeps track of all the parameter values it visits and stores them in the variable `chain`. Each iteration, it proposes new values for each parameter (`proposal`) sampled from a multivariate Normal distribution centered at the current values. The probability of accepting the proposal is equal to the exponent of the difference in log posterior densities (`paccept`, see Equation 7.3.2 in the book, taking into account that the proposal distribution is symmetric). If the proposal is accepted, then it is added to the `chain` and becomes the new `current` values (i.e., the algorithm "moves" to that location). After the run is finished, the values in `chain` are split between the burn-in samples and after burn-in. The variable `acceptance` calculates the fraction of jumps that were accepted (do not confuse with the probability of accepting an individual jump!). The higher this number is, the more efficient the algorithm is in exploring the posterior distribution.

```{r}
library(mvtnorm)
MH = function(model, init, Sigma = diag(init/10), niter = 3e4, burn = 0.5,
                      seed = 1134, ...) {
  # To make results reproducible you should set a seed (change among chains!!!)
  set.seed(seed)
  # Pre-allocate chain of values
  chain = matrix(NA, ncol = length(init), nrow = niter)
  # Chain starts at init
  current = init
  lp_current = model(current, ...)
  # Iterate niter times and update chain
  for(iter in 1:niter) {
    # Generate proposal values from multivariate Normal distribution
    proposal = rmvnorm(1, mean = current, sigma = Sigma)
    # Calculate probability of acceptance (proposal distribution is symmetric)
    lp_proposal = model(proposal, ...)
    paccept = min(1, exp(lp_proposal - lp_current))
    # Accept the proposal... or not!
    # If accept, update the current and lp_current values
    accept = runif(1) < paccept
    if(accept) {
      chain[iter,] = proposal
      lp_current = lp_proposal
      current = chain[iter,]
    } else {
      chain[iter,] = current
    }
  }
  # Calculate the length of burn-in
  nburn = floor(niter*burn)
  # Calculate final acceptance probability after burn-in (fraction of proposals accepted)
  acceptance = 1 - mean(duplicated(chain[-(1:nburn),]))
  # Package the results
  list(burnin = chain[1:nburn,], sample = chain[-(1:nburn),],
       acceptance = acceptance, nburn = nburn)
}
```

So let's tackle the negative binomial problem with the algorithm above. First, let's regenerate the data that was used in the previous section:

```{r,results='hide'}
set.seed(1001)
mu.true=1
k.true=0.4
x = rnbinom(50,mu=mu.true,size=k.true)
```

Now we can run `MH` with some values. I want to make the point that choosing a good proposal distribution matters for an efficient MCMC algorithm. So let's start with a variance-covariance matrix that is not reasonable (because it is too wide):

```{r}
Sigma = diag(c(10,10))
```

Now we can run `MH` combined with the `LPfun1` function and some initial values:

```{r}
init = log(c(1,1))
bay1 = MH(LPfun1, init, Sigma, burn = 0.3, dat = x)
```

The first result you want to check is the acceptance probability to see how succesful proposals were:

```{r}
bay1$acceptance
```

This is terrible! 98% of the proposed values were rejected so it would take really long to get a representative sample from the posterior distribution. The next step is usually to take a look at the traces of the values sampled by the MCMC (noticed that the sampling was done on the log transformation of the parameters as they are positive):

```{r, fig.height=3.5, fig.width=5, fig.align='c'}
par(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)
plot(bay1$sample[,1], t = "l", ylab = "Trace of log(mu)")
plot(bay1$sample[,2], t = "l", ylab = "Trace of log(k)")
```

The low probability of acceptance means that the traces look like "squiggly lines", getting stuck at different values for hundreds of iterations (i.e., horizontal sections in the traces). This slows down the effective sampling and can introduce biases in the estimates (unless the algorithm runs for very long).

Note that more modern MCMC algorithms (that R packages specialized on Bayesian statistics will use internally) will automatically *tune* the proposal distribution or even use alternatives methods to propose values that are more robust. However a poor man's tuned MCMC may suffice for this introduction (and for simple models with few parameters) and it works as follows:

1. Calculate the value that maximizes the posterior distribution using `optim` (*a.k.a* Maximum A Posteriori estimate or MAP for short).

2. Estimate the variance-covariance matrix of the posterior distribution using the Hessian matrix returned by `optim` (analogous to what we do for maximum likelihood estimation).

3. Run `MH` using the above results as the values for `init` and `sigma`, respectively.

The reason why this works better is because points 1 and 2 will often give a good first approximation of the posterior distribution, especially for large data (in which case the posterior distribution approaches a Normal distribution). This means that `MH` will be sampling from a distribution similar to the target distribution and hence a higher proportion of proposals will be accepted (intuitively, fewer values that are far in the tails of the posterior distribution will be proposed). This approach can be implemented as:

```{r}
mapfit = optim(fn = LPfun1, par = log(c(1,1)),
               hessian = TRUE, method = "BFGS",
               control = list(fnscale = -1), dat = x)
Sigma = solve(-mapfit$hessian)
init = mapfit$par
bay2 = MH(LPfun1, init, Sigma, burn = 0.3)
```

Notice that we should use `control = list(fnscale = -1)` because we want to maximize the posterior probability, not minimize it. That is also the reason why I add a negative sign in front of the Hessian as in `solve(-mapfit$hessian)` (in previous examples we were minimizing the negative log likelihood and we did not include the negative sign in front of the Hessian).

We can see that the matrix `Sigma` obtained from the Hessian around the MAP estimate is different from the one assumed in the first MH run (specifically, the variances are much smaller):

```{r}
Sigma
```

These lower variances mean that the Markov chain does not wonder far into the tails of the posterior distribution but rather remains in the area of high probability. Thus, the new run has a higher acceptance probability:

```{r}
bay2$acceptance
```

Now this is the good. 56% of the time the candidates will be accepted, ensuring that the chain samples efficiently from the posterior. The traces will approach white noise (these are often called "fuzzy caterpillars" in the community):

```{r, fig.height=3.5, fig.width=5, fig.align='c'}
par(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)
plot(bay2$sample[,1], t = "l", ylab = "Trace of log(mu)")
plot(bay2$sample[,2], t = "l", ylab = "Trace of log(k)")
```

At this point we would normally calculate more diagnostics to build up more confidence on the results of the MCMC chains, but we will keep it simple in this introduction. The object `bay2$sample` contains a random sample from the posterior from which we can calculate several properties. First, remember that we took the logarithm of the parameters to avoid negative values, so we need to undo this transformation in the result:

```{r}
bay2sample = exp(bay2$sample)
```

We can visualize the estimates for each parameter using `density` (more common) or `hist` (easier to interpret):

```{r, fig.height=3}
par(mfrow = c(1,2), mar = c(4,4,1.5,1))
hist(bay2sample[,1], main = "Density of mu", freq = F, xlim = c(0,4))
hist(bay2sample[,2], main = "Density of k", freq = F, xlim = c(0,1))
```

One striking feature is that the distributions are not symmetric, they have a longer tail to the right. This is typical of positive parameters that are close to 0. A consequence of this is that the mean, median and mode of the distributions will differ (though in in this case not so much). Let's compare all the estimates we have so far for the negative binomial fitted to these data:

```{r}
map = exp(mapfit$par)
meanp = colMeans(bay2sample)
medianp = c(median(bay2sample[,1]), median(bay2sample[,2]))
cbind(map, meanp, medianp,
      mom = c(mu.mom, k.mom),
      mle = opt1$par,
      true = c(mu.true, k.true))
```

For this model, data and priors, all estimates are quite similar to each other across different methods of estimation. The reason is because there is sufficient data (50 points for 2 parameters is quite some data...) such that the priors have a negligible effect.

The 95% credible intervals (analogous to 95% confidence intervals) can be calculated with the `quantile` function applied directly to the sample from the posterior:

```{r}
t(apply(bay2sample, 2, quantile, probs = c(0.025, 0.975)))
```

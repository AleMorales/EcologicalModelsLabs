# Learning goals

You will learn how to:

1. Deal with optimization problems and assess confidence limits

2. Otimally, estimate parameters in a Bayesian way using Stan


## Optimisation problems and assessing the confidence limits of parameter estimates

Fitting a model to data requires you to specify a relationship between variables. After
specifying this relationship we need to fit parameters of this model that best fits the
data. This fitting is done through computer algorithms (optimizers). However, sometimes it
may be hard to fit a model to data. After having found the best fitting model, you want to
assess how certain you are about the parameter estimates. For assessing the uncertainty of
model parameters several methods exist that have pros and cons.

If you feel comfortable with fitting models to data you are ready for a more challenging
exercise. If you do not feel comfortable yet, go back to question the previous lab and
practise a bit more.

:::::: {.callout-important title="Exercise" collapse="true"}

This exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.

1. Take the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function $a(1-e^{(-bx)}$. Fit this model with normally distributed errors through this data with `mle2` and method `Nelder-Mead`. Choose four different starting points of the optimisation: `start_a = c(5,10,20,30)`, `start_b = c(0.001,0.005,0.01,0.1)` and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the $\sigma$ to 3.


2. To understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of $a$ and $b$. For $a$ choose a number of parameter values in the range of 0-40 and for $b$ choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using `contour` plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use `points()` for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for $b$ in the range of 0.001-0.03 and check again the NLL surface.

    _hint_: See Bolker Lab 6 for inspiration on coding.

    _hint_: You can use a for a double for-loop to run over all parameters

    _hint_: Store the NLL results in a matrix (you can make a 100x100 matrix by `matrix(NA,nrow=100,ncol=100)`).


3.  Calculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:
    a. Adapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter $a$.
    b. Vary $a$ of a range and optimise the other parameteters.
    c. Plot the NLL as a function of parameter $a$.
    d. Find the values of $a$ that enclose $-L + \chi^2(1-\alpha)/2$. In `R` this can be done through `qchisq(0.95,1)/2`.
    e. Compare your results with the results from the `R` function `confint()`. `confint()` uses the profiling method along with interpolation methods.

4. *(time permitting)* Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:
    a. Get the standard error of the parameter estimates through `vcov`. Note that `vcov` return the variance/covariance matrix
    b. Calculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.

5. *(time permitting)* Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?

6. To assess the uncertainty in the predictions from the model you can construct population
   prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the
   interval in which a new observation will likely fall. To construct the PPI take the
   following steps

    a. Simulate a number of parameter values taken the uncertainty in the parameter estimates into account.

          _hint_: If the fitted mle object is called `mle2.obj`, then you can extract the variance-covariance matrix by using `vcov(mle2.obj)`. You can extract the mean parameter estimates by using `coef(mle2.obj)`. Now you are ready to simulate 1000 combinations of parameter values through `z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj))`. `mvrnorm` is a function to randomly draw values from a multivariate normal distribution.

    b. Predict the mean response based on the simulated parameter values and the values of $x$

          _hint_: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus `mu = z[i,1]*(1-exp(-z[i,2]*x))`

    c. Draw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.

          _hint_: `pred = rnorm(length(mu),mean=mu,sd=z[i,3])`. Store pred in a matrix with each simulated dataset in a seperate row.

    d. Calculate for each value of $x$ the 2.5% and the 97.5% quantiles

          _hint_: If the predictions are stored in a matrix `mat`, you can use `apply(mat,2,quantile,0.975)` to get the upper limit.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

The solution is given below in the big chunk of code

```r
shapes1= read.csv("shapes1.csv")
plot(shapes1)
nll.mle = function(a,b,sd){
    # this calculates the mean y for a given value of x: the deterministic function
    mu = a*(1-exp(-b*shapes1$x))
    # this calculates the likelihood of the function given the probability
    # distribution, the data and mu and sd
    nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))
    return(nll)
}
library(bbmle)
# Try 4 different starting points
mle2.1 = vector("list", 4)
start_a = c(5,10,20,30)
start_b = c(0.001,0.005,0.01,0.1)
for(i in 1:4) {
    mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method="Nelder-Mead")
}
# Check the best fit (in this case it is 3rd starting point)
for(i in 1:4) {
    print(logLik(mle2.1[[i]]))
}
# Extract the best fit for the rest of the analysis
best_mle2.1 = mle2.1[[3]]
summary(best_mle2.1)
logLik(best_mle2.1)
confint(best_mle2.1)
coef(best_mle2.1)
plot(shapes1)
curve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)
curve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)
# likelihood surface
a1 = seq(0,40,length.out = 100)
b1.1 = seq(0.001,0.03,length.out=100)
b1.2 = seq(0.1,10,length.out=100)
nll.grid = expand.grid(a1,b1.1)
nll.grid$NLL = NA
no = 0
# Construct first contour
for (i in 1:length(a1)){
    for (j in 1:length(b1.1)){
    no = no + 1
    nll.grid[no,1] = a1[i]
    nll.grid[no,2] = b1.1[j]
    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)
    }
}
library(reshape2)
z1.1 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])
# Construct second contour
no = 0
for (i in 1:length(a1)){
    for (j in 1:length(b1.2)){
    no = no + 1
    nll.grid[no,1] = a1[i]
    nll.grid[no,2] = b1.2[j]
    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)
    }
}
z1.2 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])
# Plot the two contours
par(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)
contour(a1,b1.2,z1.2,nlevels = 20, xaxt = "n", yaxt = "n", ylim = c(0,9))
axis(2, seq(1,9,2))
points(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)
points(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)
points(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)
points(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)
contour(a1,b1.2,z1.2,levels=120,col=2,add=T)
par(mar = c(3.5,4,0.5,1))
contour(a1,b1.1,z1.1,nlevels = 20)
points(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)
points(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)
points(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)
points(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)
contour(a1,b1.1,z1.1,levels=120,col=2,add=T)
# profile
nll.mle1 = function(a,sd){
    # this calculates the mean y for a given value of x: the deterministic function
    mu = a*(1-exp(-b*x))
    # this calculates the likelihood of the function given the probability
    # distribution, the data and mu and sd
    nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
    return(nll)
}
nll = numeric(length(b1.1))
for (i in 1:length(b1.1)){
    b = b1.1[i]
    mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead")
    nll[i] = -logLik(mle.21)
}
par(mfrow = c(1,1))
plot(nll~ b1.1,type="l",xlim=c(0.008,0.012), ylim = c(117,125))
which.min(nll)
# cutoff
-logLik(best_mle2.1) + qchisq(0.95, 1)/2
which(nll < 119.852)
b1.1[c(23,35)]
plot(nll~ b1.1,type="l",xlim=c(0.0070,0.012),ylim=c(116,125))
abline(v=c(0.00744,0.01096),lty=2)
abline(v=0.008968,lty=1,lwd=2)
abline(v=c(0.00738,0.01103),lty=2,col="red")
se.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])
b + c(-1,1)*qnorm(0.975) * se.mu
confint(best_mle2.1)
abline(v=c(0.007177,0.0107589),col="blue")
```

::::

:::::

::::::


## Bayesian parameter estimation: Hamiltonian Monte Carlo

In this section we will revisit the examples we did in Lab 6 but using a 
Markov Chain Monte Carlo approach to generate samples from the posterior
distribution without having to make assumptions about its shape. 

The software Stan implements a different type of MCMC algorithm to the one
described in Bolker's book. It uses an algorithm called Hamiltonian Monte Carlo
that has a smart way to generate candidate points in the posterior distribution
that lead to a much more efficient sampling (less autocorrelation, much faster
adaptation, etc). We will not learn the details of the algorithm (is much more
complex that basic Metropolis, an easy-ish introduction is here: https://www.youtube.com/watch?v=ZGtezhDaSpM), 
instead we will focus on how to assess the results.

Remember that in lab 6 we created a model for the prediation data (`shapes2`). 
We will modify that model to keep track of the log-likelihood for the different
parameter combinations (we need it for model comparison later). The original code
was as follows

```stan
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector<lower=0>[N] y;
  vector[6] hp;
}


parameters {
  real<lower=0> a;
  real<lower=0> b;
  real<lower=0> sigma;
}


model {
  a     ~ normal(hp[1], hp[2]);
  b     ~ normal(hp[3], hp[4]);
  sigma ~ normal(hp[5], hp[6]);
  y ~ normal(a*x./(b + x), sigma);
}
```

You need to add an extra block that computes the log-likelihood for each 
parameter value:

```r
generated quantities {
  vector[N] log_lik;
  for(n in N) log_lik[n] = normal_lpdf(y[n] | a*x[n]/(b + x[n]), sigma);
}
```

This can be added at the end of the file with the model implementation. Notice
that inside the `generated quantities` block you can only use the `normal_lpdf`
style of programming (not the `y ~ normal` style).



We could compile and load the model as follows:

```r
library(rstan)
mm_model = stan_model(file = "./Lab7/mm_ll.stan")
```

Now, instead of applying Laplace's approximation we will generate the samples
using HMC. We can achieve this with the function `sampling` (assuming you
have loaded the `shapes2` data:

```r
data = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, 
            hp = c(50, 50, 50, 100, 0, 10)) # see above for priors
options(mc.cores = 4) # Adjust to the number of cores you have
mm_fit = sampling(mm_model, data = data, cores = 4, chains = 4,
                   warmup = 1000, iter = 4000)
```

In this called to `sampling()` we are running 4 chains in parallel `ncores = 4`
and `nchains = 4`. Each chain will first run for 1000 iterations to adapt (*warmup*
is equivalent to *burn-in*) and then 3000 more to fill up the `iter = 4000`. This
means we get in total 12000 samples from posterior across 4 independent chains.

We can look at summaries of the samples with the usual function. You should
specify the parameters you want out (otherwise you get all the `log_lik` 
estimates printed:

```r
summary(mm_fit, pars = c("a", "b", "sigma"))[[1]]
```
Stan is calculate the mean of each marginal posterior distribution (one per 
parameter). It also gives several quantiles, an 
estimate of the effective sample size (corrected for autocorrelation) and the
$\hat{R}$ (`Rhat`) statistic (Bolker calls it *Gelman-Rubin* diagnostic). This
statistic should be as close to 1 as possible, with values about 1.2 indicating
a quite bad convergence (the results we get here are pretty much perfect).

When you get an $\hat{R}$ of 1.00, there is no much need to look at the traces
of the chains, but if you want to you can still do it:

```r
traceplot(mm_fit, pars = c("a", "b", "sigma"))
```

We can see that the four chains are well mixed for all parameters and they look
like white noise. If you want to process the samples your self, you can convert
the Stan object into a data.frame

```r
posterior_samples = as.data.frame(mm_fit, pars = c("a", "b", "sigma"))
```

And now we can reproduce the plots we made in the previous Lab

```r
par(mfrow = c(1,3))
hist(posterior_samples[,"a"], main = "", xlab = "a", prob = T)
hist(posterior_samples[,"b"], main = "", xlab = "b", prob = T)
plot(posterior_samples[,1:2], xlab = "a", ylab = "b")
```

Note that the uncertainty in the parameter `b` is much larger than what we 
estimated with Laplace's approximation. This is indeed the problem of using
quadratic approximation to estimate uncertainty. It does not matter whether
we are using in a Bayesian context or for maximum likelihood, in both cases
we often underestimate uncertainty.

### Modern Bayesian information criteria

Finally, we can compute information criteria for model comparison. Bolker mentions
DIC but this criterion has by now been superseded by two more recent approaches.
The first one is WAIC which is a Bayesian generalization of AIC. The other one
is an estimate of leave-one-out cross-validation (that can be done in a Bayesian
context and is much faster than actual cross-validation). The package `loo`
implements these indices:

```r
library(loo)
# Extract the log-likelihoods from the fit
log_lik = extract_log_lik(mm_fit)
# Compute the WAIC
waic_mm = waic(log_lik)
# Compute the loo IC
loo_mm = loo(log_lik)
```

Notice that there were some warnings, they refer to one data point (2% of the
data) that is probably a bit of an outlier and this can have an influence on 
any information criterion. We can identify as follows:

```r
suspect = pareto_k_ids(loo_mm)
plot(shapes2)
points(shapes2[suspect,], col = 2)
```

Indeed, that value is way above the rest of the curve (for that value of `x`).
We can remove the column in `log_lik` that corresponds to this point and 
recalculate the indices:

```r
log_lik_clean = log_lik[,-7]
waic_mm = waic(log_lik_clean)
loo_mm = loo(log_lik_clean)
```

And now complaints anymore. Let's look at these objects

```r
waic_mm
loo_mm
```

They give exactly the same result. Sometimes they will not and in that case the
recommendation is to use `loo` instead of `waic`. This will tend to happen with
fewer datapoints and/or more complex models. There are a lot of numbers there:

- `elpd_` is the average predictive log-likelihood (so it is already penalized by model complexity).

- `p_` is the effective degrees of freedom we would need to compute `elpd` from our sample. In AIC this would $2k = 6$. The reason why this number is smaller than 6 in a Bayesian prior is because prior information constrains the model, so the effective number of degrees of freedom is lower than the number of parameters being estimated. The effective degrees of freedom will also depend on the sample size (the more data we have, the less informative the priors become).

- `waic` and `looic` is just -2 times elpd so that it is expressed in the same units as AIC or BIC.

You can use WAIC or LOO-IC in a similar way to how you use AIC or BIC. Notice that we also get a standard error `SE` for the index, so you may also use that information for model comparison (i.e., if the WAIC or two models differ by more than twice the SE, you can assume a difference in predictive power between the models).

:::::: {.callout-important title="Exercise" collapse="true"}

1. Fit the model `a*x^2/(b + x^2)` to the same dataset as the model above using
Hamiltonian Monte Carlo.

2. Compare the two models using WAIC or LOO-IC

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

The stan model will look as follows (I use the probabilistic flavour because 
it is a bit easier to write down)

```stan
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector<lower=0>[N] y;
  vector[6] hp;
}

parameters {
  real<lower=0> a;
  real<lower=0> b;
  real<lower=0> sigma;
}

model {
  a     ~ normal(hp[1], hp[2]);
  b     ~ normal(hp[3], hp[4]);
  sigma ~ normal(hp[5], hp[6]);
  y ~ normal(a*pow(x, 2)./(b + pow(x, 2)), sigma);
}

generated quantities {
  vector[N] log_lik;
  for(n in 1:N) 
    log_lik[n] = normal_lpdf(y[n] | a*pow(x[n],2)/(b + pow(x[n],2)), sigma);
}
```

We then load the new model and run HMC on it:

```r
mm2_model = stan_model(file = "./Lab7/mm2_ll.stan")
data = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, 
            hp = c(50, 50, 100, 200, 0, 10)) # see above for priors
mm2_fit = sampling(mm2_model, data = data, cores = 4, chains = 4,
                   warmup = 1000, iter = 4000)
```

And I can retrieve the parameter values as I did before

```r
summary(mm2_fit, pars = c("a", "b", "sigma"))[[1]]
```

And visualize them:

```r
posterior_samples = as.data.frame(mm2_fit, pars = c("a", "b", "sigma"))
par(mfrow = c(1,3))
hist(posterior_samples[,"a"], main = "", xlab = "a", prob = T)
hist(posterior_samples[,"b"], main = "", xlab = "b", prob = T)
plot(posterior_samples[,1:2], xlab = "a", ylab = "b")
```

To compare with the other model we should compute cross-validation also without
data point 7:

```r
library(loo)
# Extract the log-likelihoods from the fit
log_lik = extract_log_lik(mm2_fit)[,-7]
# Compute the WAIC
waic_mm2 = waic(log_lik)
waic_mm2
```
 Ok, there seems to be another suspect, let's check with `loo`

```r
loo_mm2 = loo(log_lik)
loo_mm2
```
We do not get warnings so we should be safe. We can now compare the two models.

```r
rbind(loo_mm$estimates["looic",],
      loo_mm2$estimates["looic",])
```

We can see that the first model we tried was much better than the second model
even when accounting for the standard error in the estimation. We can use the
`loo_compare` function, which compares `elpd` values (i.e., without multiplying
by -2, so bigger is better):

```r
loo_compare(loo_mm, loo_mm2)
```

::::

:::::

::::::

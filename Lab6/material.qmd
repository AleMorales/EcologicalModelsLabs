# Learning goals

You will learn how to:

1. Program the likelihood function of a model.

2. Estimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.

3. Estimate the confidence intervals of the model parameters through profiling and the quadratic approximation.

4. Perform stochastic simulations from the fitted models

5 (Optionally) Estimate parameters and uncertainty using Laplace's approximation to Bayes rule


# Fitting models to data

Fitting a model to data through likelihood requires that you take five steps:

1. Specify how the dependent variable depends on the independent variable, i.e. specify a function that describes how the mean of y depends on the value of x.
2. Specify a probability distribution to describe the deviations of the observations from the mean
3. Specify a function that calculate the negative log likelihood (NLL) based on the data and the parameter values.
4. Choose the parameters of the deterministic model and the probability model such that the negative log likelihood is lowest.
5. Compare the likelihood of alternative models (change the deterministic function or the stochastic function) and compare with AIC(c) or BIC which model is most parsimonious.


For example to calculate the NLL of a linear model and a normal distribution the following function works:

```r
nll = function(a, b, sd, y, x){
  # this calculates the mean y for a given value of x:
  #the deterministic function
  mu = a + b*x
  # this calculates the likelihood of the function given the probability
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y, mean = mu, sd = sd, log = TRUE))
  return(nll)
}
```

Notice that the function takes three arguments: a vector with parameters, a vector with `y` values and a vector with `x` values. Inside the vector par, three values are stored: `a`,`b` and `sd`. Next, the mean given x is calculated with `mu=a+b*x`. The nll returns the Negative LogLikelihood of the data (`y`) given a normal distribution with mean `mu` (vector!) and a standard deviation `sd`. The `log=TRUE` returns the log of the probability densities.

Next we call an optimisation function to find the maximum likelihood estimate

```r
# Generate some fake data
x = 1:10
mu = 0.5 + 0.2*x
y = rnorm(10, mean = mu, sd = 0.2)

# Initial guess of parameter values
par = list(a = 1, b = 1, sd = 1)


# Always test the function first
nll(a = par$a, b = par$b, sd = par$sd, x = x, y = y)

# y represents the data, x the independent variable
library(bbmle)
opt1 = mle2(start = par, minuslogl = nll, data = data.frame(x = x, y = y))
```

The optimization result is a list with elements:

  - The best-fit parameters (`coef(opt1)`, with parameter names because we named the
  elements of the starting vector---see how useful this is?);}

  - The maximum log-likelihood and degrees of freedom of the model (`logLik(opt1)`);

  - Details on the optimization (`opt1@details`) including number of function and gradient evaluations, the convergence flag (should be 0 unless there were issues, in which case a message will be included) and the Hessian matrix if it was calculated


# Fitting parameters of made-up data

The simplest thing to do to convince yourself that your attempts to estimate parameters are working is to simulate the ''data'' yourself and see if you get close to the right answers back. Set the random seed to 1001 so we get identical answers across r sessions.

## Finding the maximum likelihood estimate of the paramaters

:::::: {.callout-important title="Exercise" collapse="true"}

Take the steps below

1. Generate 50 values from a negative binomial (`rnbinom`) with $\mu=1$, $k=0.4$. Save the values in variables in case we want to use them again later.

2. Plot the numbers in a frequency diagram

3. Next, define the negative log-likelihood function for a simple draw from a negative binomial distribution: first include the parameters of the model and then the variables in the data that are used in the model (see example above)

4. Calculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Remember that combine these parameter values using a `list()` and to name them so you can recognize them later on.

5. Calculate the NLL of parameter values that are far from the values that were used to generate the data (e.g. $\mu=10$, $k=10$)

6. Calculate the maximum likelihood estimate (MLE)?  Use `mle2` with the default options and use the method-of-moments estimates as the starting estimates (`par`):
  `opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom))`

7. What is the difference in NLL between the MLE estimates and the NLL derived at 5? Does it make sense?

8. Perform a likelihood ratio test at 95% confidence compare the values obtained in 5 and 7.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

First we generate a random sample assuming certain true values

```r
set.seed(1001)
mu.true = 1
k.true = 0.4
x = rnbinom(50,mu=mu.true,size=k.true)

plot(table(factor(x,levels=0:max(x))),
     ylab = "Frequency", xlab = "x")
```

We can now create a function that implements the negative log-likelihood of the
model:

```r
NLLfun1 = function(mu, k, x) {
  -sum(dnbinom(x, mu = mu, size = k, log = TRUE))
}
```

We test it with the true values and with $\mu=10$, $k=10$

```r
nll.true = NLLfun1(mu = mu.true, k = k.true, x = x)
nll.far  = NLLfun1(mu = 10,      k = 10,     x = x)
c(nll.true, nll.far)
```

We calculate the estimates based on method of moments

```r
m = mean(x)
v = var(x)
mu.mom = m
k.mom  = m/(v/m-1)
```

We now perform the optimization using these estimates as initial values

```r
opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom), data = data.frame(x = x))
coef(opt1)
```

We can now compare the different NLL values we got so far:

```r
c(MLE = -logLik(opt1), true = nll.true, far = nll.far)
```

The minimum negative log-likelihood is better than the NLL of the model with 
the true parameters. This does not mean that the maximum likelihood estimates 
are *truer* than the actual true values, but is a consequence of sampling error
(i.e., relative small samples will not match exactly the population in its
properties). 

The likelihood ratio test uses the concept of *deviance* which is equal to twice
the difference in NLL (between simpler and more complex):

```r
deviance = 2*(nll.true - -logLik(opt1))
deviance
```

We now compare this value to a $\chi^2$ distribution with n degrees of freedom.
To compute the p-value we should use the quantile function `pchisq`

```r
pchisq(deviance, df = 2)
```
The p-value of 0.57 indicates that we cannot reject the hypothesis that there
is a differnece between the true model and the MLE model. 

::::

:::::

::::::



# Maximum likelihood and continuous covariates

The following exercise has the purpose to learn you how to fit a model to data when we have a single covariate.

:::::: {.callout-important title="Exercise" collapse="true"}

1. Take the second dataset (shapes2.csv from shapes.xlsx), use a michaelis-menten as deterministic function, and a normal distribution as stochastic model. Tweak the function in the first three grey boxes (above) such that it accomodates the michaelise menten and the normal distribution.

      _hint_: In a previous exercise you have eyeballed the parameter values of 
              the functions, you can use these as starting values.

      _hint_: In case you get convergence problems, further adapt your starting 
              values, or choose a different optimizer. For example Nelder-Mead 
              is a robust one, e.g. `method = "Nelder-Mead"`.

2. Change the determinstic function for a possible alternative determinstic 
function, and fit this new model to the data. Remember that in Lab 3 you have 
proposed multiple deterministic functions for this dataset.

3. Compare the likelihoods of the data given both models.

4. Apply model selection criteria and conclude which model fits that data best.

5. Does the model makes sense from a biological perspective?

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

```r
shapes2= read.csv("shapes2.csv")
plot(y~x, data= shapes2)
nll.mle = function(a,b,sd){
    # this calculates the mean y for a given value of x: the deterministic function
    mu = (a*x)/(b+x)
    # this calculates the likelihood of the function given the probability
    # distribution, the data and mu and sd
    nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
    return(nll)
}
```

Do maximum likelihood optimisation with mle2

```r
mle2.1 = mle2(nll.mle,start = list(a = 20, b = 10, sd = 1), 
              data = data.frame(x = shapes1$x, y = shapes1$y),
              method="Nelder-Mead")
```

Print summary of the optimisation

```r
summary(mle2.1)
```
Print maximum loglikelihood of the model

```r
logLik(mle2.1)
```

Add the curve with the parameters obtained through maximum likelihood estimates to the plot

```r
curve((coef(mle2.1)[1]*x)/(coef(mle2.1)[2]+x),add=T)
```

Now make another function with another deterministic model

```r
nll.mle.alt = function(a,b,sd){
    # this calculates the mean y for a given value of x: the deterministic function
    mu = (a*x^2)/(b+x^2)
    # this calculates the likelihood of the function given the probability
    # distribution, the data and mu and sd
    nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
    return(nll)
}

mle2.2 = mle2(nll.mle.alt,start=list(a=20,b=270,sd=1), data=data.frame(x=shapes2$x,y=shapes2$y),method="Nelder-Mead")
summary(mle2.2)
logLik(mle2.2)
AIC(mle2.1,mle2.2)
```

The first model fits better according to AIC. The difference is about 9 points on the log
Likelihood scale. So that implies that the first model makes the data exp(9) $\approx$
8,000 times more likely!

::::

:::::

::::::


# Maximum likelihood with continous and categorical predictors

Sometimes you want to fit the same model to different groups (males/females, treatment/control etc.). The easiest way is to separately fit the model to the subsets, but this makes it very difficult to assess whether the fitted parameters for both groups are comparable. A more elegant method is explained below.

We use the fifth dataset of the six datasets you have worked with earlier on (shapes5.csv or the fifth sheet from shapes.xlsx). Assume that the function was generated by a decreasing exponential function $ae^{(-bx)}$ and you want to the values of $a$ and $b$. The dataset has three columns that are relevant: the independent variable $x$, the dependent variable $y$, and a dummy variable $group$ indicating to which group the observation belongs to. We want to test whether we can justify a different $a$ and $b$ for the two groups.

This is how the NLL function would look like assuming no grouping:

```r
dat = read.csv("shapes5.csv") # and select fifth dataset
# test dataset five for differences between groups
nll0 = function(a, b, x, y){
  ymean = a*exp(-b*x)
  nll = -sum(dpois(y,lambda=ymean,log=T))
  return(nll)
}

par = list(a = 4, b = 0.2)
opt1 = mle2(start = par, minuslogl = nll0, data = dat)
```

:::::: {.callout-important title="Exercise" collapse="true"}

1. Fit the above model to the data without considering differences between groups in $a$ and $b$.

2. Adjust the likelihood function such that it can accomodate for different values of $b$ depending on the group an observation belong to.

Use the following pseudocode to achieve this and/or check page 305 for in inspiration or go back to Lab 1 section 11.1.2.
    a. Adapt the likelihood function such that the parameter `b` depends on the group.
    b. Adjust the starting values so it contains multiple starting values for `b`

3. Estimate the parameters $a$ and $b$ when letting $b$ depend on the group. Compare the negative loglikelihood of this model with the model fitted in question 1. Which has a better fit?

4. Apply model selection techniques (Likelihood ratio test, AIC or BIC) to select the most parsimonious model. Are the models nested? Which model is preferred?

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

```r
# test dataset five for differences between groups
te = function(a, b1, b2, x, y, group) {
    b = c(b1, b2)
    ymean = a*exp(-b[group]*x)
    nll = -sum(dpois(y,lambda = ymean,log=T))
    return(nll)
}
par = list(a = 4, b1 = 0.2, b2 = 0.2)
opt1 = mle2(start = par,minuslogl = te, dat = dat)
```

::::

:::::

::::::


:::::: {.callout-important title="Exercise" collapse="true"}

To practice model fitting a little bit more, you could repeat the above procedure for the other 4 datasets from shapes.xlsx.

Pick a dataset, go back to the Lab 3 Question 2.1 and Lab 4 Question 2.1 and list the stochastic model and the deterministic function and the eyeballed parameters that you thought were appropriate for this dataset. Next write a negative loglikelihood function, and use mle2 or optim to obtain the maximum likelihood estimates for the parameters.

If you have practised sufficiently, you can move on with the advanced topics below.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

Hope you had fun :)

::::

:::::

::::::

# Stochastic simulation

Stochastic simulation has a number of important goals. First, by using stochastic simulation
and subsequently refitting a model to the simulated data, one can test if the parameters that
were used to simulate the data can be retrieved, or whether the estimated parameters are
biased. Secondly, one can use stochastic simulation as a form of model testing. By
simulating from the model with the MLE parameters one can visually inspect whether the
estimated model makes sense or whether it is biased in one form or another. For example, the
model maybe well capable of describing the mean relationship between $x$ and $y$ but may not
be able to capture the variation in $y$.

For stochastic simulation on needs a deterministic model (which in its simplest form is just
a single number for the mean and a parameter related to the variance) and a stochastic model
to describe the sample space (all possible outcomes) given the parameters.


## Choosing probability distributions

In this exercise we revisit the first exercise from Lab 1. In that exercise we had six different
datasets each describing a biological phenomenon. The first step was to choose a
deterministic function that describes the mean effect of the predictor variable (x) on the
response variable (y; Lab 3). The second step involved the choice of the stochastic
distribution which describes how the data varies around the mean (Lab 4).

:::::: {.callout-important title="Exercise" collapse="true"}

Reload the first dataset, revisit the choice of your deterministic function, the eyeballed
parameters, and the stochastic distribution. Next simulate data using these three
components. Compare the simulated values with the observed values in a plot.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

If the first dataset is stored in a dataframe called shapes1, one can plot the observations
as follows:

```r
plot(shapes1$y~ shapes1$x)
```

next one simulate from the data. If we assume the normal distribution to be reasonable
choice, and a Michaelis Menten as a deterministic function, data can simulated as follows:

```r
y.sim <- rnorm(55,mean=(25*shapes1$x)/(60+shapes1$x),sd=1.5)
```

The standard deviation is quite hard to assess. One thing that one could do to estimate the
standard deviation is to estimate the variation around the mean prediction. Roughly 2*sd
gives the 95% confidence interval. As most of the datapoints seems to be within 3 points
to the mean y, this would translate in an sd of ~ 1.5.

::::

:::::

::::::

# Advanced topics

## Likelihood surface

To find the likelihood surface follow the steps below (background information can be found
in Bolker Ch. 6). This exercise continues from Lab 3 where you used the negative binomial to
generate 50 numbers and fitted back the parameters.

:::::: {.callout-important title="Exercise" collapse="true"}

For the likelihood surface:

1. Set up vectors of $\mu$ and $k$ values. Let's try $\mu$ from 0.4 to 3 in steps of 0.05 and $k$ from 0.01 to 0.7 in steps of 0.01.

2. Set up a matrix to hold the results,
The matrix for the results will have rows corresponding to $\mu$ and
columns corresponding to $k$:

3. Run `for` loops to calculate and store the values. Use a `for` nested in another one

4. Drawing a contour using the function 'contour'. Change the argument `nlevels` to 100 to get a better view of the likelihood surface

5. Add the MLE estimates in the contour plot (use 'points'). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

```r
muvec = seq(0.4,3,by=0.05)
kvec = seq(0.01,0.7,by=0.01)
resmat = matrix(nrow=length(muvec),ncol=length(kvec))
for (i in 1:length(muvec)) {
    for (j in 1:length(kvec)) {
    resmat[i,j] = NLLfun1(c(muvec[i],kvec[j]))
    }
}
contour(muvec,kvec,resmat,xlab=expression(mu),ylab="k")
contour(muvec,kvec,resmat,nlevels=100,lty=2,add=TRUE)
```
::::

:::::

::::::

## Bayesian parameter estimation: Laplace's approximation (OPTIONAL)



We are going to analyze the `shapes2` dataset again, but this time using a
Bayesian approach where we approximate the posterior distribution. Notice that
When you use maximum likelihood, it makes sense to just generate some fake data
and fit models to it (for practice), but without context we cannot come up with 
priors in a Bayesian setting. The *shapes* dataset is fake data so we need to 
come up with some *fake* context and derive priors from it.

### Constructing priors for the shapes2 dataset

Let's assume that these data represent the predation rate as a function of prey
density, so it makes to try the functional responses we learnt in Chapter 3. Let's
assume that we know a prior that the prey density is in the order of 100 and that
the predation rate is in the order of 50 With this information we can already
come up with *Weakly Informative Priors* (WIPs for short) that only inform about
the order of magnitude of a variable. A simple way to create WIPs might be:

1. Define the prior information with a Normal distribution for each parameter.

2. Set the mean to the order of magnitude of the corresponding variable (or
something related to it, depends on the role)

3. Set the standard deviation equal to the mean.

4. For scale parameters (e.g. the standard deviation) the prior distribution is
often a truncated normal with mean of 0 and standard deviation dependend on the
scale of the response variable.

Let's first start with the functional response Type II that has the form 
`a*x/(b + x)`. We will come up with reasonable prior distributions and generate
a bunch of simulations (these are called prior predictions and the can be uses
to check your priors are reasonable). Because all parameters should be positive,
I use truncated normals (that only keep positive part):

```r
library(truncnorm) # to truncate parameters
# Generate random samples of each parameter
N = 1000
a = rtruncnorm(N, mean = 50, sd = 50, a  = 0) # a scales with predation rate
b = rtruncnorm(N, mean = 100/2, sd = 100, a = 0) # b scales with half prey density
sigma = rtruncnorm(N, mean = 0, sd = 10, a = 0) # sigma scales with predation rate
```

For every prior sample, we can then generate a mean prediction:

```r
xseq = 0:200
mu_y_prior = sapply(1:N, function(i) a[i]*xseq/(b[i] + xseq))
```

This matrix has 1000 columns (one for each sample of the priors) and 201 rows for
the seq of x values. We can summarize all these predictions into an average and
qunatiles:

```r
mean_mu_y_prior = rowMeans(mu_y_prior)
lower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)
upper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)
```

And now we can visualize it:

```r
plot(xseq, mean_mu_y_prior, type = "l", ylim = c(0, 100))
lines(xseq, lower_mu_y_prior, lty = 2)
lines(xseq, upper_mu_y_prior, lty = 2)
points(shapes2)
```

With WIPS you want to make sure that the range of predictions is much wider than
the observed data.

### Building the model in Stan

Before continuing with this section you need to install RStan in your computer.
Please go to https://github.com/stan-dev/rstan/wiki/Rstan-Getting-Started to 
install Stan in your computer and test that it works.

Stan models need to be built in their own files and are made of different blocks. 
You have been given the Stan file for this first exercise but please try to 
understand how it is structured based on the description before as you will not
be given the solutions for posterior exercises. This Youtube video may also help
you: https://youtu.be/YZZSYIx1-mw?si=5nSuDzOV8lrMKyCv 

A Stan model is composed of different blocks. First, we need to specify the data 
that will be used:

```stan
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector<lower=0>[N] y;
  vector[6] hp;
}
```

Here we specify the length of the data `N` as a positive integer `int<lower=0>`.
The two variables in the model (`x` and `y`) are defined as vectors of positive
(real) values (`vector<lower=0>`) and length `N`. Finally, there are six values
passed along the vector `hp` which will be used to parameterize the prior
distributions.

The next block represents the parameters of the model that will be estimated:

```stan
parameters {
  real<lower=0> a;
  real<lower=0> b;
  real<lower=0> sigma;
}

```

These are the three parameters of our likelihood function (`a` and `b` for the 
deterministic model and `sigma` for the error) which are all assumed to be 
positive.

Finally we have the model block, that actually implements the priors and likelihood.
There are two ways to implement thids in Stan, either using the probabilistic
modelling approach (which resembles the BUGS code in the book) or by adding up
log probability densities (which resembles the way we build the negative 
log-likelihood functions). 

The probabilistic approach looks as follows:

```stan
model {
  a     ~ normal(hp[1], hp[2]);
  b     ~ normal(hp[3], hp[4]);
  sigma ~ normal(hp[5], hp[6]);
  y ~ normal(a*x./(b + x), sigma);
}
```

The first three lines indicate that the parameters `a`, `b` and `sigma` follow
normal distributions with hyperparameters take from the vector `hp`. The last
line indicates that the observations `y` follow a normal distribution which mean
is given by the deterministic model `a*x./(b + x)` and the standard deviation is
`sigma`.

The alternative approach looks like this:

```stan
model {
  target += normal_lpdf(a | hp[1], hp[2]);
  target += normal_lpdf(b | hp[3], hp[4]);
  target += normal_lpdf(sigma | hp[5], hp[6]);
  target += normal_lpdf(y | a*x./(b + x), sigma);  
}
```

Where `target` is a special variable in Stan that accumulates the log-likelihood
and log-prior, `normal_lpdf` stands for the log probability density of the normal
distribution and `|` is the symbol for conditionality (on the left of `|` we 
indicate the random variable and on the right other parameters of the distribution).

### Loading the model in Stan and running Laplace's approximation

We can compile and load the model as follows (here I assume the model is in a
file called `mm.stan` inside the folder `Lab6`).

```r
library(rstan)
options(mc.cores = parallel::detectCores()) # To enable parallel chains
rstan_options(auto_write = TRUE) # To avoid recompilation
mm_model = stan_model(file = "./Lab6/mm.stan")
```

This took a while because the Stan model needs to be compiled to machine code
that can then be executed. C++ compilation can be quite slow so please be patient
(it will pay off later, trust me).

We are going to fit this model the data using Laplace's approximation. This 
approximates the posterior distribution as a Normal distribution with a mean
equal to the maximum posterior estimates and variance derived from the Hessian
matrix. Mathematically this is equivalent to maximum likelihood + quadratic
approximation (section 6.5 in the book) but with prior distributions. We can do
this in Stan using `optimizing()`:

```r
data = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, 
            hp = c(50, 50, 50, 100, 0, 10)) # see above for priors
mm_fit = optimizing(mm_model, data = data, init = list(a = 50, b = 40, sigma = 1),
                    hessian = TRUE)
```

We can retrieve the estimated mode of the posterior:

```r
mm_fit$par
```

And we can use the Hessian to compute a variance-covariance matrix:

```r
V = MASS::ginv(-mm_fit$hessian)
```

And now our posterior is defined as a multivariate normal with `mu = mm_fit$par`
and `Sigma = V`. We can generate samples using `mvrnorm`:

```r
posterior_samples = MASS::mvrnorm(1000, mu = mm_fit$par, Sigma = V)
```

We can directly visualize the marginal distribution from this sample:

```r
par(mfrow = c(1,2))
hist(posterior_samples[,"a"], main = "", xlab = "a", prob = T)
hist(posterior_samples[,"b"], main = "", xlab = "b", prob = T)
```

But also the correlation among these two parameters:

```r
plot(posterior_samples[,1:2], xlab = "a", ylab = "b")
```

:::::: {.callout-important title="Exercise" collapse="true"}

Fit the model `a*x^2/(b + x^2)` to the same dataset as the model above using
Laplace's approximation and Stan.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

The stan model will look as follows (I use the probabilistic flavour because 
it is a bit easier to write down)

```stan
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector<lower=0>[N] y;
  vector[6] hp;
}

parameters {
  real<lower=0> a;
  real<lower=0> b;
  real<lower=0> sigma;
}

model {
  a     ~ normal(hp[1], hp[2]);
  b     ~ normal(hp[3], hp[4]);
  sigma ~ normal(hp[5], hp[6]);
  y ~ normal(a*pow(x, 2)./(b + pow(x, 2)), sigma);
}
```

Notice than in C++, `x^2` needs to be written as `pow(x, 2)`. 

I can adjust the priors based on the new scale (no `a` is in the scale of `y`
but `b` is in the scale of `x`)

```r
b = rtruncnorm(N, mean = 10000/2, sd = 10000, a = 0) # b scales with half prey density
mu_y_prior = sapply(1:N, function(i) a[i]*xseq^2/(b[i] + xseq^2))
mean_mu_y_prior = rowMeans(mu_y_prior)
lower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)
upper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)
plot(xseq, mean_mu_y_prior, type = "l", ylim = c(0, 100))
lines(xseq, lower_mu_y_prior, lty = 2)
lines(xseq, upper_mu_y_prior, lty = 2)
points(shapes2)
```

We then load the new model and run Laplace's approximation:

```r
mm2_model = stan_model(file = "./Lab6/mm2.stan")
data = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, 
            hp = c(50, 50, 100, 200, 0, 10)) # see above for priors
mm2_fit = optimizing(mm2_model, data = data, init = list(a = 50, b = 4000, sigma = 1),
                    hessian = TRUE)
```

And I can retrieve the parameter values as I did before

```r
V = MASS::ginv(-mm2_fit$hessian)
posterior_samples = MASS::mvrnorm(1000, mu = mm2_fit$par, Sigma = V)
par(mfrow = c(1,3))
hist(posterior_samples[,"a"], main = "", xlab = "a", prob = T)
hist(posterior_samples[,"b"], main = "", xlab = "b", prob = T)
plot(posterior_samples[,1:2], xlab = "a", ylab = "b")
```
::::

:::::

::::::


# Supplement: hints for choosing deterministic functions and stochastic functions

1. Deterministic functions

 - dataset 1

 light response curve. There are a number of options of functions to choose from, depending on the level of sophistication:

$\frac{ax}{(b+x)}$, $a(1-e^{(-bx)})$, $\frac{1}{2\theta}(\alpha I+p_{max}-\sqrt(\alpha I+p_{max})^2-4\theta I p_{max})$ see page 98. A parameter `d` can be added in all cases to shift the curve up or down. The y represents net photosynthesis $\mu mol CO_{2}/m^2s$

 - dataset 2

The dataset describes a functional responses. Bolker mentions four of those

$\min(ax,s)$ $\frac{ax}{(b+x)}$, $\frac{ax^2}{(b^2+x^2)}$,$\frac{ax^2}{(b+cx+x^2)}$

The y is measured in grams prey eaten per unit time.

 - dataset 3
Allometric relationships generally have the form $ax^b$. The y represent the total number of cones produced.

 - dataset 4
This could be logistic growth $n(t)=\frac{K}{1+(\frac{K}{n_0})e^{-rt}}$ or the gompertz function $f(x)=e^{-ae^{-bx}}$. The y represent the population size (numbers).

 - dataset 5
What about a negative exponential? $ae{-bx}$ or a power function $ax^b$. The y represent a number per unit area.

 - dataset 6
Species reponse curves are curves that describe the probability of presence as a function of some factor. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g.
$a e^{\frac{-(x-\mu)^2}{2\sigma^2}}$. The y represent presence or absence of the species (no units).


2. Stochastic functions/Probability distributions

   - dataset 1
y represents real numbers and both positive and negative numbers occur. This implies that we should choose a continuous probability distribution. In addition, the numbers seems unbound. Within the family of continuous probability distributions, the normal seems a good candidate distribution because this one runs from -$\inf$ to +$\inf$. In contrast the Gamma and the Lognormal only can take positive numbers, so these distributions cannot handle the negative numbers. In addition, the beta distribution is not a good candidate because it runs from 0-1.

   - dataset 2
y represents real numbers and only positive numbers occur. The data represents a functional response (intake rate of the predator), and it is likely that you can only measure positive numbers (number of prey items per unit of time).  This implies that we should choose a continuous probability distribution. Within the family of continuous probability distributions, the Gamma and the Lognormal could be taken as candidate distributions because they can only take positive numbers (beware that the Gamma cannot take 0). However, you could try to use a normal as well.

   - dataset 3
y seems represents counts (this is the cone dataset that is introduced in ch. 6.). Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.

   - dataset 4
y represents population size over time. From looking at the data, they seems to represent counts. Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.

   - dataset 5
No information is given on y. The data clearly seems to represent counts. Thus the same reasoning applies here as to the two previous datasets.

   - dataset 6
The data (y) represents species occurences (presence/absence). The binomial model would be a good model to predict the probability of presence.

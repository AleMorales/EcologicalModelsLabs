<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Practical Labs CSA-40306 Ecological Modelling and Data Analysis in R – material</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Practical Labs CSA-40306 Ecological Modelling and Data Analysis in R
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Practical Labs CSA-40306 Ecological Modelling and Data Analysis in R</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab1/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab2/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab3/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab4/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab6/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 5 &amp; 6</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab7/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab9/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 9</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab10/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 10</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supplement: mle2 vs optim</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/canned.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supplement: Formula-based methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/bias_variance/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supplement: Variance and bias of estimates</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/Stan/no_solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supplement: Bayesian inference with Stan</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Solutions</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab1/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 1 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab2/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 2 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab3/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 3 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab4/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 4 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab6/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 5 &amp; 6 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab7/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 7 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab9/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 9 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Lab10/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab 10 (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/bias_variance/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Variance and bias of estimates (solutions)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Supplements/Stan/solution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supplement: Bayesian inference with Stan (solutions)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#learning-goals" id="toc-learning-goals" class="nav-link active" data-scroll-target="#learning-goals"><span class="header-section-number">1</span> Learning goals</a></li>
  <li><a href="#fitting-models-to-data" id="toc-fitting-models-to-data" class="nav-link" data-scroll-target="#fitting-models-to-data"><span class="header-section-number">2</span> Fitting models to data</a></li>
  <li><a href="#fitting-parameters-of-made-up-data" id="toc-fitting-parameters-of-made-up-data" class="nav-link" data-scroll-target="#fitting-parameters-of-made-up-data"><span class="header-section-number">3</span> Fitting parameters of made-up data</a>
  <ul class="collapse">
  <li><a href="#finding-the-maximum-likelihood-estimate-of-the-paramaters" id="toc-finding-the-maximum-likelihood-estimate-of-the-paramaters" class="nav-link" data-scroll-target="#finding-the-maximum-likelihood-estimate-of-the-paramaters"><span class="header-section-number">3.1</span> Finding the maximum likelihood estimate of the paramaters</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-and-continuous-covariates" id="toc-maximum-likelihood-and-continuous-covariates" class="nav-link" data-scroll-target="#maximum-likelihood-and-continuous-covariates"><span class="header-section-number">4</span> Maximum likelihood and continuous covariates</a></li>
  <li><a href="#maximum-likelihood-with-continous-and-categorical-predictors" id="toc-maximum-likelihood-with-continous-and-categorical-predictors" class="nav-link" data-scroll-target="#maximum-likelihood-with-continous-and-categorical-predictors"><span class="header-section-number">5</span> Maximum likelihood with continous and categorical predictors</a></li>
  <li><a href="#stochastic-simulation" id="toc-stochastic-simulation" class="nav-link" data-scroll-target="#stochastic-simulation"><span class="header-section-number">6</span> Stochastic simulation</a>
  <ul class="collapse">
  <li><a href="#choosing-probability-distributions" id="toc-choosing-probability-distributions" class="nav-link" data-scroll-target="#choosing-probability-distributions"><span class="header-section-number">6.1</span> Choosing probability distributions</a></li>
  </ul></li>
  <li><a href="#advanced-topics" id="toc-advanced-topics" class="nav-link" data-scroll-target="#advanced-topics"><span class="header-section-number">7</span> Advanced topics</a>
  <ul class="collapse">
  <li><a href="#likelihood-surface" id="toc-likelihood-surface" class="nav-link" data-scroll-target="#likelihood-surface"><span class="header-section-number">7.1</span> Likelihood surface</a></li>
  <li><a href="#optional-bayesian-parameter-estimation-laplaces-approximation" id="toc-optional-bayesian-parameter-estimation-laplaces-approximation" class="nav-link" data-scroll-target="#optional-bayesian-parameter-estimation-laplaces-approximation"><span class="header-section-number">7.2</span> (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation</a>
  <ul class="collapse">
  <li><a href="#constructing-priors" id="toc-constructing-priors" class="nav-link" data-scroll-target="#constructing-priors"><span class="header-section-number">7.2.1</span> Constructing priors</a></li>
  <li><a href="#laplaces-approximation-how-bayes-rule-was-solved-originally" id="toc-laplaces-approximation-how-bayes-rule-was-solved-originally" class="nav-link" data-scroll-target="#laplaces-approximation-how-bayes-rule-was-solved-originally"><span class="header-section-number">7.2.2</span> Laplace’s approximation: How Bayes rule was solved originally</a></li>
  <li><a href="#implementing-laplaces-approximation" id="toc-implementing-laplaces-approximation" class="nav-link" data-scroll-target="#implementing-laplaces-approximation"><span class="header-section-number">7.2.3</span> Implementing Laplace’s approximation</a></li>
  <li><a href="#computing-dic" id="toc-computing-dic" class="nav-link" data-scroll-target="#computing-dic"><span class="header-section-number">7.2.4</span> Computing DIC</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#supplement-hints-for-choosing-deterministic-functions-and-stochastic-functions" id="toc-supplement-hints-for-choosing-deterministic-functions-and-stochastic-functions" class="nav-link" data-scroll-target="#supplement-hints-for-choosing-deterministic-functions-and-stochastic-functions"><span class="header-section-number">8</span> Supplement: hints for choosing deterministic functions and stochastic functions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="learning-goals" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Learning goals</h1>
<p>You will learn how to:</p>
<ol type="1">
<li><p>Program the likelihood function of a model.</p></li>
<li><p>Estimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.</p></li>
<li><p>Estimate the confidence intervals of the model parameters through profiling and the quadratic approximation.</p></li>
<li><p>Perform stochastic simulations from the fitted models</p></li>
</ol>
<p>5 (Optionally) Estimate parameters and uncertainty using Laplace’s approximation to Bayes rule</p>
</section>
<section id="fitting-models-to-data" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Fitting models to data</h1>
<p>Fitting a model to data through likelihood requires that you take five steps:</p>
<ol type="1">
<li>Specify how the dependent variable depends on the independent variable, i.e.&nbsp;specify a function that describes how the mean of y depends on the value of x.</li>
<li>Specify a probability distribution to describe the deviations of the observations from the mean</li>
<li>Specify a function that calculate the negative log likelihood (NLL) based on the data and the parameter values.</li>
<li>Choose the parameters of the deterministic model and the probability model such that the negative log likelihood is lowest.</li>
<li>Compare the likelihood of alternative models (change the deterministic function or the stochastic function) and compare with AIC(c) or BIC which model is most parsimonious.</li>
</ol>
<p>For example to calculate the NLL of a linear model and a normal distribution the following function works:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>nll <span class="ot">=</span> <span class="cf">function</span>(a, b, sd, y, x){</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this calculates the mean y for a given value of x:</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#the deterministic function</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> a <span class="sc">+</span> b<span class="sc">*</span>x</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this calculates the likelihood of the function given the probability</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># distribution, the data and mu and sd</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  nll <span class="ot">=</span> <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dnorm</span>(y, <span class="at">mean =</span> mu, <span class="at">sd =</span> sd, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(nll)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice that the function takes three arguments: a vector with parameters, a vector with <code>y</code> values and a vector with <code>x</code> values. Inside the vector par, three values are stored: <code>a</code>,<code>b</code> and <code>sd</code>. Next, the mean given x is calculated with <code>mu=a+b*x</code>. The nll returns the Negative LogLikelihood of the data (<code>y</code>) given a normal distribution with mean <code>mu</code> (vector!) and a standard deviation <code>sd</code>. The <code>log=TRUE</code> returns the log of the probability densities.</p>
<p>Next we call an optimisation function to find the maximum likelihood estimate</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some fake data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span>x</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="at">mean =</span> mu, <span class="at">sd =</span> <span class="fl">0.2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initial guess of parameter values</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>par <span class="ot">=</span> <span class="fu">list</span>(<span class="at">a =</span> <span class="dv">1</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Always test the function first</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">nll</span>(<span class="at">a =</span> par<span class="sc">$</span>a, <span class="at">b =</span> par<span class="sc">$</span>b, <span class="at">sd =</span> par<span class="sc">$</span>sd, <span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># y represents the data, x the independent variable</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bbmle)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>opt1 <span class="ot">=</span> <span class="fu">mle2</span>(<span class="at">start =</span> par, <span class="at">minuslogl =</span> nll, <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The optimization result is a list with elements:</p>
<ul>
<li><p>The best-fit parameters (<code>coef(opt1)</code>, with parameter names because we named the elements of the starting vector—see how useful this is?);}</p></li>
<li><p>The maximum log-likelihood and degrees of freedom of the model (<code>logLik(opt1)</code>);</p></li>
<li><p>Details on the optimization (<code>opt1@details</code>) including number of function and gradient evaluations, the convergence flag (should be 0 unless there were issues, in which case a message will be included) and the Hessian matrix if it was calculated</p></li>
</ul>
</section>
<section id="fitting-parameters-of-made-up-data" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Fitting parameters of made-up data</h1>
<p>The simplest thing to do to convince yourself that your attempts to estimate parameters are working is to simulate the ‘’data’’ yourself and see if you get close to the right answers back. Set the random seed to 1001 so we get identical answers across r sessions.</p>
<section id="finding-the-maximum-likelihood-estimate-of-the-paramaters" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="finding-the-maximum-likelihood-estimate-of-the-paramaters"><span class="header-section-number">3.1</span> Finding the maximum likelihood estimate of the paramaters</h2>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Take the steps below</p>
<ol type="1">
<li><p>Generate 50 values from a negative binomial (<code>rnbinom</code>) with <span class="math inline">\(\mu=1\)</span>, <span class="math inline">\(k=0.4\)</span>. Save the values in variables in case we want to use them again later.</p></li>
<li><p>Plot the numbers in a frequency diagram</p></li>
<li><p>Next, define the negative log-likelihood function for a simple draw from a negative binomial distribution: first include the parameters of the model and then the variables in the data that are used in the model (see example above)</p></li>
<li><p>Calculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Remember that combine these parameter values using a <code>list()</code> and to name them so you can recognize them later on.</p></li>
<li><p>Calculate the NLL of parameter values that are far from the values that were used to generate the data (e.g.&nbsp;<span class="math inline">\(\mu=10\)</span>, <span class="math inline">\(k=10\)</span>)</p></li>
<li><p>Calculate the maximum likelihood estimate (MLE)? Use <code>mle2</code> with the default options and use the method-of-moments estimates as the starting estimates (<code>par</code>): <code>opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom))</code></p></li>
<li><p>What is the difference in NLL between the MLE estimates and the NLL derived at 5? Does it make sense?</p></li>
<li><p>Perform a likelihood ratio test at 95% confidence compare the values obtained in 5 and 7.</p></li>
</ol>
</div>
</div>
</div>
</section>
</section>
<section id="maximum-likelihood-and-continuous-covariates" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Maximum likelihood and continuous covariates</h1>
<p>The following exercise has the purpose to learn you how to fit a model to data when we have a single covariate.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Take the second dataset (shapes2.csv from shapes.xlsx), use a michaelis-menten as deterministic function, and a normal distribution as stochastic model. Tweak the function in the first three grey boxes (above) such that it accomodates the michaelise menten and the normal distribution.</p>
<p><em>hint</em>: In a previous exercise you have eyeballed the parameter values of the functions, you can use these as starting values.</p>
<p><em>hint</em>: In case you get convergence problems, further adapt your starting values, or choose a different optimizer. For example Nelder-Mead is a robust one, e.g.&nbsp;<code>method = "Nelder-Mead"</code>.</p></li>
<li><p>Change the determinstic function for a possible alternative determinstic function, and fit this new model to the data. Remember that in Lab 3 you have proposed multiple deterministic functions for this dataset.</p></li>
<li><p>Compare the likelihoods of the data given both models.</p></li>
<li><p>Apply model selection criteria and conclude which model fits that data best.</p></li>
<li><p>Does the model makes sense from a biological perspective?</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood-with-continous-and-categorical-predictors" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Maximum likelihood with continous and categorical predictors</h1>
<p>Sometimes you want to fit the same model to different groups (males/females, treatment/control etc.). The easiest way is to separately fit the model to the subsets, but this makes it very difficult to assess whether the fitted parameters for both groups are comparable. A more elegant method is explained below.</p>
<p>We use the fifth dataset of the six datasets you have worked with earlier on (shapes5.csv or the fifth sheet from shapes.xlsx). Assume that the function was generated by a decreasing exponential function <span class="math inline">\(ae^{(-bx)}\)</span> and you want to the values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The dataset has three columns that are relevant: the independent variable <span class="math inline">\(x\)</span>, the dependent variable <span class="math inline">\(y\)</span>, and a dummy variable <span class="math inline">\(group\)</span> indicating to which group the observation belongs to. We want to test whether we can justify a different <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for the two groups.</p>
<p>This is how the NLL function would look like assuming no grouping:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"shapes5.csv"</span>) <span class="co"># and select fifth dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># test dataset five for differences between groups</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>nll0 <span class="ot">=</span> <span class="cf">function</span>(a, b, x, y){</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ymean <span class="ot">=</span> a<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>b<span class="sc">*</span>x)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  nll <span class="ot">=</span> <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dpois</span>(y,<span class="at">lambda=</span>ymean,<span class="at">log=</span>T))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(nll)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>par <span class="ot">=</span> <span class="fu">list</span>(<span class="at">a =</span> <span class="dv">4</span>, <span class="at">b =</span> <span class="fl">0.2</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>opt1 <span class="ot">=</span> <span class="fu">mle2</span>(<span class="at">start =</span> par, <span class="at">minuslogl =</span> nll0, <span class="at">data =</span> dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Fit the above model to the data without considering differences between groups in <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p></li>
<li><p>Adjust the likelihood function such that it can accomodate for different values of <span class="math inline">\(b\)</span> depending on the group an observation belong to.</p></li>
</ol>
<p>Use the following pseudocode to achieve this and/or check page 305 for in inspiration or go back to Lab 1 section 11.1.2. a. Adapt the likelihood function such that the parameter <code>b</code> depends on the group. b. Adjust the starting values so it contains multiple starting values for <code>b</code></p>
<ol start="3" type="1">
<li><p>Estimate the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> when letting <span class="math inline">\(b\)</span> depend on the group. Compare the negative loglikelihood of this model with the model fitted in question 1. Which has a better fit?</p></li>
<li><p>Apply model selection techniques (Likelihood ratio test, AIC or BIC) to select the most parsimonious model. Are the models nested? Which model is preferred?</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To practice model fitting a little bit more, you could repeat the above procedure for the other 4 datasets from shapes.xlsx.</p>
<p>Pick a dataset, go back to the Lab 3 Question 2.1 and Lab 4 Question 2.1 and list the stochastic model and the deterministic function and the eyeballed parameters that you thought were appropriate for this dataset. Next write a negative loglikelihood function, and use mle2 or optim to obtain the maximum likelihood estimates for the parameters.</p>
<p>If you have practised sufficiently, you can move on with the advanced topics below.</p>
</div>
</div>
</div>
</section>
<section id="stochastic-simulation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Stochastic simulation</h1>
<p>Stochastic simulation has a number of important goals. First, by using stochastic simulation and subsequently refitting a model to the simulated data, one can test if the parameters that were used to simulate the data can be retrieved, or whether the estimated parameters are biased. Secondly, one can use stochastic simulation as a form of model testing. By simulating from the model with the MLE parameters one can visually inspect whether the estimated model makes sense or whether it is biased in one form or another. For example, the model maybe well capable of describing the mean relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> but may not be able to capture the variation in <span class="math inline">\(y\)</span>.</p>
<p>For stochastic simulation on needs a deterministic model (which in its simplest form is just a single number for the mean and a parameter related to the variance) and a stochastic model to describe the sample space (all possible outcomes) given the parameters.</p>
<section id="choosing-probability-distributions" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="choosing-probability-distributions"><span class="header-section-number">6.1</span> Choosing probability distributions</h2>
<p>In this exercise we revisit the first exercise from Lab 1. In that exercise we had six different datasets each describing a biological phenomenon. The first step was to choose a deterministic function that describes the mean effect of the predictor variable (x) on the response variable (y; Lab 3). The second step involved the choice of the stochastic distribution which describes how the data varies around the mean (Lab 4).</p>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Reload the first dataset, revisit the choice of your deterministic function, the eyeballed parameters, and the stochastic distribution. Next simulate data using these three components. Compare the simulated values with the observed values in a plot.</p>
</div>
</div>
</div>
</section>
</section>
<section id="advanced-topics" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Advanced topics</h1>
<section id="likelihood-surface" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="likelihood-surface"><span class="header-section-number">7.1</span> Likelihood surface</h2>
<p>To find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues from Lab 3 where you used the negative binomial to generate 50 numbers and fitted back the parameters.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For the likelihood surface:</p>
<ol type="1">
<li><p>Set up vectors of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(k\)</span> values. Let’s try <span class="math inline">\(\mu\)</span> from 0.4 to 3 in steps of 0.05 and <span class="math inline">\(k\)</span> from 0.01 to 0.7 in steps of 0.01.</p></li>
<li><p>Set up a matrix to hold the results, The matrix for the results will have rows corresponding to <span class="math inline">\(\mu\)</span> and columns corresponding to <span class="math inline">\(k\)</span>:</p></li>
<li><p>Run <code>for</code> loops to calculate and store the values. Use a <code>for</code> nested in another one</p></li>
<li><p>Drawing a contour using the function ‘contour’. Change the argument <code>nlevels</code> to 100 to get a better view of the likelihood surface</p></li>
<li><p>Add the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="optional-bayesian-parameter-estimation-laplaces-approximation" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="optional-bayesian-parameter-estimation-laplaces-approximation"><span class="header-section-number">7.2</span> (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation</h2>
<p>We are going to analyze the <code>shapes2</code> dataset again, but this time using a Bayesian approach where we approximate the posterior distribution. This approximation is sometimes called Laplace’s approximation, but that name is also reserved for a related technique that we will use in Chapter 10, so here we will call it <em>quadratic approximation</em> because it uses the same mathematical tools as the quadratic approximation in maximum likelihood (in fact, the two are equivalent if you assume uniform priors).</p>
<p>We will work with the second of the shapes datasets and we will construct a type of prior know as <em>Weakly Informative Priors</em> (WIPs). These are used when you do not have clear prior information (or don’t want to use it) except for knowing what the order of magnitude of the parameters should be. WIPs are very popular in modern Bayesian statistics because they have a small effect on the posterior distribution while addressing many of the complicated numerical issues that we encounter in maximum likelihood. Use quadratic approximation with WIPs is basically an enhanced version of maximum likelihood.</p>
<section id="constructing-priors" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="constructing-priors"><span class="header-section-number">7.2.1</span> Constructing priors</h3>
<p>Let’s assume that these data represent the predation rate as a function of prey density, so it makes to try the functional responses we learnt in Chapter 3. Let’s assume that we can build a prior that the prey density is in the order of 100 and that the predation rate is in the order of 50 With this information we can already come up with <em>Weakly Informative Priors</em> (WIPs for short) that only informs about the order of magnitude of a variable. A simple way to create WIPs might be:</p>
<ol type="1">
<li><p>Define the prior information with a Normal distribution for each parameter.</p></li>
<li><p>Set the mean to the order of magnitude of the corresponding variable (or something related to it, depends on the role)</p></li>
<li><p>Set the standard deviation equal to the mean.</p></li>
<li><p>For scale parameters (e.g.&nbsp;the standard deviation) the prior distribution is often a truncated normal with mean of 0 and standard deviation dependend on the scale of the response variable.</p></li>
</ol>
<p>Let’s first start with the functional response Type II that has the form <code>a*x/(b + x)</code>. We will come up with reasonable prior distributions and generate a bunch of simulations (these are called prior predictions and the can be used to check your priors are reasonable). Because all parameters should be positive, I use truncated normals (that only keep positive part):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(truncnorm) <span class="co"># to truncate parameters</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random samples of each parameter</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">rtruncnorm</span>(N, <span class="at">mean =</span> <span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">50</span>, <span class="at">a  =</span> <span class="dv">0</span>) <span class="co"># a scales with predation rate</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">rtruncnorm</span>(N, <span class="at">mean =</span> <span class="dv">100</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">100</span>, <span class="at">a =</span> <span class="dv">0</span>) <span class="co"># b scales with half prey density</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="fu">rtruncnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>, <span class="at">a =</span> <span class="dv">0</span>) <span class="co"># sigma scales with predation rate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For every prior sample, we can then generate a mean prediction:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>xseq <span class="ot">=</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">200</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mu_y_prior <span class="ot">=</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="cf">function</span>(i) a[i]<span class="sc">*</span>xseq<span class="sc">/</span>(b[i] <span class="sc">+</span> xseq))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>mean_mu_y_prior <span class="ot">=</span> <span class="fu">rowMeans</span>(mu_y_prior)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>lower_mu_y_prior <span class="ot">=</span> <span class="fu">apply</span>(mu_y_prior, <span class="dv">1</span>, quantile, <span class="at">prob =</span> <span class="fl">0.025</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>upper_mu_y_prior <span class="ot">=</span> <span class="fu">apply</span>(mu_y_prior, <span class="dv">1</span>, quantile, <span class="at">prob =</span> <span class="fl">0.975</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And now we can visualize it:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xseq, mean_mu_y_prior, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>),</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="ot">=</span> <span class="st">"Prey density"</span>, <span class="at">ylab =</span> <span class="st">"Predation rate"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, lower_mu_y_prior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, upper_mu_y_prior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(shapes2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With WIPS you want to make sure that the range of predictions is much wider than the observed data, while avoiding nonsensical predictions (e.g., negative values or values that are way too high).</p>
</section>
<section id="laplaces-approximation-how-bayes-rule-was-solved-originally" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="laplaces-approximation-how-bayes-rule-was-solved-originally"><span class="header-section-number">7.2.2</span> Laplace’s approximation: How Bayes rule was solved originally</h3>
<p>Remember that Bayes rule was</p>
<p><span class="math display">\[
P(\theta | D) = \frac{P(D|\theta)P(\theta)}{P(D)},
\]</span> where <span class="math inline">\(P(D|\theta)\)</span> is equivalent to the likelihood function, <span class="math inline">\(P(\theta)\)</span> is the prior probability of parameter values, <span class="math inline">\(P(\theta | D)\)</span> is the posterior probability and <span class="math inline">\(P(D)\)</span> is the probability of seeing thee particular data under the assumed mode.</p>
<p>Note that <span class="math inline">\(P(D|\theta)\)</span> and <span class="math inline">\(P(\theta)\)</span> are built by you (they are functions of <span class="math inline">\(\theta\)</span>) and therefore you know them, but you do not know <span class="math inline">\(P(D)\)</span> (this is an unknown value). If you knew it, then Bayesian statistics would be “straightforward” in the sense that you just need to multiple two functions and divide by a constant. In practice you would need to do some extra work as this only gives you a probability density function, you would then need to figure out the cumulative density function, how to generate random samples from the posterior, etc.</p>
<p>The first person to solve Bayes rule was called Laplace and he realized you could calculate <span class="math inline">\(P(D)\)</span> as:</p>
<p><span class="math display">\[
P(D) = \int{P(D|\theta)P(\theta)d\theta}
\]</span></p>
<p>This seemingly innocent integral can be quite hard (or in practice impossible) to solve beyond the simplest of models (the calculus of probability of functions can be too nasty). He developed a mathematical approximation that consists of:</p>
<ol type="1">
<li><p>Take the log of <span class="math inline">\(P(D|\theta)P(\theta\)</span> (let’s call it the <em>log probability density</em> or <em>lpd</em>).</p></li>
<li><p>Find its maximum.</p></li>
<li><p>Approximate the lpd with a 2nd Order Taylor approximation around the maximum.</p></li>
</ol>
<p>From that he derived what the integral should be if the approximation was exact, based on the Normal distribution. This procedure for approximating integrals will come back in Chapter 10.</p>
<p>After Laplace published his method it was shown (not sure when or by whom) that his method of approximating <span class="math inline">\(P(D)\)</span> was equivalent to the following:</p>
<ol type="1">
<li><p>Find the value of <span class="math inline">\(\theta\)</span> that maximizes lpd (we call it <strong>M</strong>aximum <strong>a</strong> <strong>P</strong>osteriori, or MAP).</p></li>
<li><p>Approximate the posterior distribution <span class="math inline">\(P(\theta | D)\)</span> by a normal distribution with mean equal to MAP and covariance matrix derive from the Hessian matrix of second order derivatives.</p></li>
</ol>
<p>That is, Laplace’s approximation to <span class="math inline">\(P(D)\)</span> is equivalent to approximating the posterior distribution by a Normal distribution centered around its mode.</p>
<p>Does this procedure sound familiar? If we assume uniform priors, Laplace’s approximation is exactly the same as maximum likelihood with quadratic approximation (section 6.5 in the book) which is the original maximum likelihood method published by Fisher in 1922. Indeed, Laplace invented this method 150 years before Fisher, but in a Bayesian rather than Frequentist context. The only difference is that in the Bayesian context you take into account prior distributions rather than just the likelihood.</p>
<p>As discussed in the book, this approximation becomes better as the amount of data increases (i.e., it is only exact asymptotically) and this remains true in the Bayesian context too. Some authors seem to think that the only correct way to do Bayesian statistics is to run complex algorithms such as Markov Chain Monte Carlo (we cover that in the next chapter) but using Laplace’s approximation is as valid as using the quadratic approximation for maximum likelihood. Thus, we will learn how to use this approach first and leave the more complex MCMC approaches for later.</p>
</section>
<section id="implementing-laplaces-approximation" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="implementing-laplaces-approximation"><span class="header-section-number">7.2.3</span> Implementing Laplace’s approximation</h3>
<p>We can implement Laplace’s approximation in a similar way to how we implemented the method of maximum likelihood. First, we build a function to compute the log posterior density (for consistency, we will use the negative, as in the maximum likelihood theory):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First the negative log likelihood</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>nll <span class="ot">=</span> <span class="cf">function</span>(a, b, sd, x, y) {</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> (a<span class="sc">*</span>x)<span class="sc">/</span>(b<span class="sc">+</span>x)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  nll <span class="ot">=</span> <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dnorm</span>(y, <span class="at">mean =</span> mu,<span class="at">sd =</span> sd, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we add the prior</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>nlpd <span class="ot">=</span> <span class="cf">function</span>(a, b, sd, x, y){</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    nlpd <span class="ot">=</span> <span class="fu">nll</span>(a, b, sd, x, y) <span class="sc">-</span> </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>           <span class="fu">dnorm</span>(a, <span class="at">mean =</span> <span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">50</span>, <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">-</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>           <span class="fu">dnorm</span>(b, <span class="at">mean =</span> <span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">100</span>, <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">-</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>           <span class="fu">dnorm</span>(sd, <span class="at">mean =</span> <span class="dv">0</span>,  <span class="at">sd =</span> <span class="dv">10</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(nlpd)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="fu">nlpd</span>(<span class="at">a =</span> <span class="dv">50</span>, <span class="at">b =</span> <span class="dv">50</span>, <span class="at">sd =</span> <span class="dv">10</span>, <span class="at">x =</span> shapes2<span class="sc">$</span>x, <span class="at">y =</span> shapes2<span class="sc">$</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can now run the optimizer as usual</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>MAP <span class="ot">=</span> <span class="fu">mle2</span>(nlpd, <span class="at">start =</span> <span class="fu">list</span>(<span class="at">a =</span> <span class="dv">20</span>, <span class="at">b =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> shapes2, <span class="at">method=</span><span class="st">"L-BFGS-B"</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">lower =</span> <span class="fu">c</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s look at the results:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(MAP)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Of course these results assume we were doing maximum likelihood, so much of the information is not relevant. But the first two columns of the <code>Coefficients</code> sector are now giving us the mode of the posterior distribution and the standard deviation of each of the posterior marginal distributions.</p>
<p>In good Bayesian fashion we may want to generate a sample of values from the posterior distribution and use to make other calculations (e.g., predictions). We first need to extract the variance-covariance matrix</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>V <span class="ot">=</span> <span class="fu">vcov</span>(MAP)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then we use a multivariate normal with mean equal to the MAP estimate and using the variance-covariance matrix we just extracted:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">=</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> <span class="fl">1e4</span>, <span class="at">mean =</span> <span class="fu">coef</span>(MAP), <span class="at">sigma =</span> V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s visualize it (I only choose 1000 values because otherwise it is too slow):</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(posterior[<span class="dv">1</span><span class="sc">:</span><span class="fl">1e3</span>,], <span class="at">pch =</span> <span class="st">"."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can also look at the marginal distributions and compare the posterior and prior:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">cbind</span>(<span class="at">a =</span> a, <span class="at">b =</span> b, <span class="at">sd =</span> sigma)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">density</span>(posterior[,i]), <span class="at">xlab =</span> <span class="fu">colnames</span>(posterior)[i], <span class="at">main =</span> <span class="st">""</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">density</span>(prior[,i]), <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice that in the region where the posterior samples where generated (basically where most of the probability is located), the prior distributions are practically flat. This is what makes a prior weakly informative. It has practically no effect on the posterior but it can help the algorithm find the solution much faster. This will lead to Bayesian results that are very similar to Maximum Likelihood results.</p>
<p>We can also make predictions and compare it to the prior predictions we did before.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">nrow</span>(posterior)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>xseq <span class="ot">=</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">200</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>mu_y_posterior <span class="ot">=</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="cf">function</span>(i) posterior[i,<span class="st">"a"</span>]<span class="sc">*</span>xseq<span class="sc">/</span>(posterior[i,<span class="st">"b"</span>] <span class="sc">+</span> xseq))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>mean_mu_y_posterior <span class="ot">=</span> <span class="fu">rowMeans</span>(mu_y_posterior)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>lower_mu_y_posterior <span class="ot">=</span> <span class="fu">apply</span>(mu_y_posterior, <span class="dv">1</span>, quantile, <span class="at">prob =</span> <span class="fl">0.025</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>upper_mu_y_posterior <span class="ot">=</span> <span class="fu">apply</span>(mu_y_posterior, <span class="dv">1</span>, quantile, <span class="at">prob =</span> <span class="fl">0.975</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And now we can visualize it:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xseq, mean_mu_y_posterior, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>),</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Prey density"</span>, <span class="at">ylab =</span> <span class="st">"Predation rate"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, lower_mu_y_posterior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, upper_mu_y_posterior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, mean_mu_y_prior, <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, lower_mu_y_prior, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xseq, upper_mu_y_prior, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(shapes2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you look hard you will see that the black lines are now plotted through the cloud of points and the uncertainty is quite small. Remember that this represents our uncertainty about the average predation rate and not individual values (that would require including the parameter <code>sd</code> too). The fact that our prior distribution was a bit off is not a problem as we made our prior uncertainty big enough to accommodate a wide range of possible responses.</p>
</section>
<section id="computing-dic" class="level3" data-number="7.2.4">
<h3 data-number="7.2.4" class="anchored" data-anchor-id="computing-dic"><span class="header-section-number">7.2.4</span> Computing DIC</h3>
<p>We can compute DIC. Remember that this requires two calculations:</p>
<ol type="1">
<li><p>Twice the negative log likelihood at the mean posterior estimate.</p></li>
<li><p>The same quantity but averaged over the posterior distribution.</p></li>
</ol>
<p>Let’s do the first one:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>mean_posterior <span class="ot">=</span> <span class="fu">colMeans</span>(posterior)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>term1 <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">nll</span>(<span class="at">a =</span> mean_posterior[<span class="dv">1</span>], <span class="at">b =</span> mean_posterior[<span class="dv">2</span>], <span class="at">sd =</span> mean_posterior[<span class="dv">3</span>], </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>             shapes2<span class="sc">$</span>x, shapes2<span class="sc">$</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The second one is more involved but since we already have a sample of values from the posterior we can use them directly to estimate the mean deviance (this is essentially the Monte Carlo method to calculate averages):</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>all_nlls <span class="ot">=</span> <span class="fu">apply</span>(posterior, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="dv">2</span><span class="sc">*</span><span class="fu">nll</span>(<span class="at">a =</span> x[<span class="dv">1</span>], <span class="at">b =</span> x[<span class="dv">2</span>], <span class="at">sd =</span> x[<span class="dv">3</span>],</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>                                                 <span class="at">x =</span> shapes2<span class="sc">$</span>x, <span class="at">y =</span> shapes2<span class="sc">$</span>y))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>term2 <span class="ot">=</span> <span class="fu">mean</span>(all_nlls)                                                 </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The effective number of parameters is the difference of <code>term2</code> and <code>term1</code>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pDIC <span class="ot">=</span> term2 <span class="sc">-</span> term1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice that this is slightly larger than 3. The Deviance Information Criterion then becomes:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>DIC <span class="ot">=</span> term1 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>pDIC</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the next chapter (where we will use Markov Chain Monte Carlo) we will also learn more modern information criteria that are are meant to replace DIC.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Fit the model <code>a*x^2/(b + x^2)</code> to the same dataset as the model above using Laplace’s approximation.</p></li>
<li><p>Compare this model and the previous model using DIC</p></li>
</ol>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="supplement-hints-for-choosing-deterministic-functions-and-stochastic-functions" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Supplement: hints for choosing deterministic functions and stochastic functions</h1>
<ol type="1">
<li>Deterministic functions</li>
</ol>
<ul>
<li>dataset 1</li>
</ul>
<p>light response curve. There are a number of options of functions to choose from, depending on the level of sophistication:</p>
<p><span class="math inline">\(\frac{ax}{(b+x)}\)</span>, <span class="math inline">\(a(1-e^{(-bx)})\)</span>, <span class="math inline">\(\frac{1}{2\theta}(\alpha I+p_{max}-\sqrt(\alpha I+p_{max})^2-4\theta I p_{max})\)</span> see page 98. A parameter <code>d</code> can be added in all cases to shift the curve up or down. The y represents net photosynthesis <span class="math inline">\(\mu mol CO_{2}/m^2s\)</span></p>
<ul>
<li>dataset 2</li>
</ul>
<p>The dataset describes a functional responses. Bolker mentions four of those</p>
<p><span class="math inline">\(\min(ax,s)\)</span> <span class="math inline">\(\frac{ax}{(b+x)}\)</span>, <span class="math inline">\(\frac{ax^2}{(b^2+x^2)}\)</span>,<span class="math inline">\(\frac{ax^2}{(b+cx+x^2)}\)</span></p>
<p>The y is measured in grams prey eaten per unit time.</p>
<ul>
<li><p>dataset 3 Allometric relationships generally have the form <span class="math inline">\(ax^b\)</span>. The y represent the total number of cones produced.</p></li>
<li><p>dataset 4 This could be logistic growth <span class="math inline">\(n(t)=\frac{K}{1+(\frac{K}{n_0})e^{-rt}}\)</span> or the gompertz function <span class="math inline">\(f(x)=e^{-ae^{-bx}}\)</span>. The y represent the population size (numbers).</p></li>
<li><p>dataset 5 What about a negative exponential? <span class="math inline">\(ae{-bx}\)</span> or a power function <span class="math inline">\(ax^b\)</span>. The y represent a number per unit area.</p></li>
<li><p>dataset 6 Species reponse curves are curves that describe the probability of presence as a function of some factor. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g. <span class="math inline">\(a e^{\frac{-(x-\mu)^2}{2\sigma^2}}\)</span>. The y represent presence or absence of the species (no units).</p></li>
</ul>
<ol start="2" type="1">
<li><p>Stochastic functions/Probability distributions</p>
<ul>
<li><p>dataset 1 y represents real numbers and both positive and negative numbers occur. This implies that we should choose a continuous probability distribution. In addition, the numbers seems unbound. Within the family of continuous probability distributions, the normal seems a good candidate distribution because this one runs from -<span class="math inline">\(\inf\)</span> to +<span class="math inline">\(\inf\)</span>. In contrast the Gamma and the Lognormal only can take positive numbers, so these distributions cannot handle the negative numbers. In addition, the beta distribution is not a good candidate because it runs from 0-1.</p></li>
<li><p>dataset 2 y represents real numbers and only positive numbers occur. The data represents a functional response (intake rate of the predator), and it is likely that you can only measure positive numbers (number of prey items per unit of time). This implies that we should choose a continuous probability distribution. Within the family of continuous probability distributions, the Gamma and the Lognormal could be taken as candidate distributions because they can only take positive numbers (beware that the Gamma cannot take 0). However, you could try to use a normal as well.</p></li>
<li><p>dataset 3 y seems represents counts (this is the cone dataset that is introduced in ch.&nbsp;6.). Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.</p></li>
<li><p>dataset 4 y represents population size over time. From looking at the data, they seems to represent counts. Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.</p></li>
<li><p>dataset 5 No information is given on y. The data clearly seems to represent counts. Thus the same reasoning applies here as to the two previous datasets.</p></li>
<li><p>dataset 6 The data (y) represents species occurences (presence/absence). The binomial model would be a good model to predict the probability of presence.</p></li>
</ul></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/AleMorales\.github\.io\/EcologicalModelsLabs\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
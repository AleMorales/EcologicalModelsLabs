---
title: "Lab 6 Fitting models to data"
author: "Ben Bolker, modified at several places by Bob Douma and Alejandro Morales"
date: "16 November 2022"
cache: true
visual: false
params:
  solution: false
filters:
  - custom-numbered-blocks
custom-numbered-blocks:
  groups:
    question:
      collapse: false
      colors: [00ff00, d9d9d9]
      label: "Exercise 6."
      boxstyle: foldbox.simple
    answer:
      collapse: false
      colors: [80bfff, d9d9d9]
      label: "Solution"
      boxstyle: foldbox.simple
  classes:
    Exercise:
      group: question
    Solution:
      group: answer
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Learning goals

You will learn how to:

1. Program the likelihood function of a model.

2. Estimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.

3. Estimate the confidence intervals of the model parameters through profiling and the quadratic approximation.

4. Estimate parameters in a Bayesian framework and how parameter uncertainty can be assessed

In case of time constraints, focus on sections 2-5. If you want something challenging do 6,7 and 8 as well.

# Fitting models to data
Fitting a model to data through likelihood requires that you take five steps:

1. Specify how the dependent variable depends on the independent variable, i.e. specify a function that describes how the mean of y depends on the value of x.
2. Specify a probability distribution to describe the deviations of the observations from the mean
3. Specify a function that calculate the negative log likelihood (NLL) based on the data and the parameter values.
4. Choose the parameters of the deterministic model and the probability model such that the negative log likelihood is lowest.
5. Compare the likelihood of alternative models (change the deterministic function or the stochastic function) and compare with AIC(c) or BIC which model is most parsimonious.


For example to calculate the NLL of a linear model and a normal distribution the following function works:

```{r}
nll = function(par,y,x){
  a = par[1]
  b = par[2]
  sd = par[3]
  # this calculates the mean y for a given value of x:
  #the deterministic function
  mu = a+b*x
  # this calculates the likelihood of the function given the probability
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
  return(nll)
}
```

Notice that the function takes three arguments: a vector with parameters, a vector with `y` values and a vector with `x` values. Inside the vector par, three values are stored: `a`,`b` and `sd`. Next, the mean given x is calculated with `mu=a+b*x`. The nll returns the Negative LogLikelihood of the data (`y`) given a normal distribution with mean `mu` (vector!) and a standard deviation `sd`. The `log=T` returns the log of the probability densities.

Next we call an optimisation function to find the maximum likelihood estimate

```{r, eval=F}
par=c(a=1,b=1,c=1) # initial parameters

# y represents the data, x the independent variable

opt1 = optim(par=par,nll,x=x,y=y,hessian=TRUE)
```

The optimization result is a list with
  elements:

  - the best-fit parameters (`opt1$par`, with parameter names because we named the
  elements of the starting vector---see how useful this is?);}

  - the minimum negative log-likelihood (`opt1$value`);

  - information on the number of function evaluations (`opt1$counts`; the `gradient` part is `NA` because we didn't specify a function to calculate the derivatives (and the Nelder-Mead algorithm wouldn't have used them anyway)

  - information on whether the algorithm thinks it found a good answer `opt1$convergence`, which is zero if `R` thinks everything worked and uses various numeric codes (see `?optim` for details) if something goes wrong;

  - `opt1$message` which may give further information about the when the fit converged or how it failed to converge;

  - because we set `hessian=TRUE`, we also get `opt1$hessian`, which gives the (finite difference approximation of)  the second derivatives evaluated at the MLE.

It can also be done through `mle2`
```{r}
nll.mle = function(a,b,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a+b*x
  # this calculates the likelihood of the function given the probability
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
  return(nll)
}

```

```{r, eval=F}
# the data should be supplied through data and the parameters through list().
mle2.1 = mle2(nll.mle,start=list(a=1,b=1,sd=1),data=data.frame(x,y))
summary(mle2.1)
```


# Fitting parameters of made-up data
The simplest thing to do to convince yourself that your attempts to estimate parameters are working is to simulate the ''data'' yourself and see if you get close to the right answers back. Set the random seed to 1001 so we get identical answers across r sessions.

## Finding the maximum likelihood estimate of the paramaters

::: Exercise
Take the steps below

1. Generate 50 values from a negative binomial (`rnbinom`) with $\mu=1$, $k=0.4$. Save the values in variables in case we want to use them again later.

2. Plot the numbers in a frequency diagram

3. Next, define the negative log-likelihood function for a simple draw from a negative binomial distribution:
the first parameter, `par`, will be the vector of parameters, and the second parameter, `dat`, will be the vector with simulated values.

4. Calculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Combine these parameter values into the vector `par` with  `c()` to pass them to the negative log-likelihood function. Naming the elements in the parameter vector is optional but can help avoid mistakes if the number o fparameters is large (e.g. `par = c(mu = 1,k = 2)`).

5. Calculate the NLL of parameter values that are far from the values that were used to generate the data ($\mu=10$, $k=10$)

6. Calculate the maximum likelihood estimate (MLE)?  Use `optim` with the default options (Nelder-Mead simplex method) and the method-of-moments estimates as the starting estimates (`par`):
  `opt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)`

7. What is the difference in NLL between the MLE estimates and the NLL derived at 5?

  The Likelihood Ratio Test would say, however, that the difference in likelihoods would have to be greater than $\chi^2_2(0.95)/2$ (two degrees of freedom because we are allowing both $\mu$ and $k$ to change). This can be done through `ldiff=nll.true-nll.mom` and `qchisq(0.95,df=2)/2`. So --- better, but not significantly better at $p=0.05$.
  `pchisq(2*ldiff,df=2,lower.tail=FALSE)` would tell us the exact $p$-value if we wanted to know.)
:::

<!-- The code below is a solution (to be hidden) but we need to execute the code as some of the variables created are used later on (maybe not all of it is needed, but I did not look into it in detail) -->
```{r, echo = FALSE, results = 'hide', warning = FALSE, message = FALSE}
set.seed(1001)
mu.true=1
k.true=0.4
x = rnbinom(50,mu=mu.true,size=k.true)
plot(table(factor(x,levels=0:max(x))),
   ylab="Frequency",xlab="x")
# this function calculate the NLL of the data given the set of parameters defined in p
NLLfun1 = function(p,dat=x) {
mu=p[1]
k=p[2]
-sum(dnbinom(x,mu=mu,size=k,log=TRUE))
}
# the NLL of the data given the parameter values that were used to generate the data
nll.true=NLLfun1(c(mu=mu.true,k=k.true))
nll.true
NLLfun1(c(mu=10,k=10))
m = mean(x)
v = var(x)
# calculate parameters through method of moments
mu.mom = m
k.mom = m/(v/m-1)
# find MLE estimate of the parameters given the data
opt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)
coef(opt1)
# NLL at MLE
opt1$value
# compare with nll.true
# significantly different?
ldiff=nll.true-opt1$value; ldiff
# no significant difference (which is what we would expect in 95% the generated datasets)
pchisq(2*ldiff,df=2,lower.tail=FALSE)
```

```{block2, echo = params$solution}
::: Solution

The solution is shown below in a big R chunk

        set.seed(1001)
        mu.true=1
        k.true=0.4

        x = rnbinom(50,mu=mu.true,size=k.true)

        plot(table(factor(x,levels=0:max(x))),
           ylab="Frequency",xlab="x")

        # this function calculate the NLL of the data given the set of parameters defined in p
        NLLfun1 = function(p,dat=x) {
        mu=p[1]
        k=p[2]
        -sum(dnbinom(x,mu=mu,size=k,log=TRUE))
        }

        # the NLL of the data given the parameter values that were used to generate the data
        nll.true=NLLfun1(c(mu=mu.true,k=k.true))

        nll.true

        NLLfun1(c(mu=10,k=10))

        m = mean(x)
        v = var(x)
        # calculate parameters through method of moments
        mu.mom = m
        k.mom = m/(v/m-1)

        # find MLE estimate of the parameters given the data
        opt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)
        coef(opt1)

        # NLL at MLE
        opt1$value
        # compare with nll.true

        # significantly different?
        ldiff=nll.true-opt1$value; ldiff

        # no significant difference (which is what we would expect in 95% the generated datasets)
        pchisq(2*ldiff,df=2,lower.tail=FALSE)

The minimum negative log-likelihood (`r round(opt1$value,2)`) is better than the NLL of the model with the true parameters (`r round(nll.true,2)`), but all of these are within the LRT cutoff, i.e. the negative log likelihoods differ by less than 1.92. Remember that the cut-off is based on the Likelihood Ratio Test that states that twice the difference in the log-likelihood between the simpler and more complex model will follow a $\chi^2$ distribution with n degrees of freedom. $n$ is the number of parameters that are fixed to a specific value. The cut-off value for a $\chi^2$ with 1 degree of freedom is 3.84. The value of 1.92 is derived from 3.84/2 because we evaluate the difference in log Likelihood and not twice the difference. In other words, we could also multiply all the logLikelihood surface by two and find the 3.84 cutoff.

:::
```



# Maximum likelihood and continuous covariates

The following exercise has the purpose to learn you how to fit a model to data when we have a single covariate.

::: Exercise
1. Take the second dataset (shapes2.csv from shapes.xlsx), use a michaelis-menten as deterministic function, and a normal distribution as stochastic model. Tweak the function in the first three grey boxes (above) such that it accomodates the michaelise menten and the normal distribution.

      _hint_: In a previous exercise you have eyeballed the parameter values of the functions, you can use these as starting       values.

      _hint_: In case you get convergence problems, further adapt your starting values, or choose a different optimizer. For       example Nelder-Mead is a robust one, e.g. `method = "Nelder-Mead"`.

2. Change the determinstic function for a possible alternative determinstic function, and fit this new model to the data. Remember that in Lab 3 you have proposed multiple deterministic functions for this dataset.
3. Compare the likelihoods of the data given both models
4. Apply model selection criteria and conclude which model fits that data best.
5. Does the model makes sense from a biological perspective?
:::

```{block2, echo = params$solution}
::: Solution

        shapes2= read.csv("shapes2.csv")
        plot(y~x, data= shapes2)
        nll.mle = function(a,b,sd){
          # this calculates the mean y for a given value of x: the deterministic function
          mu = (a*x)/(b+x)
          # this calculates the likelihood of the function given the probability
          # distribution, the data and mu and sd
          nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
          return(nll)
        }

Do maximum likelihood optimisation with mle2

        mle2.1 = mle2(nll.mle,start=list(a=20,b=10,sd=1), data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead")


Print summary of the optimisation

        summary(mle2.1)

Print maximum loglikelihood of the model

        logLik(mle2.1)

Add the curve with the parameters obtained through maximum likelihood estimates to the plot

        curve((coef(mle2.1)[1]*x)/(coef(mle2.1)[2]+x),add=T)

Now make another function with another deterministic model

        nll.mle.alt = function(a,b,sd){
            # this calculates the mean y for a given value of x: the deterministic function
            mu = (a*x^2)/(b+x^2)
            # this calculates the likelihood of the function given the probability
            # distribution, the data and mu and sd
            nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
            return(nll)
        }

        mle2.2 = mle2(nll.mle.alt,start=list(a=20,b=270,sd=1), data=data.frame(x=shapes2$x,y=shapes2$y),method="Nelder-Mead")
        summary(mle2.2)
        logLik(mle2.2)
        AIC(mle2.1,mle2.2)

The first model fits better according to AIC. The difference is about 9 points on the log Likelihood scale. So that implies that the first model makes the data exp(9) $\approx$ 8,000 times more likely!

:::
```


# Maximum likelihood with continous and categorical predictors

Sometimes you want to fit the same model to different groups (males/females, treatment/control etc.). The easiest way is to separately fit the model to the subsets, but this makes it very difficult to assess whether the fitted parameters for both groups are comparable. A more elegant method is explained below.

We use the fifth dataset of the six datasets you have worked with earlier on (shapes5.csv or the fifth sheet from shapes.xlsx). Assume that the function was generated by a decreasing exponential function $ae^{(-bx)}$ and you want to the values of $a$ and $b$. The dataset has three columns that are relevant: the independent variable $x$, the dependent variable $y$, and a dummy variable $group$ indicating to which group the observation belongs to. We want to test whether we can justify a different $a$ and $b$ for the two groups.

This is how the NLL function would look like assuming no grouping:

```{r,eval=F}
read.csv("shapes5.csv") # and select fifth dataset
# test dataset five for differences between groups
nll0 = function(par,dat){
  a = par[1]
  b = par[2]
  ymean = a*exp(-b*dat$x)
  nll = -sum(dpois(dat$y,lambda=ymean,log=T))
  return(nll)
}

par=c(4,0.2)
opt1 = optim(par=par,fn=nll0,dat=dat)

```

::: Exercise
1. Fit the above model to the data without considering differences between groups in $a$ and $b$.

2. Adjust the likelihood function such that it can accomodate for different values of $b$ depending on the group an observation belong to.

Use the following pseudocode to achieve this and/or check page 305 for in inspiration or go back to Lab 1 section 11.1.2.
    a. Adapt the likelihood function such that the parameter `b` depends on the group.
    b. Adjust the starting values so it contains multiple starting values for `b`

3. Estimate the parameters $a$ and $b$ when letting $b$ depend on the group. Compare the negative loglikelihood of this model with the model fitted in question 1. Which has a better fit?

4. Apply model selection techniques (Likelihood ratio test, AIC or BIC) to select the most parsimonious model. Are the models nested? Which model is preferred?
:::

```{block2, echo = params$solution}
::: Solution

        # test dataset five for differences between groups
        dat = data.frame(x,y,group)
        te = function(par,dat){
            a = par[1]
            b = par[2:3]
            ymean = a*exp(-b[dat$group]*dat$x)
            nll = -sum(dpois(dat$y,lambda=ymean,log=T))
            return(nll)
        }
        par=c(4,0.2,0.2)
        opt1 = optim(par=par,fn=te,dat=dat)

:::
```


::: Exercise
To practice model fitting a little bit more, you could repeat the above procedure for the other 4 datasets from shapes.xlsx.

Pick a dataset, go back to the Lab 3 Question 2.1 and Lab 4 Question 2.1 and list the stochastic model and the deterministic function and the eyeballed parameters that you thought were appropriate for this dataset. Next write a negative loglikelihood function, and use mle2 or optim to obtain the maximum likelihood estimates for the parameters.

If you have practised sufficiently, you can move on with the advanced topics below.
:::

# Advanced topics

## Likelihood surface
To find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues on  exercise #3.1 (Lab 3) where you used the negative binomial to generate 50 numbers and fitted back the parameters.

::: Exercise
For the likelihood surface:

1. Set up vectors of $\mu$ and $k$ values. Let's try $\mu$ from 0.4 to 3 in steps of 0.05 and $k$ from 0.01 to 0.7 in steps of 0.01.

2. Set up a matrix to hold the results,
The matrix for the results will have rows corresponding to $\mu$ and
columns corresponding to $k$:

3. Run `for` loops to calculate and store the values. Use a `for` nested in another one

4. Drawing a contour using the function 'contour'. Change the argument `nlevels` to 100 to get a better view of the likelihood surface

5. Add the MLE estimates in the contour plot (use 'points'). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.
:::

```{block2, echo = params$solution}
::: Solution

        muvec = seq(0.4,3,by=0.05)
        kvec = seq(0.01,0.7,by=0.01)
        resmat = matrix(nrow=length(muvec),ncol=length(kvec))
        for (i in 1:length(muvec)) {
          for (j in 1:length(kvec)) {
            resmat[i,j] = NLLfun1(c(muvec[i],kvec[j]))
          }
        }
        contour(muvec,kvec,resmat,xlab=expression(mu),ylab="k")
        contour(muvec,kvec,resmat,nlevels=100,lty=2,add=TRUE)

:::
```



# Hints for choosing deterministic functions and stochastic functions

  1. Deterministic functions

    - dataset 1

    light response curve. There are a number of options of functions to choose from, depending on the level of sophistication:

  $\frac{ax}{(b+x)}$, $a(1-e^{(-bx)})$, $\frac{1}{2\theta}(\alpha I+p_{max}-\sqrt(\alpha I+p_{max})^2-4\theta I p_{max})$ see page 98. A parameter `d` can be added in all cases to shift the curve up or down. The y represents net photosynthesis $\mu mol CO_{2}/m^2s$

    - dataset 2

  The dataset describes a functional responses. Bolker mentions four of those

  $\min(ax,s)$ $\frac{ax}{(b+x)}$, $\frac{ax^2}{(b^2+x^2)}$,$\frac{ax^2}{(b+cx+x^2)}$

  The y is measured in grams prey eaten per unit time.

    - dataset 3
  Allometric relationships generally have the form $ax^b$. The y represent the total number of cones produced.

    - dataset 4
  This could be logistic growth $n(t)=\frac{K}{1+(\frac{K}{n_0})e^{-rt}}$ or the gompertz function $f(x)=e^{-ae^{-bx}}$. The y represent the population size (numbers).

    - dataset 5
  What about a negative exponential? $ae{-bx}$ or a power function $ax^b$. The y represent a number per unit area.

    - dataset 6
  Species reponse curves are curves that describe the probability of presence as a function of some factor. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g.
  $a e^{\frac{-(x-\mu)^2}{2\sigma^2}}$. The y represent presence or absence of the species (no units).

  2. Stochastic functions/Probability distributions

      - dataset 1
  y represents real numbers and both positive and negative numbers occur. This implies that we should choose a continuous probability distribution. In addition, the numbers seems unbound. Within the family of continuous probability distributions, the normal seems a good candidate distribution because this one runs from -$\inf$ to +$\inf$. In contrast the Gamma and the Lognormal only can take positive numbers, so these distributions cannot handle the negative numbers. In addition, the beta distribution is not a good candidate because it runs from 0-1.

      - dataset 2
  y represents real numbers and only positive numbers occur. The data represents a functional response (intake rate of the predator), and it is likely that you can only measure positive numbers (number of prey items per unit of time).  This implies that we should choose a continuous probability distribution. Within the family of continuous probability distributions, the Gamma and the Lognormal could be taken as candidate distributions because they can only take positive numbers (beware that the Gamma cannot take 0). However, you could try to use a normal as well.

      - dataset 3
  y seems represents counts (this is the cone dataset that is introduced in ch. 6.). Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.

      - dataset 4
  y represents population size over time. From looking at the data, they seems to represent counts. Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.

      - dataset 5
  No information is given on y. The data clearly seems to represent counts. Thus the same reasoning applies here as to the two previous datasets.

      - dataset 6
  The data (y) represents species occurences (presence/absence). The binomial model would be a good model to predict the probability of presence.

[
  {
    "objectID": "Supplements/Stan/material.html",
    "href": "Supplements/Stan/material.html",
    "title": "Supplement: Using Stan",
    "section": "",
    "text": "1 Learning goals\nYou will learn how to:\n\nBuild programs for the Stan language\nRun Laplace’s approximation with Stan.\nRun Hamiltonian Monte Carlo with Stan\n\n\n\n2 Why Stan?\nStan is considered the golden standard for Bayesian inference nowadays. It is a implemented in C++ meaning that it cannot work with models written in R, so we will need to write the models in a separate language and file, but we can run the setup and analysis of the posterior samples in R.\nThe usual experience is that writing models in Stan can be a bit harder because of the stricter syntax of C++. Also, users may find it frustrating that when we want to run the model we need to wait for the model to compile, but once it compiles, it runs extremely fast! For simple models such as the one in Lab 6 and 7, Stan does not offer much advantage (except that the samples from the posterior may be of higher quality due to the superiority of Hamiltonian Monte Carlo). But when it comes to larger, multilevel models (e.g., the problems in Lab 10) Stan is really the tool to go. Also, the package brms (see Supplement on formulate-based methods) has made Bayesian statistics much more accessible to the average R user, but this package is just a way to generate Stan code.\n\n\n3 Installation of Stan and Rtools\nYou need to be able to compile C++ code in your computer. If you are using Windows, please install Rtools: https://cran.r-project.org/bin/windows/Rtools/rtools45/rtools.html\nThen you need to install the package RStan. Please go to https://github.com/stan-dev/rstan/wiki/Rstan-Getting-Started and follow the instructions there. Make sure that you test the installation to check that everything is correct.\n\n\n4 Analyzing shapes2 using Laplace’s approximation\nWe are going to analyze the shapes2 dataset again with Laplace’s approximation as in Lab 6. Remember that we want to fit a Michaelis-Menten model (\\(y = ax/(b + x)\\)) and we came up with the following priors:\n\\[\n\\begin{align*}\na &\\sim \\text{Half-Normal}(\\mu=50,\\sigma=50) \\\\\nb &\\sim \\text{Half-Normal}(\\mu=50,\\sigma=100) \\\\\n\\sigma &\\sim \\text{Half-Normal}(\\mu=0,\\sigma=10) \\\\\n\\end{align*}\n\\]\nWhere Half-Normal refers to the fact that all the parameters were assumed to be positive.\nStan models need to be built in their own files and are made of different blocks. You have been given the Stan file for this first exercise but please try to understand how it is structured based on the description before as you will not be given the solutions for posterior exercises. This Youtube video may also help you: https://youtu.be/YZZSYIx1-mw?si=5nSuDzOV8lrMKyCv\nA Stan model is composed of different blocks. First, we need to specify the data that will be used:\ndata {\n  int&lt;lower=0&gt; N;\n  vector&lt;lower=0&gt;[N] x;\n  vector&lt;lower=0&gt;[N] y;\n  vector[6] hp;\n}\nHere we specify the length of the data N as a positive integer int&lt;lower=0&gt;. The two variables in the model (x and y) are defined as vectors of positive (real) values (vector&lt;lower=0&gt;) and length N. Finally, there are six values passed along the vector hp which will be used to parameterize the prior distributions (in case we want to try different priors!).\nThe next block represents the parameters of the model that will be estimated:\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\nThese are the three parameters of our likelihood function (a and b for the deterministic model and sigma for the error) which are all assumed to be positive.\nFinally we have the model block, that actually implements the priors and likelihood. There are two ways to implement thids in Stan, either using the probabilistic modelling approach (which resembles the BUGS code in the book) or by adding up log probability densities (which resembles the way we build the negative log-likelihood functions).\nThe probabilistic approach looks as follows:\nmodel {\n  a     ~ normal(hp[1], hp[2]);\n  b     ~ normal(hp[3], hp[4]);\n  sigma ~ normal(hp[5], hp[6]);\n  y ~ normal(a*x./(b + x), sigma);\n}\nThe first three lines indicate that the parameters a, b and sigma follow normal distributions with hyperparameters take from the vector hp. The last line indicates that the observations y follow a normal distribution which mean is given by the deterministic model a*x./(b + x) and the standard deviation is sigma. Notice the . before /. This is to signal that I am making this calculation for multiple values of x (by default C++ cannot apply element-wise calculations on vectors like R, Stan extends some of these capabilities).\nThe alternative approach looks like this:\nmodel {\n  target += normal_lpdf(a | hp[1], hp[2]);\n  target += normal_lpdf(b | hp[3], hp[4]);\n  target += normal_lpdf(sigma | hp[5], hp[6]);\n  target += normal_lpdf(y | a*x./(b + x), sigma);  \n}\nWhere target is a special variable in Stan that accumulates the log-likelihood and log-prior, normal_lpdf stands for the log probability density of the normal distribution and | is the symbol for conditionality (on the left of | we indicate the random variable and on the right other parameters of the distribution).\nWe can compile and load the model as follows (here I assume the model is in a file called mm.stan in the local folder).\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores()) # To enable parallel chains\nrstan_options(auto_write = TRUE) # To avoid recompilation\nmm_model = stan_model(file = \"mm.stan\")\nThis took a while because the Stan model needs to be compiled to machine code that can then be executed. C++ compilation can be quite slow so please be patient (it will pay off later, trust me).\nWe are going to fit this model the data using Laplace’s approximation. This approximates the posterior distribution as a Normal distribution with a mean equal to the maximum posterior estimates and variance derived from the Hessian matrix. Mathematically this is equivalent to maximum likelihood + quadratic approximation (section 6.5 in the book) but with prior distributions. We can do this in Stan using optimizing():\ndata = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, \n            hp = c(50, 50, 50, 100, 0, 10)) # see above for priors\nmm_fit = optimizing(mm_model, data = data, init = list(a = 50, b = 40, sigma = 1),\n                    hessian = TRUE)\nWe can retrieve the estimated mode of the posterior:\nmm_fit$par\nAnd we can use the Hessian to compute a variance-covariance matrix:\nV = MASS::ginv(-mm_fit$hessian)\nAnd now our posterior is defined as a multivariate normal with mu = mm_fit$par and Sigma = V. We can generate samples using mvrnorm:\nposterior_samples = MASS::mvrnorm(1000, mu = mm_fit$par, Sigma = V)\nWe can directly visualize the marginal distribution from this sample:\npar(mfrow = c(1,2))\nhist(posterior_samples[,\"a\"], main = \"\", xlab = \"a\", prob = T)\nhist(posterior_samples[,\"b\"], main = \"\", xlab = \"b\", prob = T)\nBut also the correlation among these two parameters:\nplot(posterior_samples[,1:2], xlab = \"a\", ylab = \"b\")\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFit the model a*x^2/(b + x^2) to the same dataset as the model above using Laplace’s approximation and Stan.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe stan model will look as follows (I use the probabilistic flavour because it is a bit easier to write down)\ndata {\n  int&lt;lower=0&gt; N;\n  vector&lt;lower=0&gt;[N] x;\n  vector&lt;lower=0&gt;[N] y;\n  vector[6] hp;\n}\n\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  a     ~ normal(hp[1], hp[2]);\n  b     ~ normal(hp[3], hp[4]);\n  sigma ~ normal(hp[5], hp[6]);\n  y ~ normal(a*pow(x, 2)./(b + pow(x, 2)), sigma);\n}\nNotice than in C++, x^2 needs to be written as pow(x, 2).\nI can adjust the priors based on the new scale (no a is in the scale of y but b is in the scale of x)\nb = rtruncnorm(N, mean = 10000/2, sd = 10000, a = 0) # b scales with half prey density\nmu_y_prior = sapply(1:N, function(i) a[i]*xseq^2/(b[i] + xseq^2))\nmean_mu_y_prior = rowMeans(mu_y_prior)\nlower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)\nupper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)\nplot(xseq, mean_mu_y_prior, type = \"l\", ylim = c(0, 100))\nlines(xseq, lower_mu_y_prior, lty = 2)\nlines(xseq, upper_mu_y_prior, lty = 2)\npoints(shapes2)\nWe then load the new model and run Laplace’s approximation:\nmm2_model = stan_model(file = \"./Lab6/mm2.stan\")\ndata = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, \n            hp = c(50, 50, 100, 200, 0, 10)) # see above for priors\nmm2_fit = optimizing(mm2_model, data = data, init = list(a = 50, b = 4000, sigma = 1),\n                    hessian = TRUE)\nAnd I can retrieve the parameter values as I did before\nV = MASS::ginv(-mm2_fit$hessian)\nposterior_samples = MASS::mvrnorm(1000, mu = mm2_fit$par, Sigma = V)\npar(mfrow = c(1,3))\nhist(posterior_samples[,\"a\"], main = \"\", xlab = \"a\", prob = T)\nhist(posterior_samples[,\"b\"], main = \"\", xlab = \"b\", prob = T)\nplot(posterior_samples[,1:2], xlab = \"a\", ylab = \"b\")\n\n\n\n\n\n\n\n\n5 Analyzing shapes2 using Hamiltonian Monte Carlo\nThe software Stan implements a different type of MCMC algorithm to the one described in Bolker’s book. It uses an algorithm called Hamiltonian Monte Carlo that has a smart way to generate candidate points in the posterior distribution that lead to a much more efficient sampling (less autocorrelation, much faster adaptation, etc). We will not learn the details of the algorithm (is much more complex that basic Metropolis, an easy-ish introduction is here: https://www.youtube.com/watch?v=ZGtezhDaSpM), instead we will focus on how to assess the results.\nIn the previosu section we created a model for the predation data (shapes2). We will modify that model to keep track of the log-likelihood for the different parameter combinations (we need it for model comparison later). The original code was as follows\ndata {\n  int&lt;lower=0&gt; N;\n  vector&lt;lower=0&gt;[N] x;\n  vector&lt;lower=0&gt;[N] y;\n  vector[6] hp;\n}\n\n\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\n\n\nmodel {\n  a     ~ normal(hp[1], hp[2]);\n  b     ~ normal(hp[3], hp[4]);\n  sigma ~ normal(hp[5], hp[6]);\n  y ~ normal(a*x./(b + x), sigma);\n}\nYou need to add an extra block that computes the log-likelihood for each parameter value (we will use this for modern model comparison indices):\ngenerated quantities {\n  vector[N] log_lik;\n  for(n in N) log_lik[n] = normal_lpdf(y[n] | a*x[n]/(b + x[n]), sigma);\n}\nThis can be added at the end of the file with the model implementation. Notice that inside the generated quantities block you can only use the normal_lpdf style of programming (not the y ~ normal style).\nWe can compile and load the model as follows:\nlibrary(rstan)\nmm_model = stan_model(file = \"mm_ll.stan\")\nNow, instead of applying Laplace’s approximation we will generate the samples using HMC. We can achieve this with the function sampling (assuming you have loaded the shapes2 data:\ndata = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, \n            hp = c(50, 50, 50, 100, 0, 10)) # see above for priors\noptions(mc.cores = 4) # Adjust to the number of cores you have\nmm_fit = sampling(mm_model, data = data, cores = 4, chains = 4,\n                   warmup = 1000, iter = 4000)\nIn this called to sampling() we are running 4 chains in parallel ncores = 4 and nchains = 4. Each chain will first run for 1000 iterations to adapt (warmup is equivalent to burn-in) and then 3000 more to fill up the iter = 4000. This means we get in total 12000 samples from posterior across 4 independent chains.\nWe can look at summaries of the samples with the usual function. You should specify the parameters you want out (otherwise you get all the log_lik estimates printed:\nsummary(mm_fit, pars = c(\"a\", \"b\", \"sigma\"))[[1]]\nStan is calculate the mean of each marginal posterior distribution (one per parameter). It also gives several quantiles, an estimate of the effective sample size (corrected for autocorrelation) and the \\(\\hat{R}\\) (Rhat) statistic (Bolker calls it Gelman-Rubin diagnostic). This statistic should be as close to 1 as possible, with values about 1.2 indicating a quite bad convergence (the results we get here are pretty much perfect).\nWhen you get an \\(\\hat{R}\\) of 1.00, there is no much need to look at the traces of the chains, but if you want to you can still do it:\ntraceplot(mm_fit, pars = c(\"a\", \"b\", \"sigma\"))\nWe can see that the four chains are well mixed for all parameters and they look like white noise. If you want to process the samples your self, you can convert the Stan object into a data.frame\nposterior_samples = as.data.frame(mm_fit, pars = c(\"a\", \"b\", \"sigma\"))\nAnd now we can reproduce the plots we made in the previous section\npar(mfrow = c(1,3))\nhist(posterior_samples[,\"a\"], main = \"\", xlab = \"a\", prob = T)\nhist(posterior_samples[,\"b\"], main = \"\", xlab = \"b\", prob = T)\nplot(posterior_samples[,1:2], xlab = \"a\", ylab = \"b\")\nNote that the uncertainty in the parameter b is much larger than what we estimated with Laplace’s approximation. This is indeed the problem of using quadratic approximation to estimate uncertainty. It does not matter whether we are using in a Bayesian context or for maximum likelihood, in both cases we often underestimate uncertainty.\n\n5.0.1 Modern Bayesian information criteria\nFinally, we can compute information criteria for model comparison. Bolker mentions DIC but this criterion has by now been superseded by two more recent approaches. The first one is WAIC which is a Bayesian generalization of AIC. The other one is an estimate of leave-one-out cross-validation (that can be done in a Bayesian context and is much faster than actual cross-validation). The package loo implements these indices:\nlibrary(loo)\n# Extract the log-likelihoods from the fit\nlog_lik = extract_log_lik(mm_fit)\n# Compute the WAIC\nwaic_mm = waic(log_lik)\n# Compute the loo IC\nloo_mm = loo(log_lik)\nNotice that there were some warnings, they refer to one data point (2% of the data) that is probably a bit of an outlier and this can have an influence on any information criterion. The loo criteria is however more robust (and in any casem if it is only one point we should worry too much).\nwaic_mm\nloo_mm\nThey give practically the same result. Sometimes they will not and in that case the recommendation is to use loo instead of waic. This will tend to happen with fewer datapoints and/or more complex models. There are a lot of numbers there:\n\nelpd_ is the average predictive log-likelihood (so it is already penalized by model complexity).\np_ is the effective degrees of freedom we would need to compute elpd from our sample. In AIC this would \\(2k = 6\\). The reason why this number is smaller than 6 in a Bayesian prior is because prior information constrains the model, so the effective number of degrees of freedom is lower than the number of parameters being estimated. The effective degrees of freedom will also depend on the sample size (the more data we have, the less informative the priors become).\nwaic and looic is just -2 times elpd so that it is expressed in the same units as AIC or BIC.\n\nYou can use WAIC or LOO-IC in a similar way to how you use AIC or BIC. Notice that we also get a standard error SE for the index, so you may also use that information for model comparison (i.e., if the WAIC or two models differ by more than twice the SE, you can assume a difference in predictive power between the models).\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using WAIC or LOO-IC\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe stan model will look as follows (I use the probabilistic flavour because it is a bit easier to write down)\ndata {\n  int&lt;lower=0&gt; N;\n  vector&lt;lower=0&gt;[N] x;\n  vector&lt;lower=0&gt;[N] y;\n  vector[6] hp;\n}\n\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  a     ~ normal(hp[1], hp[2]);\n  b     ~ normal(hp[3], hp[4]);\n  sigma ~ normal(hp[5], hp[6]);\n  y ~ normal(a*pow(x, 2)./(b + pow(x, 2)), sigma);\n}\n\ngenerated quantities {\n  vector[N] log_lik;\n  for(n in 1:N) \n    log_lik[n] = normal_lpdf(y[n] | a*pow(x[n],2)/(b + pow(x[n],2)), sigma);\n}\nWe then load the new model and run HMC on it:\nmm2_model = stan_model(file = \"./Lab7/mm2_ll.stan\")\ndata = list(N = nrow(shapes2), x = shapes2$x, y = shapes2$y, \n            hp = c(50, 50, 100, 200, 0, 10)) # see above for priors\nmm2_fit = sampling(mm2_model, data = data, cores = 4, chains = 4,\n                   warmup = 1000, iter = 4000)\nAnd I can retrieve the parameter values as I did before\nsummary(mm2_fit, pars = c(\"a\", \"b\", \"sigma\"))[[1]]\nAnd visualize them:\nposterior_samples = as.data.frame(mm2_fit, pars = c(\"a\", \"b\", \"sigma\"))\npar(mfrow = c(1,3))\nhist(posterior_samples[,\"a\"], main = \"\", xlab = \"a\", prob = T)\nhist(posterior_samples[,\"b\"], main = \"\", xlab = \"b\", prob = T)\nplot(posterior_samples[,1:2], xlab = \"a\", ylab = \"b\")\nTo compare with the other model we should compute cross-validation also without data point 7:\nlibrary(loo)\n# Extract the log-likelihoods from the fit\nlog_lik = extract_log_lik(mm2_fit)[,-7]\n# Compute the WAIC\nwaic_mm2 = waic(log_lik)\nwaic_mm2\nOk, there seems to be another suspect, let’s check with loo\nloo_mm2 = loo(log_lik)\nloo_mm2\nWe do not get warnings so we should be safe. We can now compare the two models.\nrbind(loo_mm$estimates[\"looic\",],\n      loo_mm2$estimates[\"looic\",])\nWe can see that the first model we tried was much better than the second model even when accounting for the standard error in the estimation. We can use the loo_compare function, which compares elpd values (i.e., without multiplying by -2, so bigger is better):\nloo_compare(loo_mm, loo_mm2)"
  },
  {
    "objectID": "Supplements/canned.html",
    "href": "Supplements/canned.html",
    "title": "Supplement: Formula-based methods",
    "section": "",
    "text": "Here I exemplify several typical statistical models with an example of each. For each type of model I provide:\n\nMathematical description\nFunction implementing negative log likelihood\nCanned method that relies on formula\n\nThere are functions that allow specifying different types of models with formulae and exist for both Maximum Likelihood and Bayesian methods. In the case of maximum likelihood methods, they often implement methods of estimation that are specialized to a given class of model and are therefore more efficient than using non-linear optimization.\nThe formulae that are used to specify models can be slightly different for each package, but there are some common elements that are useful to know and I described in the section below. For possible extensions, read the documentation of each individual package or function.\nI list different possible options below, for each type of model, starting with the simplest (linear models) up to the most general (non-linear mixed models with non-Normal responses).",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#stats",
    "href": "Supplements/canned.html#stats",
    "title": "Supplement: Formula-based methods",
    "section": "3.1 stats",
    "text": "3.1 stats\nOne of the packages hat comes with R pre-installed, allows building models with only one stochastic component. The functions in introduces are:\n\nlm: Linear models.\n\nglm: Generalized linear models.\n\nnls: Non-linear models.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#stats4",
    "href": "Supplements/canned.html#stats4",
    "title": "Supplement: Formula-based methods",
    "section": "3.2 stats4",
    "text": "3.2 stats4\nAllows for general maximum likelihood, using both formulae and user-defined functions for the negative log-likelihood. It introduces the mle function. This allows specifying all the models in the package stats plus all other forms of non-linear and generalized non-linear models.\nNote: Here we focus on the formula version of this function. If you implement your own negative log-likelihood function you can fit any more in this document as shown in the course.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#bbmle",
    "href": "Supplements/canned.html#bbmle",
    "title": "Supplement: Formula-based methods",
    "section": "3.3 bbmle",
    "text": "3.3 bbmle\nThe MLE package by Benjamin Bolker, developed in parallel with the book. This package propposes the function mle2 that replaces the mle function in stats4 with similar functionality.\nNote: Here we focus on the formula version of this function. If you implement your own negative log-likelihood function you can fit any more in this document as shown in the course.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#nlme",
    "href": "Supplements/canned.html#nlme",
    "title": "Supplement: Formula-based methods",
    "section": "3.4 nlme",
    "text": "3.4 nlme\nPackage for linear and non-linear mixed effect models and generalized least squares. This is the original package developed by Pinheiro and Bates and associated to their mixed model book and has been one of the most popular packages for mixed effects models in R for a long time. The functions it introduces are:\n\ngls: Linear models with generalized least squares.\n\ngnls: Non-linear models with generalized least squares.\n\nmle: Linear mixed effect models.\n\nnlme: Non-linear mixed effect models.\n\nThe package also contains many functions for modeling the variance of the response as a function of groups and covariates (see ?varClasses) and also the correlation structure of residuals (see ?corClasses).",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#lme4",
    "href": "Supplements/canned.html#lme4",
    "title": "Supplement: Formula-based methods",
    "section": "3.5 lme4",
    "text": "3.5 lme4\nPackage for linear, generalized linear and non-linear mixed effect models (the second big package by Bates, currently maintained by Benjamin Bolker). The functions it introduces are:\n\nlmer: Linear mixed effect models.\n\nglmer: Generalized linear mixed effect models.\n\nnlmer: Non-linear mixed effect models.\n\nUnlike nlme, the package lme4 does not allow modelling the varaince in the response or correlation structure of residuals.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#glmmtmb",
    "href": "Supplements/canned.html#glmmtmb",
    "title": "Supplement: Formula-based methods",
    "section": "3.6 glmmTMB",
    "text": "3.6 glmmTMB\nPackage for linear, generalized and non-linear (mixed) models. Unlike previous package, this one allows random effects as optional. Developed and maintaned by Mollie Brooks, also with contributions by Bolker. This package builds on top of TMB (a package for defining models in C++). It offers a single but very versatile glmmTMB function that will automatically detect the correct type of model from the formula. Like nlme it allows modelling the correlation structure of the residuals, but not the variance or scale parameter.\nRecently, they added the possibility of defining priors. This allows performing Maximum a Posterior Estimation (see example in Chapter 6) but also to run the model in Stan that implements Hamiltonian Monte Carlo for Bayesian inference. The system for specifying priors has been take from the package brms (see below).",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#rstanarm",
    "href": "Supplements/canned.html#rstanarm",
    "title": "Supplement: Formula-based methods",
    "section": "3.7 rstanarm",
    "text": "3.7 rstanarm\nPackage for Bayesian estimation of linear and generalized (mixed) models. It is built on top of Stan but all the C++ code is precompiled and heavily optimized, so it is a very efficient tool for quick Bayesian modeling. The functions it offers are:\n\nstan_lm: Linear models.\n\nstan_glm: Generalized linear models.\n\nstan_lmer: Linear mixed models as in lmer.\n\nstan_glmer: Generalized linear mixed models as in glmer.\n\nstan_nlmer: Non-linear mixed models as in nlmer but only works with the basic self-starting functions (see below for relevant section).",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#brms",
    "href": "Supplements/canned.html#brms",
    "title": "Supplement: Formula-based methods",
    "section": "3.8 brms",
    "text": "3.8 brms\nPackage for Bayesian estimate that covers all the types of models described in this document. It is built on top of Stan but unlike rstanarm, it will generate ad-hoc Stan code based on the model formulated by the user. This means that the user will have to pay the cost of compiling the model (which can often exceed the time it takes to generate the samples) but this brings an enormous flexibility to the user. The only limitation (like all other packages in this document) is that random effects must be described with Normal distribution, even though it is technically possible to fit models in Stan with non-Normal random effects. The generated code may also not be the most optimal.\nThe package offers a single function brm with a very flexible formula system (it is also useful to know the function bf to specify more complex formula).",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#linear-model",
    "href": "Supplements/canned.html#linear-model",
    "title": "Supplement: Formula-based methods",
    "section": "4.1 Linear model",
    "text": "4.1 Linear model\nStochastic model: A single level assumed Normal. Mean response: Linear function of covariates. Variance response: Assumed constant.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nBase\nlm\n\n\nLikelihood\nstats4\nmle\n\n\nLikelihood\nbbmle\nmle2\n\n\nBoth\nglmmTMB\nglmmTMB\n\n\nBayesian\nrstanarm\nstan_lm\n\n\nBayesian\nbrms\nbrm\n\n\n\nExample withe categorical predictor\nA linear model with a categorical predictor with 3 levels.\nData simulation:\nset.seed(1234)\nsd  = 1\nmu0 = 1\ndeltas = c(4, 1.5)\nmus = c(mu0, mu0 + deltas)\nn   = 5\ngroups = rep(1L:3L, each = n)\ny   = rnorm(3*n, rep(mus, each = n), sd = sd)\ndata = data.frame(y = y, groups = groups, fgroups = as.factor(groups))\nExplicit maximum likelihood:\nlibrary(bbmle)\nNLL = function(mu0, delta1, delta2, lsd, groups, y) {\n  mus = c(mu0, mu0 + delta1, mu0 + delta2)[groups]\n  -sum(dnorm(y, mean = mus, sd = exp(lsd), log = TRUE))\n}\nmle2(minuslogl = NLL, start = list(mu0 = 1, delta1 = 0, delta2 = 0, lsd = log(1)),\n     data = data)\nWith lm:\nlm(y~fgroups, data = data)\nWith mle2:\nlibrary(bbmle)\nmle2(y~dnorm(mean = mu, sd = exp(lsd)), parameters = list(mu ~ fgroups),\n           start = list(mu = 1, lsd = log(1)), data = data)\nWith glmmTMB:\nlibrary(glmmTMB)\nglmmTMB(y~fgroups, family = gaussian(), data = data)\nWith rstanarm:\nlibrary(rstanarm)\nstan_lm(y~fgroups, data = data, prior = NULL)\nWith brm\nlibrary(brms)\nbrm(y~fgroups, data = data, family = gaussian())\nExample with continuous predictor\nA simple linear regression.\nData simulation:\nset.seed(1234)\na   = 0\nb   = 1\nsd  = 1\nn   = 5\nx   = rep(1:10, each = 5)\ny   = rnorm(50, a + b*x, sd = sd)\ndata = data.frame(y = y, x = x)\nNegative log-likelihood:\nNLL = function(mu1, mu2, mu3, sd, groups, y) {\n  mus = c(mu1, mu2, mu3)[groups]\n  -sum(dnorm(y, mean = mus, sd = sd, log = TRUE))\n}\nWith lm:\nlm(y~fgroups, data = data)\nWith mle2:\nlibrary(bbmle)\nmle2(y~dnorm(mean = mu, sd = sd), parameters = list(mu ~ fgroups),\n           start = list(mu = 3, sd = 1), data = data)\nWith glmmTMB:\nlibrary(glmmTMB)\nglmmTMB(y~fgroups, family = gaussian(), data = data)\nWith rstanarm:\nlibrary(rstanarm)\nstan_lm(y~fgroups, data = data, prior = NULL)\nWith brm\nlibrary(brms)\nbrm(y~fgroups, data = data, family = gaussian())",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#generalized-least-squares",
    "href": "Supplements/canned.html#generalized-least-squares",
    "title": "Supplement: Formula-based methods",
    "section": "4.2 Generalized least squares",
    "text": "4.2 Generalized least squares\nStochastic model: A single level assumed Normal. Mean response: Linear function of covariates. Variance response: Non-linear function of covariates and/or correlation structure.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nnlme\ngls\n\n\nLikelihood\nstats4\nmle\n\n\nLikelihood\nbbmme\nmle2\n\n\nBayesian\nbrms\nbrm\n\n\n\nExamples\nA linear model with variance increasing exponentially with covariate.\nData simulation:\nset.seed(1234)\na   = 0\nb   = 1\nc   = 0.05\nn   = 5\nx   = rep(1:10, each = 5)\ny   = rnorm(50, a + b*x, sd = sqrt(exp(2*c*x)))\ndata = data.frame(y = y, x = x)\nNegative log-likelihood:\nNLL = function(a, b, c, x, y) {\n  mus = a + b*x\n  sd  = sqrt(exp(2*c*x))\n  -sum(dnorm(y, mean = mus, sd = sd, log = TRUE))\n}\nWith gls:\nsummary(gls(y~x, weights = varExp(form = ~x), data = data, method = \"ML\"))\nWith mle2:\n\nWith brm:\nbrm(bf(y~x, sigma~exp(x)), data = data, family = gaussian())\nNotes: The function gls allows only specific functions",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#linear-mixed-model",
    "href": "Supplements/canned.html#linear-mixed-model",
    "title": "Supplement: Formula-based methods",
    "section": "4.3 Linear mixed model",
    "text": "4.3 Linear mixed model\nStochastic model: Multiple levels assumed Normal. Mean response: Linear function of covariates. Variance response: Non-linear function of covariates and/or correlation structure.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nnlme\nlme\n\n\nLikelihood\nlme4\nlmer\n\n\nBoth\nglmmTMB\nglmmTMB\n\n\nBayesian\nrstanarm\nstan_lmer\n\n\nBayesian\nbrms\nbrm\n\n\n\nNote that lmer and stan_lmer allow for some correlation structure in residuals but do not allow modelling the variance itself. For that you need to use lme or brm.\nExamples # Non-linear responses\nModels where the mean is modeled by a non-linear function of covariates.\nCanned only support non-linear models that can be expressed as a formula so they must be quite simple. In Likelihood versions, it is important to choose good initial values as otherwise the algorithms will not converge. For that reason it is common to use self-starting functions that already specify a model and an internal algorithm to come up with good starting values (these are compatible with nls and gnls.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#on-self-starting-functions",
    "href": "Supplements/canned.html#on-self-starting-functions",
    "title": "Supplement: Formula-based methods",
    "section": "4.4 On self-starting functions",
    "text": "4.4 On self-starting functions\nFor non-linear models it is very important to have good initial estimates of parameters. R has the concept of self-starting models. A self-starting model implements a particular model and a procedure to “eye-ball” the initial values:\n\nPackage stats provides SSasymp, SSasympOff, SSasympOrig, SSbiexp, SSfol, SSfpl, SSgompertz, SSlogis, SSmicmen, SSweibull\nThe package nlraa provides 28 new self-starting functions that are used in agricultural research but could be useful in more general ecological applications.\nThe package vega provides 4 new self-starting functions to model the relationship between species richness and area\n\nSee this blog post on how to write your own self-starting functions: https://www.statforbiology.com/2020/stat_nls_selfstarting/",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#non-linear-model",
    "href": "Supplements/canned.html#non-linear-model",
    "title": "Supplement: Formula-based methods",
    "section": "4.5 Non-linear model",
    "text": "4.5 Non-linear model\nStochastic model: A single level, assumed Normal. Mean response: Non-linear function of covariates. Variance response: Assumed constant (but see note).\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nBase\nnls\n\n\nLikelihood\nnlme1,2\ngnls\n\n\nLikelihood\nstats4\nmle\n\n\nLikelihood\nbbmle2\nmle2\n\n\nBayesian\nbrms1,2\nbrm\n\n\n\n1 Correlation structure for residuals 2 Variance can be modeled as a non-linear function of covariates.\nExample with constant variance\nA Michaelis-Menten model:\nset.seed(1234)\nsd  = 1\na   = 10\nb   = 3\nx   = 1:10\ny   = rnorm(10, mean = a*x/(b + x), sd = sd)\ndata = data.frame(y = y, x = x)\nWith nls\nfit = nls(y~a*x/(b + x), start = list(a = 5, b = 2), data = data)\nWith mle2\nfit = mle2(y~dnorm(mean = a*x/(b + x), sd = sd), start = list(a = 5, b = 2, sd = 1), data = data)\nWith brm\nsummary(brm(y~a*x/(b + x), data = data))\nExample with increasing variance\nA Michaelis-Menten model with error increasing exponential with predictor\nfit = gnls(y~a*x/(b + x), start = list(a = 5, b = 2), \n             weights = varExp(form = ~x), data = data)",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#non-linear-mixed-models",
    "href": "Supplements/canned.html#non-linear-mixed-models",
    "title": "Supplement: Formula-based methods",
    "section": "4.6 Non-linear mixed models",
    "text": "4.6 Non-linear mixed models\nStochastic model: Multiple levels assumed Normal. Mean response: Linear function of covariates after link transformation. Variance response: Correlation structure for residuals and variance can be modeled as non-linear function of covariates.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nnlme\nnlme\n\n\nBayesian\nbrms\nbrm",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#generalized-linear-models",
    "href": "Supplements/canned.html#generalized-linear-models",
    "title": "Supplement: Formula-based methods",
    "section": "5.1 Generalized linear models",
    "text": "5.1 Generalized linear models\nStochastic model: A single level of one of the pre-specified distributions. Mean response: Linear function of covariates after link transformation. Variance response: Scale parameter (if present) assumed constant (but see note).\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nBase\nglm\n\n\nBoth\nglmmTMB1\nglmmTMB\n\n\nBayesian\nstanarm\nstan_glm\n\n\nBayesian\nbrms1,2\nbrm\n\n\n\n1 Correlation structure for residuals 2 Scale parameter can be modeled as non-linear function of covariates.\nExample\nAn Poisson model with an log link function\nWith glm\nfit = glm(y~x, family = poisson(link = \"log\"), data = data)",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#generalized-linear-mixed-models",
    "href": "Supplements/canned.html#generalized-linear-mixed-models",
    "title": "Supplement: Formula-based methods",
    "section": "5.2 Generalized linear mixed models",
    "text": "5.2 Generalized linear mixed models\nStochastic model: Multiple levels. The latent ones must be Normal. Mean response: Linear function of covariates after link transformation. Variance response: In some cases in can be modeled as a function of covariates.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nlme4\nglmer\n\n\nBoth\nglmmTMB1\nglmmTMB\n\n\nBayesian\nstanarm\nstan_glmer\n\n\nBayesian\nbrms1,2\nbrm\n\n\n\n1 Correlation structure for residuals 2 Scale parameter can be modeled as non-linear function of covariates.",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#generalized-non-linear-models",
    "href": "Supplements/canned.html#generalized-non-linear-models",
    "title": "Supplement: Formula-based methods",
    "section": "6.1 Generalized non-linear models",
    "text": "6.1 Generalized non-linear models\nStochastic model: A single level of one of the pre-specified distributions. Mean response: Non-linear function of covariates. Variance response: Variance can be modeled as non-linear function of covariates.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nLikelihood\nstats4\nmle\n\n\nLikelihood\nbbmle\nmle2\n\n\nBayesian\nbrms1\nbrm\n\n\n\n1 Also allows correlation structures",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/canned.html#generalized-non-linear-mixed-models",
    "href": "Supplements/canned.html#generalized-non-linear-mixed-models",
    "title": "Supplement: Formula-based methods",
    "section": "6.2 Generalized non-linear mixed models",
    "text": "6.2 Generalized non-linear mixed models\nStochastic model: Multiple levels. The latent ones must be Normal. Mean response: Non-linear function of covariates. Variance response: Correlation structure for residuals and variance can be modeled as non-linear function of covariates.\n\n\n\nParadigm\nPackage\nFunction\n\n\n\n\nBayesian\nbrms\nbrm",
    "crumbs": [
      "Supplement: Canned methods"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/no_solution.html",
    "href": "Supplements/bias_variance/no_solution.html",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "",
    "text": "You will learn how to:\n\nReducing bias using restricted maximum likelihood (REML)\nReducing variance by using the mode of the posterior (MAP)",
    "crumbs": [
      "Supplement: Variance and bias of estimates"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-linear-models",
    "href": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.1 Bias in estimates of linear models",
    "text": "2.1 Bias in estimates of linear models\nIn a linear model the maximum likelihood estimates of intercepts and slopes are unbiased, but the maximum likelihood estimate of the variance (\\(\\sigma^2\\)) is biased. This motivated the development of REML to compute unbiased estimates of the parameters related to the variance in linear models. If you are fitting linear models, you probably want to use one of the canned methods.\n# Assume a very vanilla linear model but with std dev increasing exponentially\nx = seq(0,1, 0.1)\n# Generate many many samples\nN = 1e4\nY = t(sapply(x, function(x) rnorm(N, 2 + 10*x, exp(x))))\nWe will use the canned method gls from the package nlme to fit the model with normal maximum likelihood:\nlibrary(nlme)\ny = Y[,1]\nfit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\nfit\nWe can extract the intercept slope and exponential coefficient as follows:\nc(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\nWe can now do it for all the samples\nlibrary(future.apply)\nplan(multisession)\ncoefs = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nhead(coefs)\nWe can now calculate the means of each column and compare to the true values:\nbias = colMeans(coefs) - c(2, 10, 1)\nbias\nThis makes more relative to the true values:\nbias/c(2, 10, 1)\nNotice how the estimates of the coefficient relating \\(\\sigma\\) to \\(x\\) is being overestimated by 15% when we compare the average of the sampling distribution to the true value. We do not see this bias in the intercept and slope.\nIf we now redo the fits using REML we will get also unbiased estimates for the coefficient related to the variance:\ncoefs_REML = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"REML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nThe estimates are now unbiased\nbias_REML = colMeans(coefs_REML) - c(2, 10, 1)\nbias_REML/c(2, 10, 1)\nMany statisticians seem to be obsessed with the bias in estimates, but remember that any estimate that you make from a single sample will have an error. For example, we can quantify the Mean Squared Error (MSE) for the ML estimate:\nMSE = c(a = mean((coefs[,1] - 2)^2),\n        b = mean((coefs[,2] - 10)^2),\n        c = mean((coefs[,3] - 1)^2))\nMSE\nThe MSE is actually equal to the bias squared plus the variance of the estimator. This means that we can compute the contribution of bias and variance to the error:\nvars = c(a = var(coefs[,1]), b = var(coefs[,2]), c = var(coefs[,3]))\ncbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)\nWe can see that in the parameters a and b all the error is due to variance in the parameter (since they are unbiased) and in c only 11% of the error is due to the bias.\nThis means that using REML only reduces the error of the estimation by a small amount. Let’s compare the new MSE with REML:\nMSE_REML = c(a = mean((coefs_REML[,1] - 2)^2),\n             b = mean((coefs_REML[,2] - 10)^2),\n             c = mean((coefs_REML[,3] - 1)^2))\ncbind(MSE, MSE_REML, Reduction = (MSE - MSE_REML)/MSE)\nSo, using REML decreased the error in the estimation by c by 33% (incidentally the error in a and b also decreased, that will not always happen but can). This is a pretty good result, especially if you care about the variation in your data (i.e., if it is of ecological relevance and not just noise). So, if you can use REML in a linear model, it pays off to use it.\nNote that methods for model comparison that rely on (pure) maximum likelihood will not apply to models estimated via REML. This includes AIC, BIC and likelihood ratio tests. So, model comparison should always be done with models estimated via maximum likelihood, not REML (in practice this means you end up with two versions of your model).",
    "crumbs": [
      "Supplement: Variance and bias of estimates"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-generalized-linear-models",
    "href": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-generalized-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.2 Bias in estimates of generalized linear models",
    "text": "2.2 Bias in estimates of generalized linear models\nAdd example for glm",
    "crumbs": [
      "Supplement: Variance and bias of estimates"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-non-linear-models",
    "href": "Supplements/bias_variance/no_solution.html#bias-in-estimates-of-non-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.3 Bias in estimates of non-linear models",
    "text": "2.3 Bias in estimates of non-linear models\nLet’s do a similar analysis for the log-normal model with exponentially decaying cv that we fitted in Lab 9 I am going to assume some true parameter values (similar to what we got after fitting to the data):\n# True parameters\na = 0.61\nb = 2.1\nc = 4.0\nd = 0.13\n# Use the cleanFIR dataset for DBH\nmu = a*cleanFIR$DBH^b\ncv = c*exp(-d*cleanFIR$DBH)\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\nN = nrow(cleanFIR)\nY = rlnorm(N, meanlog, sdlog)\nplot(cleanFIR$DBH, Y)\nLet’s simulate 1000 datasets:\nlibrary(future.apply)\nplan(multisession)\nY = future_sapply(1:1e3, function(x) rlnorm(N, meanlog, sdlog), future.seed=TRUE)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model to each of the simulated datasets (check how you did it before). Make sure that the bounds are sufficiently wide so that none of the estimates lie on the bound.\nCheck the bias in the parameter estimates. How does it differ from the linear model?\nCompare the bias to the mean absolute error of the parameter estimates, how relevant is the bias?",
    "crumbs": [
      "Supplement: Variance and bias of estimates"
    ]
  },
  {
    "objectID": "Lab9/solution.html",
    "href": "Lab9/solution.html",
    "title": "Lab 9: Modeling variance and dispersion (solutions)",
    "section": "",
    "text": "1 Learning goals\nYou will learn how to:\n\nAllow for variance to vary across groups\nAllow for variance to vary as a continuous function\n\n\n\n2 Variance across groups\nIn previous labs we have been modelling the mean of the population or process under study. In this lab we are going to look into models of the variance or parameters related to variance (called scale parameters). For many distributions, the mean and variance are not independent and it might not be possible to specify them directly with the usual parameterizations. When it doubt, check Chapter 4 of the book (I will give you some hints on possible ways to decouple mean and scale parameters).\nIn this first section we will work with allometry data relating the length and biomass (as ash-free dry weight or AFD) of clams on a beach in Argentina (this data was used in the book by Zuur et al. (2009)). I have given you the data in the file Clams.txt:\nlibrary(ggplot2)\nclams = read.table(\"./Lab9/Clams.txt\", header = TRUE)\nggplot(clams, aes(x = LENGTH, y = AFD, color = as.factor(MONTH))) + geom_point()\nThis is an allometric relationship, so I will try a power law with a log-normal. I will also parameterize the log-normal distribution as a function of the actual mean \\(\\mu\\) and coefficient of variation \\(cv\\). We can calculate the meanlog and sdlog from mu and cv as\n# Derived from the moments of log-normal (check Wikipedia)\nmu = 0.2\ncv = 1\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\n# Test it\ntest = rlnorm(10000, meanlog = meanlog, sdlog = sdlog)\n(mean(test) - mu)/mu\n(mean(test)/sd(test) - cv)/cv\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a power law of the form \\(a x ^b\\) to the clam data with a fixed cv. Make sure to use proper lower and upper bounds on all parameters. Make sure that mu remains positive or the code will crash. Also, since a is very small you probably want to work with log(a) or something like that and undo the transformation inside the nll function.\nThe scatter seems to vary across months. Refit the model but vary cv per month. Interpret the result: is there evidence that cv would vary across months? Can we justify using this second model?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFirst eyeball possible parameter values:\n\nwith(clams, plot(LENGTH, AFD))\ncurve(5e-4*x^1.7, add = TRUE)\nCalculate MLE\nlibrary(bbmle)\nnll = function(la, b, cv) {\n  a = 10^la # Avoid scale issues\n  mu = a*clams$LENGTH^b\n  if(any(mu &lt; 0)) browser()\n  meanlog = log(mu/sqrt(1 + cv^2))\n  sdlog   = sqrt(log(1 + cv^2))\n  -sum(dlnorm(clams$AFD, meanlog, sdlog, log = TRUE))\n}\npar0 = c(la = log10(5e-4), b = 1.7, cv = 0.1)\nfit = mle2(minuslogl = nll, start = as.list(par0), method = \"L-BFGS-B\",\n           lower = c(la = -6, b = 0, cv = 1e-2),\n           upper = c(la = 0, b = 10, cv = 1),\n           control = list(parscale = abs(par0), trace = TRUE))\nsummary(fit)\nLook at results\nmle_a = 10^coef(fit)[1]\nwith(clams, plot(LENGTH, AFD))\ncurve(mle_a*x^coef(fit)[2], add = TRUE, col = 2)\n\nNew NLL where a different cv is computed per month\n\nlibrary(bbmle)\nmonth_index = as.integer(as.factor(clams$MONTH))\nnll2 = function(la, b, cv1, cv2, cv3, cv4, cv5, cv6) {\n  # Avoid scale issues\n  a = 10^la\n  # Select the right cv\n  cv = c(cv1, cv2, cv3, cv4, cv5, cv6)[month_index]\n  mu = a*clams$LENGTH^b\n  if(any(mu &lt; 0)) browser()\n  meanlog = log(mu/sqrt(1 + cv^2))\n  sdlog   = sqrt(log(1 + cv^2))\n  -sum(dlnorm(clams$AFD, meanlog, sdlog, log = TRUE))\n}\npar0 = c(la = log10(5e-4), b = 1.7, cv1 = 0.1, cv2 = 0.1, cv3 = 0.1,\n         cv4 = 0.1, cv5 = 0.1, cv6 = 0.1)\nfit2 = mle2(minuslogl = nll2, start = as.list(par0), method = \"L-BFGS-B\",\n           lower = c(la = -6, b = 0, cv1 = 0.01, cv2 = 0.01, cv3 = 0.01,\n                     cv4 = 0.01, cv5 = 0.01, cv6 = 0.01),\n           upper = c(la = 0, b = 10, cv1 = 1, cv2 = 1, cv3 = 1,\n                     cv4 = 1, cv5 = 1, cv6 = 1),\n           control = list(parscale = abs(par0), trace = TRUE))\nsummary(fit2)\nThere is substantial variation in the estimated cvs that cannot be explain by uncertainty, suggesting that they might be different across months (at least when we hold a and b constant).\nBecause the models are nested and we have a lot of data, we could use a likelihood ratio test to compare them or AIC:\nanova(fit, fit2)\nAIC(fit, fit2)\nBoth indices confirm that the second model is much better.\n\n\n\n\n\n\n\n\n3 Variance along continuous variables\nThis is count data so we should change the model to a discrete distribution!!\nIn the book, Bolker analyzes a dataset on cone production by fir trees as a function of their diameter:\nlibrary(emdbook)\nwith(cleanFIR, plot(TOTCONES~DBH))\nWe can see that the average cone production increases with the diameter but also the scatter in the data. Therefore, it might be a good idea to model how this scatter increases with diameter.\nTo helps us figure out a way to model this, we can try to visualize how the standard deviation varies across diameter but for that we need to bin the data. I show you below how to do this, and I also calculate the coefficient of variation because in many distributions (like Gamma and Log-Normal) the standard deviation is not constant but the coefficient of variation is.\n# Remove missing data\ncleanFIR = FirDBHFec[!is.na(FirDBHFec$TOTCONES) & !is.na(FirDBHFec$DBH),]\n# Create bins\nDBHpoints = c(6,8,10,12,14,18)\n# Allocate vetor to store cvs and sd and calculate for first bin\ncvs = numeric(length(DBHpoints))\nsds = numeric(length(DBHpoints))\ntemp = subset(cleanFIR, DBH &lt;= DBHpoints[1])$TOTCONES\ncvs[1] = sd(temp)/mean(temp)\nsds[1] = sd(temp)\n# Repeat for all bins\nfor(i in 2:length(DBHpoints)) {\n  temp = subset(cleanFIR, DBH &lt;= DBHpoints[i] & DBH &gt;= DBHpoints[i - 1])$TOTCONES\n  cvs[i] = sd(temp)/mean(temp)\n  sds[i] = sd(temp)\n}\nWe can now plot the results\npar(mfrow = c(1,2))\nplot(c(5,7,9,11,13,16), sds, xlab = \"DBH\", ylab = \"sd TOTCONES\")\nplot(c(5,7,9,11,13,16), cvs, xlab = \"DBH\", ylab = \"cv TOTCONES\")\npar(mfrow = c(1,1))\nWe can see that while the standard deviation increases with DBH, the coefficient of variation decreases. If I were to model these data with a normal distribution (which is not a good idea because it will predict a lot of negative cone production) I would need to add an increase in the standard deviation with diameter. If I were to model with a log-normal parameterized with a coefficient of variation (like in the example with clams), I would need to model the decrease in the coefficient of variation wit diameter.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a non-linear model with a log-normal error distribution to the relationship between TOTCONES and DBH using the same parameterization as we used for the study on clams. Make sure to include a component to capture the decrease in the coefficient of variation with diameter.\n\nBe careful that TOTCONES contains zeros so you will need to adjust those (the trick that Bolker uses is to just add 1).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nI am going to fit a power law for the mean trend, just like for the clams. Let’s eyeball its coefficients from the raw data:\nwith(cleanFIR, plot(TOTCONES~DBH))\ncurve(0.5*x^2, add = TRUE)\nAnd I am going to assume an exponential decrease in the coefficient of variation with respect to diameter.\nwith(cleanFIR, plot(cvs~c(5,7,9,11,13,16)))\ncurve(3*exp(-0.15*x), add = TRUE)\nWe can now fit the model using these four starting values:\nnll = function(a, b, c, d) {\n  mu = a*cleanFIR$DBH^b\n  cv = c*exp(-d*cleanFIR$DBH)\n  meanlog = log(mu/sqrt(1 + cv^2))\n  sdlog   = sqrt(log(1 + cv^2))\n  NLL = -sum(dlnorm(cleanFIR$TOTCONES + 1, meanlog, sdlog, log = TRUE))\n  NLL\n}\npar0 = c(a = 0.5, b = 2, c = 3, d = 0.15)\nfit = mle2(minuslogl = nll, start = as.list(par0), method = \"L-BFGS-B\",\n           lower = c(a = 0.1, b = 1, c = 1, d = 0),\n           upper = c(a = 2, b = 4, c = 10, d = 1),\n           control = list(parscale = abs(par0), trace = TRUE))\nsummary(fit)",
    "crumbs": [
      "Solutions",
      "Lab 9 (solutions)"
    ]
  },
  {
    "objectID": "Lab9/material.html",
    "href": "Lab9/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "1 Learning goals\nYou will learn how to:\n\nAllow for variance to vary across groups\nAllow for variance to vary as a continuous function\n\n\n\n2 Variance across groups\nIn previous labs we have been modelling the mean of the population or process under study. In this lab we are going to look into models of the variance or parameters related to variance (called scale parameters). For many distributions, the mean and variance are not independent and it might not be possible to specify them directly with the usual parameterizations. When it doubt, check Chapter 4 of the book (I will give you some hints on possible ways to decouple mean and scale parameters).\nIn this first section we will work with allometry data relating the length and biomass (as ash-free dry weight or AFD) of clams on a beach in Argentina (this data was used in the book by Zuur et al. (2009)). I have given you the data in the file Clams.txt:\nlibrary(ggplot2)\nclams = read.table(\"./Lab9/Clams.txt\", header = TRUE)\nggplot(clams, aes(x = LENGTH, y = AFD, color = as.factor(MONTH))) + geom_point()\nThis is an allometric relationship, so I will try a power law with a log-normal. I will also parameterize the log-normal distribution as a function of the actual mean \\(\\mu\\) and coefficient of variation \\(cv\\). We can calculate the meanlog and sdlog from mu and cv as\n# Derived from the moments of log-normal (check Wikipedia)\nmu = 0.2\ncv = 1\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\n# Test it\ntest = rlnorm(10000, meanlog = meanlog, sdlog = sdlog)\n(mean(test) - mu)/mu\n(mean(test)/sd(test) - cv)/cv\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a power law of the form \\(a x ^b\\) to the clam data with a fixed cv. Make sure to use proper lower and upper bounds on all parameters. Make sure that mu remains positive or the code will crash. Also, since a is very small you probably want to work with log(a) or something like that and undo the transformation inside the nll function.\nThe scatter seems to vary across months. Refit the model but vary cv per month. Interpret the result: is there evidence that cv would vary across months? Can we justify using this second model?\n\n\n\n\n\n\n3 Variance along continuous variables\nThis is count data so we should change the model to a discrete distribution!!\nIn the book, Bolker analyzes a dataset on cone production by fir trees as a function of their diameter:\nlibrary(emdbook)\nwith(cleanFIR, plot(TOTCONES~DBH))\nWe can see that the average cone production increases with the diameter but also the scatter in the data. Therefore, it might be a good idea to model how this scatter increases with diameter.\nTo helps us figure out a way to model this, we can try to visualize how the standard deviation varies across diameter but for that we need to bin the data. I show you below how to do this, and I also calculate the coefficient of variation because in many distributions (like Gamma and Log-Normal) the standard deviation is not constant but the coefficient of variation is.\n# Remove missing data\ncleanFIR = FirDBHFec[!is.na(FirDBHFec$TOTCONES) & !is.na(FirDBHFec$DBH),]\n# Create bins\nDBHpoints = c(6,8,10,12,14,18)\n# Allocate vetor to store cvs and sd and calculate for first bin\ncvs = numeric(length(DBHpoints))\nsds = numeric(length(DBHpoints))\ntemp = subset(cleanFIR, DBH &lt;= DBHpoints[1])$TOTCONES\ncvs[1] = sd(temp)/mean(temp)\nsds[1] = sd(temp)\n# Repeat for all bins\nfor(i in 2:length(DBHpoints)) {\n  temp = subset(cleanFIR, DBH &lt;= DBHpoints[i] & DBH &gt;= DBHpoints[i - 1])$TOTCONES\n  cvs[i] = sd(temp)/mean(temp)\n  sds[i] = sd(temp)\n}\nWe can now plot the results\npar(mfrow = c(1,2))\nplot(c(5,7,9,11,13,16), sds, xlab = \"DBH\", ylab = \"sd TOTCONES\")\nplot(c(5,7,9,11,13,16), cvs, xlab = \"DBH\", ylab = \"cv TOTCONES\")\npar(mfrow = c(1,1))\nWe can see that while the standard deviation increases with DBH, the coefficient of variation decreases. If I were to model these data with a normal distribution (which is not a good idea because it will predict a lot of negative cone production) I would need to add an increase in the standard deviation with diameter. If I were to model with a log-normal parameterized with a coefficient of variation (like in the example with clams), I would need to model the decrease in the coefficient of variation wit diameter.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a non-linear model with a log-normal error distribution to the relationship between TOTCONES and DBH using the same parameterization as we used for the study on clams. Make sure to include a component to capture the decrease in the coefficient of variation with diameter.\n\nBe careful that TOTCONES contains zeros so you will need to adjust those (the trick that Bolker uses is to just add 1)."
  },
  {
    "objectID": "Lab7/no_solution.html",
    "href": "Lab7/no_solution.html",
    "title": "Lab 7: Optimisation and all that",
    "section": "",
    "text": "You will learn how to:\n\nDeal with optimization problems and assess confidence limits\nOptionally, estimate parameters in a Bayesian way using Stan\n\n\n\nFitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\n\n\n\nIn this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC",
    "crumbs": [
      "Lab 7"
    ]
  },
  {
    "objectID": "Lab7/no_solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "href": "Lab7/no_solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "title": "Lab 7: Optimisation and all that",
    "section": "",
    "text": "Fitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.",
    "crumbs": [
      "Lab 7"
    ]
  },
  {
    "objectID": "Lab7/no_solution.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "href": "Lab7/no_solution.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "title": "Lab 7: Optimisation and all that",
    "section": "",
    "text": "In this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC",
    "crumbs": [
      "Lab 7"
    ]
  },
  {
    "objectID": "Lab6/solution.html",
    "href": "Lab6/solution.html",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation (solutions)",
    "section": "",
    "text": "You will learn how to:\n\nProgram the likelihood function of a model.\nEstimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.\nEstimate the confidence intervals of the model parameters through profiling and the quadratic approximation.\nPerform stochastic simulations from the fitted models\n\n5 (Optionally) Estimate parameters and uncertainty using Laplace’s approximation to Bayes rule",
    "crumbs": [
      "Solutions",
      "Lab 5 & 6 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "href": "Lab6/solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation (solutions)",
    "section": "3.1 Finding the maximum likelihood estimate of the paramaters",
    "text": "3.1 Finding the maximum likelihood estimate of the paramaters\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTake the steps below\n\nGenerate 50 values from a negative binomial (rnbinom) with \\(\\mu=1\\), \\(k=0.4\\). Save the values in variables in case we want to use them again later.\nPlot the numbers in a frequency diagram\nNext, define the negative log-likelihood function for a simple draw from a negative binomial distribution: first include the parameters of the model and then the variables in the data that are used in the model (see example above)\nCalculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Remember that combine these parameter values using a list() and to name them so you can recognize them later on.\nCalculate the NLL of parameter values that are far from the values that were used to generate the data (e.g. \\(\\mu=10\\), \\(k=10\\))\nCalculate the maximum likelihood estimate (MLE)? Use mle2 with the default options and use the method-of-moments estimates as the starting estimates (par): opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom))\nWhat is the difference in NLL between the MLE estimates and the NLL derived at 5? Does it make sense?\nPerform a likelihood ratio test at 95% confidence compare the values obtained in 5 and 7.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst we generate a random sample assuming certain true values\nset.seed(1001)\nmu.true = 1\nk.true = 0.4\nx = rnbinom(50,mu=mu.true,size=k.true)\n\nplot(table(factor(x,levels=0:max(x))),\n     ylab = \"Frequency\", xlab = \"x\")\nWe can now create a function that implements the negative log-likelihood of the model:\nNLLfun1 = function(mu, k, x) {\n  -sum(dnbinom(x, mu = mu, size = k, log = TRUE))\n}\nWe test it with the true values and with \\(\\mu=10\\), \\(k=10\\)\nnll.true = NLLfun1(mu = mu.true, k = k.true, x = x)\nnll.far  = NLLfun1(mu = 10,      k = 10,     x = x)\nc(nll.true, nll.far)\nWe calculate the estimates based on method of moments\nm = mean(x)\nv = var(x)\nmu.mom = m\nk.mom  = m/(v/m-1)\nWe now perform the optimization using these estimates as initial values\nopt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom), data = data.frame(x = x))\ncoef(opt1)\nWe can now compare the different NLL values we got so far:\nc(MLE = -logLik(opt1), true = nll.true, far = nll.far)\nThe minimum negative log-likelihood is better than the NLL of the model with the true parameters. This does not mean that the maximum likelihood estimates are truer than the actual true values, but is a consequence of sampling error (i.e., relative small samples will not match exactly the population in its properties).\nThe likelihood ratio test uses the concept of deviance which is equal to twice the difference in NLL (between simpler and more complex):\ndeviance = 2*(nll.true - -logLik(opt1))\ndeviance\nWe now compare this value to a \\(\\chi^2\\) distribution with n degrees of freedom. To compute the p-value we should use the quantile function pchisq\npchisq(deviance, df = 2)\nThe p-value of 0.57 indicates that we cannot reject the hypothesis that there is a differnece between the true model and the MLE model.",
    "crumbs": [
      "Solutions",
      "Lab 5 & 6 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/solution.html#choosing-probability-distributions",
    "href": "Lab6/solution.html#choosing-probability-distributions",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation (solutions)",
    "section": "6.1 Choosing probability distributions",
    "text": "6.1 Choosing probability distributions\nIn this exercise we revisit the first exercise from Lab 1. In that exercise we had six different datasets each describing a biological phenomenon. The first step was to choose a deterministic function that describes the mean effect of the predictor variable (x) on the response variable (y; Lab 3). The second step involved the choice of the stochastic distribution which describes how the data varies around the mean (Lab 4).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nReload the first dataset, revisit the choice of your deterministic function, the eyeballed parameters, and the stochastic distribution. Next simulate data using these three components. Compare the simulated values with the observed values in a plot.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the first dataset is stored in a dataframe called shapes1, one can plot the observations as follows:\nplot(shapes1$y~ shapes1$x)\nnext one simulate from the data. If we assume the normal distribution to be reasonable choice, and a Michaelis Menten as a deterministic function, data can simulated as follows:\ny.sim &lt;- rnorm(55,mean=(25*shapes1$x)/(60+shapes1$x),sd=1.5)\nThe standard deviation is quite hard to assess. One thing that one could do to estimate the standard deviation is to estimate the variation around the mean prediction. Roughly 2*sd gives the 95% confidence interval. As most of the datapoints seems to be within 3 points to the mean y, this would translate in an sd of ~ 1.5.",
    "crumbs": [
      "Solutions",
      "Lab 5 & 6 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/solution.html#likelihood-surface",
    "href": "Lab6/solution.html#likelihood-surface",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation (solutions)",
    "section": "7.1 Likelihood surface",
    "text": "7.1 Likelihood surface\nTo find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues from Lab 3 where you used the negative binomial to generate 50 numbers and fitted back the parameters.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFor the likelihood surface:\n\nSet up vectors of \\(\\mu\\) and \\(k\\) values. Let’s try \\(\\mu\\) from 0.4 to 3 in steps of 0.05 and \\(k\\) from 0.01 to 0.7 in steps of 0.01.\nSet up a matrix to hold the results, The matrix for the results will have rows corresponding to \\(\\mu\\) and columns corresponding to \\(k\\):\nRun for loops to calculate and store the values. Use a for nested in another one\nDrawing a contour using the function ‘contour’. Change the argument nlevels to 100 to get a better view of the likelihood surface\nAdd the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmuvec = seq(0.4,3,by=0.05)\nkvec = seq(0.01,0.7,by=0.01)\nresmat = matrix(nrow=length(muvec),ncol=length(kvec))\nfor (i in 1:length(muvec)) {\n    for (j in 1:length(kvec)) {\n    resmat[i,j] = NLLfun1(c(muvec[i],kvec[j]))\n    }\n}\ncontour(muvec,kvec,resmat,xlab=expression(mu),ylab=\"k\")\ncontour(muvec,kvec,resmat,nlevels=100,lty=2,add=TRUE)",
    "crumbs": [
      "Solutions",
      "Lab 5 & 6 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/solution.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "href": "Lab6/solution.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation (solutions)",
    "section": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation",
    "text": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation\nWe are going to analyze the shapes2 dataset again, but this time using a Bayesian approach where we approximate the posterior distribution. This approximation is sometimes called Laplace’s approximation, but that name is also reserved for a related technique that we will use in Chapter 10, so here we will call it quadratic approximation because it uses the same mathematical tools as the quadratic approximation in maximum likelihood (in fact, the two are equivalent if you assume uniform priors).\nWe will work with the second of the shapes datasets and we will construct a type of prior know as Weakly Informative Priors (WIPs). These are used when you do not have clear prior information (or don’t want to use it) except for knowing what the order of magnitude of the parameters should be. WIPs are very popular in modern Bayesian statistics because they have a small effect on the posterior distribution while addressing many of the complicated numerical issues that we encounter in maximum likelihood. Use quadratic approximation with WIPs is basically an enhanced version of maximum likelihood.\n\n7.2.1 Constructing priors\nLet’s assume that these data represent the predation rate as a function of prey density, so it makes to try the functional responses we learnt in Chapter 3. Let’s assume that we can build a prior that the prey density is in the order of 100 and that the predation rate is in the order of 50 With this information we can already come up with Weakly Informative Priors (WIPs for short) that only informs about the order of magnitude of a variable. A simple way to create WIPs might be:\n\nDefine the prior information with a Normal distribution for each parameter.\nSet the mean to the order of magnitude of the corresponding variable (or something related to it, depends on the role)\nSet the standard deviation equal to the mean.\nFor scale parameters (e.g. the standard deviation) the prior distribution is often a truncated normal with mean of 0 and standard deviation dependend on the scale of the response variable.\n\nLet’s first start with the functional response Type II that has the form a*x/(b + x). We will come up with reasonable prior distributions and generate a bunch of simulations (these are called prior predictions and the can be used to check your priors are reasonable). Because all parameters should be positive, I use truncated normals (that only keep positive part):\nlibrary(truncnorm) # to truncate parameters\n# Generate random samples of each parameter\nN = 1000\na = rtruncnorm(N, mean = 50, sd = 50, a  = 0) # a scales with predation rate\nb = rtruncnorm(N, mean = 100/2, sd = 100, a = 0) # b scales with half prey density\nsigma = rtruncnorm(N, mean = 0, sd = 10, a = 0) # sigma scales with predation rate\nFor every prior sample, we can then generate a mean prediction:\nxseq = 0:200\nmu_y_prior = sapply(1:N, function(i) a[i]*xseq/(b[i] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_prior = rowMeans(mu_y_prior)\nlower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)\nupper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_prior, type = \"l\", ylim = c(0, 100),\n    xlab = = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_prior, lty = 2)\nlines(xseq, upper_mu_y_prior, lty = 2)\npoints(shapes2)\nWith WIPS you want to make sure that the range of predictions is much wider than the observed data, while avoiding nonsensical predictions (e.g., negative values or values that are way too high).\n\n\n7.2.2 Laplace’s approximation: How Bayes rule was solved originally\nRemember that Bayes rule was\n\\[\nP(\\theta | D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)},\n\\] where \\(P(D|\\theta)\\) is equivalent to the likelihood function, \\(P(\\theta)\\) is the prior probability of parameter values, \\(P(\\theta | D)\\) is the posterior probability and \\(P(D)\\) is the probability of seeing thee particular data under the assumed mode.\nNote that \\(P(D|\\theta)\\) and \\(P(\\theta)\\) are built by you (they are functions of \\(\\theta\\)) and therefore you know them, but you do not know \\(P(D)\\) (this is an unknown value). If you knew it, then Bayesian statistics would be “straightforward” in the sense that you just need to multiple two functions and divide by a constant. In practice you would need to do some extra work as this only gives you a probability density function, you would then need to figure out the cumulative density function, how to generate random samples from the posterior, etc.\nThe first person to solve Bayes rule was called Laplace and he realized you could calculate \\(P(D)\\) as:\n\\[\nP(D) = \\int{P(D|\\theta)P(\\theta)d\\theta}\n\\]\nThis seemingly innocent integral can be quite hard (or in practice impossible) to solve beyond the simplest of models (the calculus of probability of functions can be too nasty). He developed a mathematical approximation that consists of:\n\nTake the log of \\(P(D|\\theta)P(\\theta\\) (let’s call it the log probability density or lpd).\nFind its maximum.\nApproximate the lpd with a 2nd Order Taylor approximation around the maximum.\n\nFrom that he derived what the integral should be if the approximation was exact, based on the Normal distribution. This procedure for approximating integrals will come back in Chapter 10.\nAfter Laplace published his method it was shown (not sure when or by whom) that his method of approximating \\(P(D)\\) was equivalent to the following:\n\nFind the value of \\(\\theta\\) that maximizes lpd (we call it Maximum a Posteriori, or MAP).\nApproximate the posterior distribution \\(P(\\theta | D)\\) by a normal distribution with mean equal to MAP and covariance matrix derive from the Hessian matrix of second order derivatives.\n\nThat is, Laplace’s approximation to \\(P(D)\\) is equivalent to approximating the posterior distribution by a Normal distribution centered around its mode.\nDoes this procedure sound familiar? If we assume uniform priors, Laplace’s approximation is exactly the same as maximum likelihood with quadratic approximation (section 6.5 in the book) which is the original maximum likelihood method published by Fisher in 1922. Indeed, Laplace invented this method 150 years before Fisher, but in a Bayesian rather than Frequentist context. The only difference is that in the Bayesian context you take into account prior distributions rather than just the likelihood.\nAs discussed in the book, this approximation becomes better as the amount of data increases (i.e., it is only exact asymptotically) and this remains true in the Bayesian context too. Some authors seem to think that the only correct way to do Bayesian statistics is to run complex algorithms such as Markov Chain Monte Carlo (we cover that in the next chapter) but using Laplace’s approximation is as valid as using the quadratic approximation for maximum likelihood. Thus, we will learn how to use this approach first and leave the more complex MCMC approaches for later.\n\n\n7.2.3 Implementing Laplace’s approximation\nWe can implement Laplace’s approximation in a similar way to how we implemented the method of maximum likelihood. First, we build a function to compute the log posterior density (for consistency, we will use the negative, as in the maximum likelihood theory):\n# First the negative log likelihood\nnll = function(a, b, sd, x, y) {\n  mu = (a*x)/(b+x)\n  nll = -sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Now we add the prior\nnlpd = function(a, b, sd, x, y){\n    nlpd = nll(a, b, sd, x, y) - \n           dnorm(a, mean = 50, sd = 50, log = TRUE) -\n           dnorm(b, mean = 50, sd = 100, log = TRUE) -\n           dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n    return(nlpd)\n}\nnlpd(a = 50, b = 50, sd = 10, x = shapes2$x, y = shapes2$y)\nWe can now run the optimizer as usual\nMAP = mle2(nlpd, start = list(a = 20, b = 10, sd = 1), \n           data = shapes2, method=\"L-BFGS-B\",\n           lower = c(a = 0, b = 0, sd = 0))\nLet’s look at the results:\nsummary(MAP)\nOf course these results assume we were doing maximum likelihood, so much of the information is not relevant. But the first two columns of the Coefficients sector are now giving us the mode of the posterior distribution and the standard deviation of each of the posterior marginal distributions.\nIn good Bayesian fashion we may want to generate a sample of values from the posterior distribution and use to make other calculations (e.g., predictions). We first need to extract the variance-covariance matrix\nV = vcov(MAP)\nThen we use a multivariate normal with mean equal to the MAP estimate and using the variance-covariance matrix we just extracted:\nlibrary(mvtnorm)\nposterior = rmvnorm(n = 1e4, mean = coef(MAP), sigma = V)\nLet’s visualize it (I only choose 1000 values because otherwise it is too slow):\npairs(posterior[1:1e3,], pch = \".\")\nWe can also look at the marginal distributions and compare the posterior and prior:\nprior = cbind(a = a, b = b, sd = sigma)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  plot(density(posterior[,i]), xlab = colnames(posterior)[i], main = \"\")\n  lines(density(prior[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nNotice that in the region where the posterior samples where generated (basically where most of the probability is located), the prior distributions are practically flat. This is what makes a prior weakly informative. It has practically no effect on the posterior but it can help the algorithm find the solution much faster. This will lead to Bayesian results that are very similar to Maximum Likelihood results.\nWe can also make predictions and compare it to the prior predictions we did before.\nN = nrow(posterior)\nxseq = 0:200\nmu_y_posterior = sapply(1:N, function(i) posterior[i,\"a\"]*xseq/(posterior[i,\"b\"] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_posterior = rowMeans(mu_y_posterior)\nlower_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.025)\nupper_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_posterior, type = \"l\", ylim = c(0, 100),\n    xlab = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_posterior, lty = 2)\nlines(xseq, upper_mu_y_posterior, lty = 2)\nlines(xseq, mean_mu_y_prior, col = 2)\nlines(xseq, lower_mu_y_prior, lty = 2, col = 2)\nlines(xseq, upper_mu_y_prior, lty = 2, col = 2)\npoints(shapes2)\nIf you look hard you will see that the black lines are now plotted through the cloud of points and the uncertainty is quite small. Remember that this represents our uncertainty about the average predation rate and not individual values (that would require including the parameter sd too). The fact that our prior distribution was a bit off is not a problem as we made our prior uncertainty big enough to accommodate a wide range of possible responses.\n\n\n7.2.4 Computing DIC\nWe can compute DIC. Remember that this requires two calculations:\n\nTwice the negative log likelihood at the mean posterior estimate.\nThe same quantity but averaged over the posterior distribution.\n\nLet’s do the first one:\nmean_posterior = colMeans(posterior)\nterm1 = 2*nll(a = mean_posterior[1], b = mean_posterior[2], sd = mean_posterior[3], \n             shapes2$x, shapes2$y)\nThe second one is more involved but since we already have a sample of values from the posterior we can use them directly to estimate the mean deviance (this is essentially the Monte Carlo method to calculate averages):\nall_nlls = apply(posterior, 1, function(x) 2*nll(a = x[1], b = x[2], sd = x[3],\n                                                 x = shapes2$x, y = shapes2$y))\nterm2 = mean(all_nlls)                                                 \nThe effective number of parameters is the difference of term2 and term1:\npDIC = term2 - term1\nNotice that this is slightly larger than 3. The Deviance Information Criterion then becomes:\nDIC = term1 + 2*pDIC\nIn the next chapter (where we will use Markov Chain Monte Carlo) we will also learn more modern information criteria that are are meant to replace DIC.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b + x^2) to the same dataset as the model above using Laplace’s approximation.\nCompare this model and the previous model using DIC\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1 Fit the model\nThe model would be\nnll2 = function(a, b, sd, x, y) {\n    mu = (a*x^2)/(b^2+x^2)\n    nll = -sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\nnlpd2 = function(a, b, sd, x, y) {\n    # We add the negative log prior density\n    nlpd = nll2(a, b, sd, x, y) - \n           dnorm(a, mean = 50, sd = 50, log = TRUE) -\n           dnorm(b, mean = 50, sd = 100, log = TRUE) -\n           dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n    return(nlpd)\n}\nnlpd2(a = 50, b = 50, sd = 10, x = shapes2$x, y = shapes2$y)\nAnd then I run the optimization as usual:\nMAP2 = mle2(nlpd2, start = list(a = 20, b = 10, sd = 1), \n           data = shapes2, method=\"L-BFGS-B\",\n           lower = c(a = 0, b = 0, sd = 0))\nsummary(MAP2)\n\nCompute DIC\n\nV2 = vcov(MAP2)\nposterior2 = rmvnorm(n = 1e4, mean = coef(MAP2), sigma = V2)\nmean_posterior2 = colMeans(posterior2)\nterm1 = 2*nll2(a = mean_posterior2[1], b = mean_posterior2[2], sd = mean_posterior2[3], \n             shapes2$x, shapes2$y)\nall_nlls2 = apply(posterior2, 1, function(x) 2*nll2(a = x[1], b = x[2], sd = x[3],\n                                                 x = shapes2$x, y = shapes2$y))\nterm2 = mean(all_nlls2)    \npDIC2 = term2 - term1\nDIC2 = term1 + 2*pDIC2\nc(pDIC, pDIC2)\nc(DIC, DIC2)\nLike for AIC, the smaller the DIC the better. In this case the effective number of parameters is (practically) the same, which makes sense since the actual number of parameters is the same. But adding the quadratic term has not improved the fit so it is clearly not a better model.",
    "crumbs": [
      "Solutions",
      "Lab 5 & 6 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/material.html",
    "href": "Lab6/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "You will learn how to:\n\nProgram the likelihood function of a model.\nEstimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.\nEstimate the confidence intervals of the model parameters through profiling and the quadratic approximation.\nPerform stochastic simulations from the fitted models\n\n5 (Optionally) Estimate parameters and uncertainty using Laplace’s approximation to Bayes rule"
  },
  {
    "objectID": "Lab6/material.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "href": "Lab6/material.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "title": "1 Learning goals",
    "section": "3.1 Finding the maximum likelihood estimate of the paramaters",
    "text": "3.1 Finding the maximum likelihood estimate of the paramaters\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTake the steps below\n\nGenerate 50 values from a negative binomial (rnbinom) with \\(\\mu=1\\), \\(k=0.4\\). Save the values in variables in case we want to use them again later.\nPlot the numbers in a frequency diagram\nNext, define the negative log-likelihood function for a simple draw from a negative binomial distribution: first include the parameters of the model and then the variables in the data that are used in the model (see example above)\nCalculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Remember that combine these parameter values using a list() and to name them so you can recognize them later on.\nCalculate the NLL of parameter values that are far from the values that were used to generate the data (e.g. \\(\\mu=10\\), \\(k=10\\))\nCalculate the maximum likelihood estimate (MLE)? Use mle2 with the default options and use the method-of-moments estimates as the starting estimates (par): opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom))\nWhat is the difference in NLL between the MLE estimates and the NLL derived at 5? Does it make sense?\nPerform a likelihood ratio test at 95% confidence compare the values obtained in 5 and 7."
  },
  {
    "objectID": "Lab6/material.html#choosing-probability-distributions",
    "href": "Lab6/material.html#choosing-probability-distributions",
    "title": "1 Learning goals",
    "section": "6.1 Choosing probability distributions",
    "text": "6.1 Choosing probability distributions\nIn this exercise we revisit the first exercise from Lab 1. In that exercise we had six different datasets each describing a biological phenomenon. The first step was to choose a deterministic function that describes the mean effect of the predictor variable (x) on the response variable (y; Lab 3). The second step involved the choice of the stochastic distribution which describes how the data varies around the mean (Lab 4).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nReload the first dataset, revisit the choice of your deterministic function, the eyeballed parameters, and the stochastic distribution. Next simulate data using these three components. Compare the simulated values with the observed values in a plot."
  },
  {
    "objectID": "Lab6/material.html#likelihood-surface",
    "href": "Lab6/material.html#likelihood-surface",
    "title": "1 Learning goals",
    "section": "7.1 Likelihood surface",
    "text": "7.1 Likelihood surface\nTo find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues from Lab 3 where you used the negative binomial to generate 50 numbers and fitted back the parameters.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFor the likelihood surface:\n\nSet up vectors of \\(\\mu\\) and \\(k\\) values. Let’s try \\(\\mu\\) from 0.4 to 3 in steps of 0.05 and \\(k\\) from 0.01 to 0.7 in steps of 0.01.\nSet up a matrix to hold the results, The matrix for the results will have rows corresponding to \\(\\mu\\) and columns corresponding to \\(k\\):\nRun for loops to calculate and store the values. Use a for nested in another one\nDrawing a contour using the function ‘contour’. Change the argument nlevels to 100 to get a better view of the likelihood surface\nAdd the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments."
  },
  {
    "objectID": "Lab6/material.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "href": "Lab6/material.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "title": "1 Learning goals",
    "section": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation",
    "text": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation\nWe are going to analyze the shapes2 dataset again, but this time using a Bayesian approach where we approximate the posterior distribution. This approximation is sometimes called Laplace’s approximation, but that name is also reserved for a related technique that we will use in Chapter 10, so here we will call it quadratic approximation because it uses the same mathematical tools as the quadratic approximation in maximum likelihood (in fact, the two are equivalent if you assume uniform priors).\nWe will work with the second of the shapes datasets and we will construct a type of prior know as Weakly Informative Priors (WIPs). These are used when you do not have clear prior information (or don’t want to use it) except for knowing what the order of magnitude of the parameters should be. WIPs are very popular in modern Bayesian statistics because they have a small effect on the posterior distribution while addressing many of the complicated numerical issues that we encounter in maximum likelihood. Use quadratic approximation with WIPs is basically an enhanced version of maximum likelihood.\n\n7.2.1 Constructing priors\nLet’s assume that these data represent the predation rate as a function of prey density, so it makes to try the functional responses we learnt in Chapter 3. Let’s assume that we can build a prior that the prey density is in the order of 100 and that the predation rate is in the order of 50 With this information we can already come up with Weakly Informative Priors (WIPs for short) that only informs about the order of magnitude of a variable. A simple way to create WIPs might be:\n\nDefine the prior information with a Normal distribution for each parameter.\nSet the mean to the order of magnitude of the corresponding variable (or something related to it, depends on the role)\nSet the standard deviation equal to the mean.\nFor scale parameters (e.g. the standard deviation) the prior distribution is often a truncated normal with mean of 0 and standard deviation dependend on the scale of the response variable.\n\nLet’s first start with the functional response Type II that has the form a*x/(b + x). We will come up with reasonable prior distributions and generate a bunch of simulations (these are called prior predictions and the can be used to check your priors are reasonable). Because all parameters should be positive, I use truncated normals (that only keep positive part):\nlibrary(truncnorm) # to truncate parameters\n# Generate random samples of each parameter\nN = 1000\na = rtruncnorm(N, mean = 50, sd = 50, a  = 0) # a scales with predation rate\nb = rtruncnorm(N, mean = 100/2, sd = 100, a = 0) # b scales with half prey density\nsigma = rtruncnorm(N, mean = 0, sd = 10, a = 0) # sigma scales with predation rate\nFor every prior sample, we can then generate a mean prediction:\nxseq = 0:200\nmu_y_prior = sapply(1:N, function(i) a[i]*xseq/(b[i] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_prior = rowMeans(mu_y_prior)\nlower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)\nupper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_prior, type = \"l\", ylim = c(0, 100),\n    xlab = = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_prior, lty = 2)\nlines(xseq, upper_mu_y_prior, lty = 2)\npoints(shapes2)\nWith WIPS you want to make sure that the range of predictions is much wider than the observed data, while avoiding nonsensical predictions (e.g., negative values or values that are way too high).\n\n\n7.2.2 Laplace’s approximation: How Bayes rule was solved originally\nRemember that Bayes rule was\n\\[\nP(\\theta | D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)},\n\\] where \\(P(D|\\theta)\\) is equivalent to the likelihood function, \\(P(\\theta)\\) is the prior probability of parameter values, \\(P(\\theta | D)\\) is the posterior probability and \\(P(D)\\) is the probability of seeing thee particular data under the assumed mode.\nNote that \\(P(D|\\theta)\\) and \\(P(\\theta)\\) are built by you (they are functions of \\(\\theta\\)) and therefore you know them, but you do not know \\(P(D)\\) (this is an unknown value). If you knew it, then Bayesian statistics would be “straightforward” in the sense that you just need to multiple two functions and divide by a constant. In practice you would need to do some extra work as this only gives you a probability density function, you would then need to figure out the cumulative density function, how to generate random samples from the posterior, etc.\nThe first person to solve Bayes rule was called Laplace and he realized you could calculate \\(P(D)\\) as:\n\\[\nP(D) = \\int{P(D|\\theta)P(\\theta)d\\theta}\n\\]\nThis seemingly innocent integral can be quite hard (or in practice impossible) to solve beyond the simplest of models (the calculus of probability of functions can be too nasty). He developed a mathematical approximation that consists of:\n\nTake the log of \\(P(D|\\theta)P(\\theta\\) (let’s call it the log probability density or lpd).\nFind its maximum.\nApproximate the lpd with a 2nd Order Taylor approximation around the maximum.\n\nFrom that he derived what the integral should be if the approximation was exact, based on the Normal distribution. This procedure for approximating integrals will come back in Chapter 10.\nAfter Laplace published his method it was shown (not sure when or by whom) that his method of approximating \\(P(D)\\) was equivalent to the following:\n\nFind the value of \\(\\theta\\) that maximizes lpd (we call it Maximum a Posteriori, or MAP).\nApproximate the posterior distribution \\(P(\\theta | D)\\) by a normal distribution with mean equal to MAP and covariance matrix derive from the Hessian matrix of second order derivatives.\n\nThat is, Laplace’s approximation to \\(P(D)\\) is equivalent to approximating the posterior distribution by a Normal distribution centered around its mode.\nDoes this procedure sound familiar? If we assume uniform priors, Laplace’s approximation is exactly the same as maximum likelihood with quadratic approximation (section 6.5 in the book) which is the original maximum likelihood method published by Fisher in 1922. Indeed, Laplace invented this method 150 years before Fisher, but in a Bayesian rather than Frequentist context. The only difference is that in the Bayesian context you take into account prior distributions rather than just the likelihood.\nAs discussed in the book, this approximation becomes better as the amount of data increases (i.e., it is only exact asymptotically) and this remains true in the Bayesian context too. Some authors seem to think that the only correct way to do Bayesian statistics is to run complex algorithms such as Markov Chain Monte Carlo (we cover that in the next chapter) but using Laplace’s approximation is as valid as using the quadratic approximation for maximum likelihood. Thus, we will learn how to use this approach first and leave the more complex MCMC approaches for later.\n\n\n7.2.3 Implementing Laplace’s approximation\nWe can implement Laplace’s approximation in a similar way to how we implemented the method of maximum likelihood. First, we build a function to compute the log posterior density (for consistency, we will use the negative, as in the maximum likelihood theory):\n# First the negative log likelihood\nnll = function(a, b, sd, x, y) {\n  mu = (a*x)/(b+x)\n  nll = -sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Now we add the prior\nnlpd = function(a, b, sd, x, y){\n    nlpd = nll(a, b, sd, x, y) - \n           dnorm(a, mean = 50, sd = 50, log = TRUE) -\n           dnorm(b, mean = 50, sd = 100, log = TRUE) -\n           dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n    return(nlpd)\n}\nnlpd(a = 50, b = 50, sd = 10, x = shapes2$x, y = shapes2$y)\nWe can now run the optimizer as usual\nMAP = mle2(nlpd, start = list(a = 20, b = 10, sd = 1), \n           data = shapes2, method=\"L-BFGS-B\",\n           lower = c(a = 0, b = 0, sd = 0))\nLet’s look at the results:\nsummary(MAP)\nOf course these results assume we were doing maximum likelihood, so much of the information is not relevant. But the first two columns of the Coefficients sector are now giving us the mode of the posterior distribution and the standard deviation of each of the posterior marginal distributions.\nIn good Bayesian fashion we may want to generate a sample of values from the posterior distribution and use to make other calculations (e.g., predictions). We first need to extract the variance-covariance matrix\nV = vcov(MAP)\nThen we use a multivariate normal with mean equal to the MAP estimate and using the variance-covariance matrix we just extracted:\nlibrary(mvtnorm)\nposterior = rmvnorm(n = 1e4, mean = coef(MAP), sigma = V)\nLet’s visualize it (I only choose 1000 values because otherwise it is too slow):\npairs(posterior[1:1e3,], pch = \".\")\nWe can also look at the marginal distributions and compare the posterior and prior:\nprior = cbind(a = a, b = b, sd = sigma)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  plot(density(posterior[,i]), xlab = colnames(posterior)[i], main = \"\")\n  lines(density(prior[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nNotice that in the region where the posterior samples where generated (basically where most of the probability is located), the prior distributions are practically flat. This is what makes a prior weakly informative. It has practically no effect on the posterior but it can help the algorithm find the solution much faster. This will lead to Bayesian results that are very similar to Maximum Likelihood results.\nWe can also make predictions and compare it to the prior predictions we did before.\nN = nrow(posterior)\nxseq = 0:200\nmu_y_posterior = sapply(1:N, function(i) posterior[i,\"a\"]*xseq/(posterior[i,\"b\"] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_posterior = rowMeans(mu_y_posterior)\nlower_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.025)\nupper_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_posterior, type = \"l\", ylim = c(0, 100),\n    xlab = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_posterior, lty = 2)\nlines(xseq, upper_mu_y_posterior, lty = 2)\nlines(xseq, mean_mu_y_prior, col = 2)\nlines(xseq, lower_mu_y_prior, lty = 2, col = 2)\nlines(xseq, upper_mu_y_prior, lty = 2, col = 2)\npoints(shapes2)\nIf you look hard you will see that the black lines are now plotted through the cloud of points and the uncertainty is quite small. Remember that this represents our uncertainty about the average predation rate and not individual values (that would require including the parameter sd too). The fact that our prior distribution was a bit off is not a problem as we made our prior uncertainty big enough to accommodate a wide range of possible responses.\n\n\n7.2.4 Computing DIC\nWe can compute DIC. Remember that this requires two calculations:\n\nTwice the negative log likelihood at the mean posterior estimate.\nThe same quantity but averaged over the posterior distribution.\n\nLet’s do the first one:\nmean_posterior = colMeans(posterior)\nterm1 = 2*nll(a = mean_posterior[1], b = mean_posterior[2], sd = mean_posterior[3], \n             shapes2$x, shapes2$y)\nThe second one is more involved but since we already have a sample of values from the posterior we can use them directly to estimate the mean deviance (this is essentially the Monte Carlo method to calculate averages):\nall_nlls = apply(posterior, 1, function(x) 2*nll(a = x[1], b = x[2], sd = x[3],\n                                                 x = shapes2$x, y = shapes2$y))\nterm2 = mean(all_nlls)                                                 \nThe effective number of parameters is the difference of term2 and term1:\npDIC = term2 - term1\nNotice that this is slightly larger than 3. The Deviance Information Criterion then becomes:\nDIC = term1 + 2*pDIC\nIn the next chapter (where we will use Markov Chain Monte Carlo) we will also learn more modern information criteria that are are meant to replace DIC.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b + x^2) to the same dataset as the model above using Laplace’s approximation.\nCompare this model and the previous model using DIC"
  },
  {
    "objectID": "Lab4/no_solution.html",
    "href": "Lab4/no_solution.html",
    "title": "Lab 4: Probability distributions",
    "section": "",
    "text": "This lab has two goals:\n\nPractice with probability distributions on different types of data.\nMake you familiar with the technicalities of stochastic distributions in R.\n\nIn particular, how to generate values from probability distributions and how to make your own probability distribution. Under time constraints, make sure that you make at least the exercises in the first three sections.",
    "crumbs": [
      "Lab 4"
    ]
  },
  {
    "objectID": "Lab4/no_solution.html#jensens-inequality",
    "href": "Lab4/no_solution.html#jensens-inequality",
    "title": "Lab 4: Probability distributions",
    "section": "4.1 Jensen’s inequality",
    "text": "4.1 Jensen’s inequality\nJensen’s inequality states the following: Suppose you have a number of values, \\(x\\), with a mean \\(\\bar{x}\\), and a non-linear function \\(f(x)\\). Then the mean of \\(f(x)\\) is not equal to \\(f(\\bar{x})\\).\nJensen’s inequality can be important in a number of cases. The first one is mentioned in Chapter 4 (page 104) on how variability can change the mean behaviour of a system (damselfish).\nAnother example where Jensen’s inequality kicks in is when transforming your data. Data transformations are commonly applied to get normally distributed errors. Because in statistical models you are often interested in the mean effect of a given treatment.\nNote that quantiles are not afffected by Jensen’s inequality as long as the transformation is monotonic. Thus, if you were to model the median effect of a given treatment you could back transform that.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind out what the effect of Jensen’s inequality is on a series of log-tranformed datapoints with respect to the estimated mean.\nUse the following pseudo-code:\n\nGenerate 10 random deviates from a uniform distribution (choose the range of 0 to 10).\nCalculate the mean of those 10 deviates.\nPlot the function \\(\\log(x)\\) with curve() on the range from 0-10, and plot random sample onto it.\nCalculate the mean of the log-transformed values and transform this mean back the normal scale, and compare to the mean calculated at 1.\nPlot the means with abline(h=...) if you want to draw a horizontal line or abline(v=...) to draw a vertical line.\nExplain differences between the two means.\n\n\n\n\nThis exercise shows that it is usually a good idea to leave variables untransformed when estimating the properties from this data.",
    "crumbs": [
      "Lab 4"
    ]
  },
  {
    "objectID": "Lab4/no_solution.html#zero-inflated-distributions",
    "href": "Lab4/no_solution.html#zero-inflated-distributions",
    "title": "Lab 4: Probability distributions",
    "section": "6.1 Zero-inflated distributions",
    "text": "6.1 Zero-inflated distributions\nThe general formula for the probability distribution of a zero-inflated distribution, with an underlying distribution \\(P(x)\\) and a zero-inflation probability of \\(p_z\\), is:\n\\[\n\\begin{eqnarray*}\n\\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\\\\n\\mbox{Prob}(x&gt;0) & = & (1-p_z) P(x)\n\\end{eqnarray*}\n\\]\nSo, for example, we could define a probability distribution for a zero-inflated negative binomial as follows:\ndzinbinom = function(x,mu,size,zprob) {\n  ifelse(x==0,\n         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),\n         (1-zprob)*dnbinom(x,mu=mu,size=size))\n}\nThe name, dzinbinom, follows the R convention for a probability distribution function: a d followed by the abbreviated name of the distribution, in this case zinbinom for “zero-inflated negative binomial”).\nThe ifelse() command checks every element of x to see whether it is zero or not and fills in the appropriate value depending on the answer.\nThe sampling function for our zero-inflated distribution would look like this:\nrzinbinom = function(n,mu,size,zprob) {\n  ifelse(runif(n)&lt;zprob,\n         0,\n         rnbinom(n,mu=mu,size=size))\n}\nThe command runif(n) picks n random values between 0 and 1; the ifelse command compares them with the value of zprob. If an individual value is less than zprob (which happens with probability zprob=\\(p_z\\)), then the corresponding random number is zero; otherwise it is a value picked out of the appropriate negative binomial distribution.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCheck graphically that these functions actually work. For instance, you could compare the results with a negative binomial function with the same mean and variance as the data.",
    "crumbs": [
      "Lab 4"
    ]
  },
  {
    "objectID": "Lab3/solution.html",
    "href": "Lab3/solution.html",
    "title": "Lab 3: Analyzing functions (solutions)",
    "section": "",
    "text": "In this lab you will learn to analyse mathematical functions. This is an important step in ecological modelling. Next, we proceed with analysing and programming these functions in R. To do so, you will need more advanced programming skills such as for-loops, if-else statements and functions.",
    "crumbs": [
      "Solutions",
      "Lab 3 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/solution.html#plotting-curves",
    "href": "Lab3/solution.html#plotting-curves",
    "title": "Lab 3: Analyzing functions (solutions)",
    "section": "3.1 Plotting curves",
    "text": "3.1 Plotting curves\nHere are the R commands used to generate Figure 3.2 in the book (p 74). They just use curve(), with add=FALSE (the default, which draws a new plot) and add=TRUE (adds the curve to an existing plot), particular values of from and to, and various graphical parameters (ylim, ylab, lty).\ncurve(2*exp(-x/2),from=0,to=7,ylim=c(0,2),ylab=\"\")\ncurve(2*exp(-x),add=TRUE,lty=4)\ncurve(x*exp(-x/2),add=TRUE,lty=2)\ncurve(2*x*exp(-x/2),add=TRUE,lty=3)\ntext(0.4,1.9,expression(paste(\"exponential: \",2*e^(-x/2))),adj=0)\ntext(4,.5,expression(paste(\"Ricker: \",x*e^(-x/2))))\ntext(4,1,expression(paste(\"Ricker: \",2*x*e^(-x/2))),adj=0)\ntext(2.8,0,expression(paste(\"exponential: \",2*e^(-x))))\nThe only new thing in this figure is the use of expression() to add a mathematical formula to an R graphic. text(x,y,\"x^2\") puts x^2 on the graph at position \\((x,y)\\); text(x,y,expression(x^2)) (no quotation marks) puts \\(x^2\\) on the graph. See ?plotmath or ?demo(plotmath) for (much) more information.\nAn alternate way of plotting the exponential parts of this curve:\nxvec = seq(0,7,length=100)\nexp1_vec = 2*exp(-xvec/2)\nexp2_vec = 2*exp(-xvec)\nplot(xvec,exp1_vec,type=\"l\",ylim=c(0,2),ylab=\"\")\nlines(xvec,exp2_vec,lty=4)\nFinally, if you have a more complicated function you could use sapply() to call this function along with appropriate parameter values. you could say:\nexpfun = function(x,a=1,b=1) {\n   a*exp(-b*x)\n }\nexp1_vec = sapply(xvec,expfun,a=2,b=1/2)\nexp2_vec = sapply(xvec,expfun,a=2,b=1)\nThe advantage of curve() is that you don’t have to define any vectors: the advantage of doing things the other way arises when you want to keep the vectors around to do other calculations with them.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nConstruct a curve that has a maximum at (\\(x=5\\), \\(y=1\\)). Write the equation, draw the curve in R, and explain how you got there.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor example: \\(-(x-5)^2+1\\)\ncurve(-(x-5)^2+1,from=-10,to=10)\nabline(v=5)\nabline(h=1)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChoose appropriate parameter values through eyeballing so that the chosen curves more or less match the data. Eyeballing means that you knowledge on the effect of parameter values on the shape of the function. Later we will use likelihood methods to estimate the parameter values. Plot the curves on top of the data using curve.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCheck the plotted curve against the data to see how well the eyeballed parameters work\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTime permitting repeat the previous three questions for three of the shapes datasets.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFirst read in the data as shown at the start of this practical.\nAfter you have read in the data, you can make a plot, e.g. through plot(shapes1$y~ shapes1$x) or plot(y~x,data=shapes1) for dataset 1. Multiple plots can be made through specifying par(mfrow=c(3,2)). This will setup the grid, after using plot six times, the grid will be filled with plots.\n\nCurves can be added to the plots through curve: e.g. curve(2x+2x,from=0,to=20)\ndataset 1: Reasonable values for the first dataset assuming a Michaelis-Menten relationship are a = 25 and b = 60. For the non-rectangular parabola one could choose values of theta = 0.7; a = 0.25; pmax = 25.\ndataset 2\ncurve(ifelse(x&gt;27,18,(2/3)*x),add=T)\ncurve(20*x/(10+x),add=T)\ndataset 3\ncurve(0.6*x^2,add=T)\ndataset 4\nK = 200; r = 0.2; N0=2; curve(K/(1+(K/N0)*exp(-r*x)),add=T)\ndataset 5\ncurve(8*(exp(-0.75*x)),add=T)\ndataset 6\nmu = 5; b = 2; curve(exp(-(mu-x)^2/b)  ,add=T)",
    "crumbs": [
      "Solutions",
      "Lab 3 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/solution.html#for-loops",
    "href": "Lab3/solution.html#for-loops",
    "title": "Lab 3: Analyzing functions (solutions)",
    "section": "4.1 For loops",
    "text": "4.1 For loops\nWhen programming your data analysis, you often need to iterate over multiple elements of a collection. These elements could be rows of a data.frame, datasets inside a list, numbers inside a vector, etc. The iteration usually means that you apply the same code over each element of the collection and you don’t want to “copy-paste” the code for each element. Iterating over a collection is called a “for loop” in programming. A for loop consists of three components:\n\nA collection over which you want to iterate.\nA variable that keeps track of where you are in the collection in each iteration.\nThe body of the loop where you apply some code.\n\nImagine that you want to calculate the factorial of 10. The factorial of a number is simply the product of all positive numbers smaller or equal than the number you specify (and it is denote with a “!” after the number). For example, the factorial of 3 is \\(3! = 3 \\times 2 \\times 1\\). A simple way of calculating the factorial of 10 by using a for loop is:\nresult = 1\nfor(i in 1:10) {\n  result = result*i\n}\nIn this for loop, the collection is 1:10, the variable to keep track of the number is i and the body of the loop is result = result*i. This for loop shows a very typical pattern: we want to summarise some collection of numbers into a single number, in this case, result.\nAnother typical pattern is when we want to calculate the elements of a collection. In this case, it is a good practice to “pre-allocate” your output collection before looping, so R knows how much memory to allocate for this vector. For example,\nx = 1:10\ny = numeric(10)\nfor(i in 1:10) {\n  y[i] = exp(x[i])\n}\nIn this case, the result of the for loop (y) is a collection of 10 elements where each element is the exponential transformation of the element in x with the same position. Note that we specify before the loop that y will have 10 elements. Although this is not strictly required in this case, it is a good practice both to avoid errors and to make your code run faster.\nFor loops are not that common in R as in other languages The reason is that many mathematical and statistical functions are already, implicitly, looping over your collections. For examples, when you take the exponential (exp()) of a collection of numbers, it will produce a new collection which is the result of looping over the original collection. That is:\nx = 1:10\ny = exp(x)\nis equivalent to the previous loop described before. As you can see, this second option requires less code and it is easier to read, which is one of the reasons why R is such a greate language for working with data. In addition, if you rely on this implicit looping your code will run much faster.\nHowever, there may be situations where you really cannot avoid a for loop. For example, if you have collected multiple datasets and need to perform the same analysis on each dataset, you could store your datasets in a list and use a for loop to iterate over the different datasets.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCalculate the logarithm of the sequence 1:10, using first a for loop and then without a for loop.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWith a for loop it looks like this:\nfor (i in 1:10) {\n    log(i)\n    print(log(i))\n}\nNote that the print statement was added to print to screen. Without a foor loop it would be:\nlog(1:10)",
    "crumbs": [
      "Solutions",
      "Lab 3 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/solution.html#handling-missing-values-na",
    "href": "Lab3/solution.html#handling-missing-values-na",
    "title": "Lab 3: Analyzing functions (solutions)",
    "section": "4.2 Handling missing values (NA)",
    "text": "4.2 Handling missing values (NA)\nMissing values in data collection and analysis deserve special attention. You can get missing values during you data collection for various reasons, e.g. you missed the opportunity to take a measurement, you lose one of the replicates due to contamination, you wrote down a wrong value, or you even lost some of your data.\nThe most important thing you need to understand, is that in almost all cases of a missing value, you should not represent a missing value with a zero (0). This will throw your analysis out of balance and give erroneous results. A common way of representing missing data when you are performing data input, would be with a character like an asterisk (*) or a hyphen (-).\nMany of the functions that read data in R have an argument that allows you to select how you have represented a missing-value in your data. As an example the function read.csv (which reads a comma-delimited data file) would be used like this to read from a file named “your-data-file”:\nmyData &lt;- read.csv(\"you-data-file\", na.strings=c(\"*\",\"-\") )\nIn this case we instructed R, with the argument na.strings=c(\"*\",\"_\") to read our file, and substitute any occurence of an asterisk (*) or a hyphen(-) with an NA symbol.\nThe R languages has a special way of representing missing values in a dataset. A missing value is denoted with the symbol NA which stands for “Not Available”. By default, missing values will “propagate” throughout the calculations. For example, given two vectors of data:\nx = c(1,2,3)\ny = c(2,4,NA)\nWhen you combine these vectors (e.g. add them or multiply them) you will see that the third component is always NA\nx + y\nx*y\nWhen you calculate some statistical property of your data (e.g. mean, standard deviation) it will, by default, report NA if there is at least one missing value in your data\nmean(x)\nmean(y)\nMost statistical functions in R allow you to specify how to deal with missing values. Most often, you are given the option to ignore any missing values from the data when calculating an statistical property through an argument often called na.rm. For example, in order to get the mean of the non-missing values of y we need:\nmean(y, na.rm = TRUE)\nwhich, of course, is the mean of 2 and 4. However, other functions not have an option to handle NA even though you still need to make a decision on how to deal with them. For example, when you calculate the length of a dataset (length()) do you want to consider the whole data or only the non-missing values? This is not a trivial question and the answer on the context where you will use the result. In any case, if you want to remove the NA when calculating the length you need to be more creative. Fortunately, R offers the function is.na which returns a vector of TRUE or FALSE values corresponding to the index of mssing or non-missing data values in the vector y:\nis.na(y)\nNext a vector without NA can be obtained through:\nlength(y[!is.na(y)])\nWhich only gives 2 as the third element is missing. Remember that ! is a negation operator, so !is.na actually means “is not NA”.\nBy the way, you should not confuse NA with NaN which stands for “Not a Number”. An NaN is the result of either an expression with indeterminate form (e.g. 0/0 or Inf/Inf) or when a function is evaluated outside of its valid domain (e.g. sqrt(-1) or log(-1)).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGiven some data created from the following code c(25,1,10,89, NA, NA), calculate the mean value and the standard error of this mean (\\(s.e.m. = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation and \\(n\\) is the number of items) by ignoring missing values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndata &lt;- c(25,1,10,89, NA, NA)\nsd(data,na.rm=T)/sqrt(length(na.omit(data)))",
    "crumbs": [
      "Solutions",
      "Lab 3 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/solution.html#making-a-function",
    "href": "Lab3/solution.html#making-a-function",
    "title": "Lab 3: Analyzing functions (solutions)",
    "section": "4.3 Making a function",
    "text": "4.3 Making a function\nWhen you want to repeat a calculation for different data, it is best to code your calculations inside a function. R consists of many built-in functions, but sometimes you need to do a calculation that is not available in R. A function is defined by 4 elements\n\nThe name of the function. For example, in R there is a function that calculates the arithmetic mean of a vector of data and its name is mean. You should make sure that the name of your function does not coincide with existing functions, that it is not too long and that it conveys its meaning. You can check if a function already exist in the base or any of the packages you loaded through ?nameoffunction.\nThe arguments of the function. These are the variables that you need to pass to the function (i.e., inputs). The arguments are defined by a position and a name. Also, some arguments may have default values which means that you do not need to specify them every time you call the function. For example, the function mean, contains three arguments (x, trim and na.rm) but the last two have default values.\nThe body of the function. This is the actual code that the function will execute. The real mean function in R has some crytpic body that requires advanced knowledge of the language to understand. However, a more “naive” implementation of mean could be sum(x)/length(x). Note that the body of a function can consist of multiple lines.\nThe return value of the function. This is the result of applying the function on the arguments. By default, the result of the last line code in the body of the function is the return value of the function. You can also return from any point in the body with the function return() with the variable you want to return inside.\n\nThe R language specifies a particular syntax on how to build a function. For example, a naive_mean could be defined as:\nnaive_mean = function(x, na.remove = FALSE) {\n  total = sum(x, na.rm = na.remove)\n  n = length(x[!is.na(x)])\n  result = total/n\n  return(result)\n}\nIn this case, the function naive_mean has two arguments (x and na.remove) where the second argument has a default value of FALSE and the body consists of several lines of code. These are respectively the sum of the elements of x with the na.rm depending on whether you specified TRUE or FALSE in the na.remove argument; n that calculates the length of the vector x without NAs, and the calculation of the mean. The last statement returns the result. Notice that arguments are separated by commas and the body of the function is enclosed in curly braces {}. The name of the function is simply the name of the variable to which you assigned the function (i.e., naive_mean). You can see below that you can use this function in a similar manner to the built-in mean\nx = 1:10\nnaive_mean(x)\nNotice that we did not specify the value of na.remove as the default is ok in this case. However, if we had missing values, the NA would propagate to the output:\nx = c(1,2,NA,4)\nnaive_mean(x)\nSpecifying na.remove=FALSE can be used as a double check that there are no NAs in your vector. If they are present it forces us to make a decision about what to do with the NAs. Let’s say that, for the moment, we want to just remove the values that are NA from the calculation. In this case, we just change the value of the default parameter.\nnaive_mean(x, na.remove = TRUE)\nFor convenience, default parameters are specified by name rather than position. However we could have also said naive_mean(x,TRUE) or even naive_mean(x = x, na.remove = TRUE). All these forms of calling functions are OK, whether you choose one style or another is a matter of taste.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nBuild a function to calculate the standard deviation (\\(\\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n\\left(x_i - \\bar x\\right)^2}{n - 1}}\\)). Test your function with some data that includes missing values, and compare to the built in function for the standard deviation sd.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsigma.self = function(x,na.rm=F) {\n        mean.x = mean(x,na.rm=na.rm)\n        n = length(na.omit(x))\n        sd = sqrt(sum((x-mean.x)^2,na.rm=na.rm)/(n-1))\n        return(sd)\n}\n\n\n\n\n\n\nSuprisingly the base R does not have a built in function for the standard error of the mean (aka sem). The sem is defined as \\(\\frac{\\sigma}{\\sqrt(n)}\\).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake you own function for the sem and use your own home-made function of the standard deviation for that.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsem.self &lt;- function(x,na.rm=F) {\n        length.x &lt;- length(na.omit(x))\n        sigma.self(x,na.rm=na.rm)/sqrt(length(x))\n}\n\n\n\n\n\n\nAs you see you can call functions inside functions. It is recommended to divide the work you want to do into little functions that each carry out a specific task, and then combine those functions into a larger function that combines these tasks. This facilitates error checking.",
    "crumbs": [
      "Solutions",
      "Lab 3 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/material.html",
    "href": "Lab3/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "In this lab you will learn to analyse mathematical functions. This is an important step in ecological modelling. Next, we proceed with analysing and programming these functions in R. To do so, you will need more advanced programming skills such as for-loops, if-else statements and functions."
  },
  {
    "objectID": "Lab3/material.html#plotting-curves",
    "href": "Lab3/material.html#plotting-curves",
    "title": "1 Learning goals",
    "section": "3.1 Plotting curves",
    "text": "3.1 Plotting curves\nHere are the R commands used to generate Figure 3.2 in the book (p 74). They just use curve(), with add=FALSE (the default, which draws a new plot) and add=TRUE (adds the curve to an existing plot), particular values of from and to, and various graphical parameters (ylim, ylab, lty).\ncurve(2*exp(-x/2),from=0,to=7,ylim=c(0,2),ylab=\"\")\ncurve(2*exp(-x),add=TRUE,lty=4)\ncurve(x*exp(-x/2),add=TRUE,lty=2)\ncurve(2*x*exp(-x/2),add=TRUE,lty=3)\ntext(0.4,1.9,expression(paste(\"exponential: \",2*e^(-x/2))),adj=0)\ntext(4,.5,expression(paste(\"Ricker: \",x*e^(-x/2))))\ntext(4,1,expression(paste(\"Ricker: \",2*x*e^(-x/2))),adj=0)\ntext(2.8,0,expression(paste(\"exponential: \",2*e^(-x))))\nThe only new thing in this figure is the use of expression() to add a mathematical formula to an R graphic. text(x,y,\"x^2\") puts x^2 on the graph at position \\((x,y)\\); text(x,y,expression(x^2)) (no quotation marks) puts \\(x^2\\) on the graph. See ?plotmath or ?demo(plotmath) for (much) more information.\nAn alternate way of plotting the exponential parts of this curve:\nxvec = seq(0,7,length=100)\nexp1_vec = 2*exp(-xvec/2)\nexp2_vec = 2*exp(-xvec)\nplot(xvec,exp1_vec,type=\"l\",ylim=c(0,2),ylab=\"\")\nlines(xvec,exp2_vec,lty=4)\nFinally, if you have a more complicated function you could use sapply() to call this function along with appropriate parameter values. you could say:\nexpfun = function(x,a=1,b=1) {\n   a*exp(-b*x)\n }\nexp1_vec = sapply(xvec,expfun,a=2,b=1/2)\nexp2_vec = sapply(xvec,expfun,a=2,b=1)\nThe advantage of curve() is that you don’t have to define any vectors: the advantage of doing things the other way arises when you want to keep the vectors around to do other calculations with them.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nConstruct a curve that has a maximum at (\\(x=5\\), \\(y=1\\)). Write the equation, draw the curve in R, and explain how you got there.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChoose appropriate parameter values through eyeballing so that the chosen curves more or less match the data. Eyeballing means that you knowledge on the effect of parameter values on the shape of the function. Later we will use likelihood methods to estimate the parameter values. Plot the curves on top of the data using curve.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTime permitting repeat the previous three questions for three of the shapes datasets."
  },
  {
    "objectID": "Lab3/material.html#for-loops",
    "href": "Lab3/material.html#for-loops",
    "title": "1 Learning goals",
    "section": "4.1 For loops",
    "text": "4.1 For loops\nWhen programming your data analysis, you often need to iterate over multiple elements of a collection. These elements could be rows of a data.frame, datasets inside a list, numbers inside a vector, etc. The iteration usually means that you apply the same code over each element of the collection and you don’t want to “copy-paste” the code for each element. Iterating over a collection is called a “for loop” in programming. A for loop consists of three components:\n\nA collection over which you want to iterate.\nA variable that keeps track of where you are in the collection in each iteration.\nThe body of the loop where you apply some code.\n\nImagine that you want to calculate the factorial of 10. The factorial of a number is simply the product of all positive numbers smaller or equal than the number you specify (and it is denote with a “!” after the number). For example, the factorial of 3 is \\(3! = 3 \\times 2 \\times 1\\). A simple way of calculating the factorial of 10 by using a for loop is:\nresult = 1\nfor(i in 1:10) {\n  result = result*i\n}\nIn this for loop, the collection is 1:10, the variable to keep track of the number is i and the body of the loop is result = result*i. This for loop shows a very typical pattern: we want to summarise some collection of numbers into a single number, in this case, result.\nAnother typical pattern is when we want to calculate the elements of a collection. In this case, it is a good practice to “pre-allocate” your output collection before looping, so R knows how much memory to allocate for this vector. For example,\nx = 1:10\ny = numeric(10)\nfor(i in 1:10) {\n  y[i] = exp(x[i])\n}\nIn this case, the result of the for loop (y) is a collection of 10 elements where each element is the exponential transformation of the element in x with the same position. Note that we specify before the loop that y will have 10 elements. Although this is not strictly required in this case, it is a good practice both to avoid errors and to make your code run faster.\nFor loops are not that common in R as in other languages The reason is that many mathematical and statistical functions are already, implicitly, looping over your collections. For examples, when you take the exponential (exp()) of a collection of numbers, it will produce a new collection which is the result of looping over the original collection. That is:\nx = 1:10\ny = exp(x)\nis equivalent to the previous loop described before. As you can see, this second option requires less code and it is easier to read, which is one of the reasons why R is such a greate language for working with data. In addition, if you rely on this implicit looping your code will run much faster.\nHowever, there may be situations where you really cannot avoid a for loop. For example, if you have collected multiple datasets and need to perform the same analysis on each dataset, you could store your datasets in a list and use a for loop to iterate over the different datasets.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCalculate the logarithm of the sequence 1:10, using first a for loop and then without a for loop."
  },
  {
    "objectID": "Lab3/material.html#handling-missing-values-na",
    "href": "Lab3/material.html#handling-missing-values-na",
    "title": "1 Learning goals",
    "section": "4.2 Handling missing values (NA)",
    "text": "4.2 Handling missing values (NA)\nMissing values in data collection and analysis deserve special attention. You can get missing values during you data collection for various reasons, e.g. you missed the opportunity to take a measurement, you lose one of the replicates due to contamination, you wrote down a wrong value, or you even lost some of your data.\nThe most important thing you need to understand, is that in almost all cases of a missing value, you should not represent a missing value with a zero (0). This will throw your analysis out of balance and give erroneous results. A common way of representing missing data when you are performing data input, would be with a character like an asterisk (*) or a hyphen (-).\nMany of the functions that read data in R have an argument that allows you to select how you have represented a missing-value in your data. As an example the function read.csv (which reads a comma-delimited data file) would be used like this to read from a file named “your-data-file”:\nmyData &lt;- read.csv(\"you-data-file\", na.strings=c(\"*\",\"-\") )\nIn this case we instructed R, with the argument na.strings=c(\"*\",\"_\") to read our file, and substitute any occurence of an asterisk (*) or a hyphen(-) with an NA symbol.\nThe R languages has a special way of representing missing values in a dataset. A missing value is denoted with the symbol NA which stands for “Not Available”. By default, missing values will “propagate” throughout the calculations. For example, given two vectors of data:\nx = c(1,2,3)\ny = c(2,4,NA)\nWhen you combine these vectors (e.g. add them or multiply them) you will see that the third component is always NA\nx + y\nx*y\nWhen you calculate some statistical property of your data (e.g. mean, standard deviation) it will, by default, report NA if there is at least one missing value in your data\nmean(x)\nmean(y)\nMost statistical functions in R allow you to specify how to deal with missing values. Most often, you are given the option to ignore any missing values from the data when calculating an statistical property through an argument often called na.rm. For example, in order to get the mean of the non-missing values of y we need:\nmean(y, na.rm = TRUE)\nwhich, of course, is the mean of 2 and 4. However, other functions not have an option to handle NA even though you still need to make a decision on how to deal with them. For example, when you calculate the length of a dataset (length()) do you want to consider the whole data or only the non-missing values? This is not a trivial question and the answer on the context where you will use the result. In any case, if you want to remove the NA when calculating the length you need to be more creative. Fortunately, R offers the function is.na which returns a vector of TRUE or FALSE values corresponding to the index of mssing or non-missing data values in the vector y:\nis.na(y)\nNext a vector without NA can be obtained through:\nlength(y[!is.na(y)])\nWhich only gives 2 as the third element is missing. Remember that ! is a negation operator, so !is.na actually means “is not NA”.\nBy the way, you should not confuse NA with NaN which stands for “Not a Number”. An NaN is the result of either an expression with indeterminate form (e.g. 0/0 or Inf/Inf) or when a function is evaluated outside of its valid domain (e.g. sqrt(-1) or log(-1)).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGiven some data created from the following code c(25,1,10,89, NA, NA), calculate the mean value and the standard error of this mean (\\(s.e.m. = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation and \\(n\\) is the number of items) by ignoring missing values."
  },
  {
    "objectID": "Lab3/material.html#making-a-function",
    "href": "Lab3/material.html#making-a-function",
    "title": "1 Learning goals",
    "section": "4.3 Making a function",
    "text": "4.3 Making a function\nWhen you want to repeat a calculation for different data, it is best to code your calculations inside a function. R consists of many built-in functions, but sometimes you need to do a calculation that is not available in R. A function is defined by 4 elements\n\nThe name of the function. For example, in R there is a function that calculates the arithmetic mean of a vector of data and its name is mean. You should make sure that the name of your function does not coincide with existing functions, that it is not too long and that it conveys its meaning. You can check if a function already exist in the base or any of the packages you loaded through ?nameoffunction.\nThe arguments of the function. These are the variables that you need to pass to the function (i.e., inputs). The arguments are defined by a position and a name. Also, some arguments may have default values which means that you do not need to specify them every time you call the function. For example, the function mean, contains three arguments (x, trim and na.rm) but the last two have default values.\nThe body of the function. This is the actual code that the function will execute. The real mean function in R has some crytpic body that requires advanced knowledge of the language to understand. However, a more “naive” implementation of mean could be sum(x)/length(x). Note that the body of a function can consist of multiple lines.\nThe return value of the function. This is the result of applying the function on the arguments. By default, the result of the last line code in the body of the function is the return value of the function. You can also return from any point in the body with the function return() with the variable you want to return inside.\n\nThe R language specifies a particular syntax on how to build a function. For example, a naive_mean could be defined as:\nnaive_mean = function(x, na.remove = FALSE) {\n  total = sum(x, na.rm = na.remove)\n  n = length(x[!is.na(x)])\n  result = total/n\n  return(result)\n}\nIn this case, the function naive_mean has two arguments (x and na.remove) where the second argument has a default value of FALSE and the body consists of several lines of code. These are respectively the sum of the elements of x with the na.rm depending on whether you specified TRUE or FALSE in the na.remove argument; n that calculates the length of the vector x without NAs, and the calculation of the mean. The last statement returns the result. Notice that arguments are separated by commas and the body of the function is enclosed in curly braces {}. The name of the function is simply the name of the variable to which you assigned the function (i.e., naive_mean). You can see below that you can use this function in a similar manner to the built-in mean\nx = 1:10\nnaive_mean(x)\nNotice that we did not specify the value of na.remove as the default is ok in this case. However, if we had missing values, the NA would propagate to the output:\nx = c(1,2,NA,4)\nnaive_mean(x)\nSpecifying na.remove=FALSE can be used as a double check that there are no NAs in your vector. If they are present it forces us to make a decision about what to do with the NAs. Let’s say that, for the moment, we want to just remove the values that are NA from the calculation. In this case, we just change the value of the default parameter.\nnaive_mean(x, na.remove = TRUE)\nFor convenience, default parameters are specified by name rather than position. However we could have also said naive_mean(x,TRUE) or even naive_mean(x = x, na.remove = TRUE). All these forms of calling functions are OK, whether you choose one style or another is a matter of taste.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nBuild a function to calculate the standard deviation (\\(\\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n\\left(x_i - \\bar x\\right)^2}{n - 1}}\\)). Test your function with some data that includes missing values, and compare to the built in function for the standard deviation sd.\n\n\n\nSuprisingly the base R does not have a built in function for the standard error of the mean (aka sem). The sem is defined as \\(\\frac{\\sigma}{\\sqrt(n)}\\).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake you own function for the sem and use your own home-made function of the standard deviation for that.\n\n\n\nAs you see you can call functions inside functions. It is recommended to divide the work you want to do into little functions that each carry out a specific task, and then combine those functions into a larger function that combines these tasks. This facilitates error checking."
  },
  {
    "objectID": "Lab2/no_solution.html",
    "href": "Lab2/no_solution.html",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "",
    "text": "This lab will teach you\n\nTo read in data and reshape it so it matches your needs\nTo make different types of graphs that you need for data exploration and presentation purposes.\n\nIt does so by reproducing the figures shown in Chapter 2 and more. The exercises, which will be more difficult than those in Lab 1, will typically involve variations on the figures shown in the text. You will work through reading in the different data sets and constructing the figures shown, or variants of them. It would be even better to work through reading in and making exploratory plots of your own data.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#reading-data",
    "href": "Lab2/no_solution.html#reading-data",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "2.1 Reading data",
    "text": "2.1 Reading data\nFind the file called seedpred.dat. It is in the right format (plain text, long format), so you can just read it in with\ndata = read.table(\"seedpred.dat\", header = TRUE)\nAdd the variable available to the data frame by combining taken and remaining (using the $ symbol):\ndata$available = data$taken + data$remaining\nPitfall #1: finding your file If R responds to your read.table() or read.csv() command with an error like\nError in file(file, \"r\") : unable to open connection In addition: Warning message: cannot open file 'myfile.csv'\nit means it can’t find your file, probably because it isn’t looking in the right place. By default, R’s working directory is the directory in which the R program starts up, which is by default something like C:/Program Files/R/rw2010/bin. (R uses / as the [operating-system-independent] separator between directories in a file path.) If you are using Rstudio you have several options:\n\nGo to ‘Session’ and click ‘Set working directory’\nUse the setwd() command to set the working directory.\nWork within an RStudio project so that the working directory is automatically set to the folder.\n\ngetwd() tells you what the current working directory is. While you could just throw everything on your desktop, it’s good to get in the habit of setting up a separate working directory for different projects, so that your data files, metadata files, R script files, and so forth, are all in the same place. Depending on how you have gotten your data files onto your system (e.g. by downloading them from the web), Windows will sometimes hide or otherwise screw up the extension of your file (e.g. adding .txt to a file called mydata.dat). R needs to know the full name of the file, including the extension.\nFor example to set a working directory:\nsetwd(\"D:/Bolker/labs/\")\nPitfall #2: checking number of fields In some cases the number of fields is not the same for every line in your data file. In that case you may get an error like:\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\nor\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 1 did not have 5 elements\nIf you need to check on the number of fields that R thinks you have on each line, use\ncount.fields(\"myfile.dat\",sep=\",\")\n(you can omit the sep=\",\" argument if you have whitespace- rather than comma delimited data). If you are checking a long data file you can try\ncf = count.fields(\"myfile.dat\",sep=\",\")\nwhich(cf!=cf[1])\nto get the line numbers with numbers of fields different from the first line. By default R will try to fill in what it sees as missing fields with NA (“not available”) values; this can be useful but can also hide errors. You can try\nmydata &lt;- read.csv(\"myfile.dat\", fill = FALSE)\nto turn off this behavior; if you don’t have any missing fields at the end of lines in your data this should work.\nIf your file is a comma separated file, you can also use read.csv. This function has set some arguments to default, e.g. the seperator is a comma in this case (sep=\",\")\nPitfall #3: List separator It may happen when you save a file in excel as .csv that the decimals are not indicated by a dot . but by a comma , and that the list separator is a semi-colon. You have two options:\n\nUse read.csv2 that will automatically use ; as separator.\nChange the settings of MS Office to use . as decimal separator (this will force Excel to save csv with ,) and you will never encounter this problem in the future.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#checking-data",
    "href": "Lab2/no_solution.html#checking-data",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "2.2 Checking data",
    "text": "2.2 Checking data\nR will automatically recognize the type of data of the columns that are read in, but sometimes it goes wrong. To check that all your variables have been classified correctly:\nsapply(data, class)\nThis applies the class() command, which identifies the type of a variable, to each column in your data. Alternatively,\nsummary(data)\ncan be very helplful to get a glance of the characteristics of the data.\nNon-numeric missing-variable strings (such as a star, *) will also make R misclassify. Use na.strings in your read.table() command:\nmydata &lt;- read.table(\"mydata.dat\", na.strings = \"*\")\nYou can specify more than one value with (e.g.) na.strings=c(“”,”**”,“bad”,“-9999”).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTry out head(), summary() and str() on data; make sure you understand the results.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#reshaping-data",
    "href": "Lab2/no_solution.html#reshaping-data",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "2.3 Reshaping data",
    "text": "2.3 Reshaping data\nReshaping a dataframe is an important part of data exploration. For example, for making field recordings it is convenient to have the measurements of different plots or transects in different columns on your sheet. This is called a wide format. However, for data analysis in R (and other statistical programs) you need to have all measurements in one column, with an additional column indicating which plot a measurement belongs to (so called long format).\nBelow we create a dataframe in long format and reshape it. Here are the commands to generate the data frame I used as an example in the text (I use LETTERS, a built-in vector of the capitalized letters of the alphabet, and runif(), which picks a specified number of random numbers from a uniform distribution between 0 and 1. The command round(x,3) rounds x to 3 digits after the decimal place.):\nloc = factor(rep(LETTERS[1:3],2))\nday = factor(rep(1:2,each=3))\nval = round(runif(6),3)\nd = data.frame(loc,day,val)\nThis data set is stored in long format. To go to wide format, we first need to (install and) load the library tidyr. In Lab 1 you learned how to install packages. You can load a package by library() or require(). Thus to use an additional package it must be (i) installed on your machine (with install.packages()) or through the menu system and (ii) loaded in your current R session (with library()):\nlibrary(tidyr)\nd2 = pivot_wider(d, names_from = day, values_from = val)\nd2\nThis means that a column will be created for every value in day, such that every row corresponds to a value of loc and the values filled-in are taken from val\nTo go back to long format, we simply write:\npivot_longer(d2, cols = -loc, names_to = \"day\", values_to = \"val\")\ncols = -loc means all columns except loc are gathered into long format, names_to = \"day\" puts the column names into a new column called day and values_to = \"value\" puts the values into a new column called val.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake a new data.frame similar to the previous data.frame d, but with an extra column month consisting of two levels. The value of month should be 1 for the first records and 2 for the second. Next, reshape to wide format and back to long format. The long format is what we commonly use in statistics.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#advanced-data-types-time-permitting",
    "href": "Lab2/no_solution.html#advanced-data-types-time-permitting",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "2.4 Advanced data types (Time permitting)",
    "text": "2.4 Advanced data types (Time permitting)\nWhile you can usually get away by coding data in not quite the right way - for example, coding dates as numeric values or categorical variables as strings - R tries to “do the right thing” with your data, and it is more likely to do the right thing the more it knows about how your data are structured.\nFactors instead of strings\nSometimes you wan to keep strings as \"character\" and sometimes you want to convert them to \"factor\": if your strings are unique identifiers and you want to make them factors, the default read.table will not convert them automatically. You can specify the class to be assume with each column with the argument colClasses (see below).\nFactors instead of numeric values\nSometimes you have numeric labels for data that are really categorical values - for example if your sites or species have integer codes (often data sets will have redundant information in them, e.g. both a species name and a species code number). It’s best to specify appropriate data types, so use colClasses to force R to treat the data as a factor. For example, if we wanted to make tcum a factor instead of a numeric variable:\ndata2 = read.table(\"seedpred.dat\", header = TRUE, colClasses = c(rep(\"factor\", 2), rep(\"numeric\", 3)))\nsapply(data2, class)\nn.b.: by default, R sets the order of the factor levels alphabetically. You can find out the levels and their order in a factor f with levels(f). If you want your levels ordered in some other way (e.g. site names in order along some transect), you need to specify this explicitly. Most confusingly, R will sort strings in alphabetic order too, even if they represent numbers.\nThis is OK:\nf = factor(1:10)\nlevels(f)\nHowever, if we create a factor f through:\nf = factor(as.character(1:10))\nlevels(f)\nit will put the 10-th level as second. You can fix the levels by using the levels argument in factor() to tell R explicitly what you want it to do, e.g.:\nf = factor(as.character(1:10), levels = c(1:10))\nSo the levels=1:10 argument explicitly states that there are ten levels and that the order of these levels is 1,2,3,4,5,6,7,8,9,10. The levels argument needs a vector of unique numeric values or character strings (c(1:10)).\nAdditionally, if you create a factor with levels ‘north’, ‘middle’ and ‘south’ they will be sorted by alphabet\nx = c(\"north\", \"middle\", \"south\")\nfactor(x)\nIf you want to sort them geographically instead of alphabetically you again can use the levels argument. Additionally, you can add levels that were not included in the vector itself:\nf = factor(x, levels = c(\"far_north\", \"north\", \"middle\", \"south\"))\nLikewise, if your data contain a subset of integer values in a range, but you want to make sure the levels of the factor you construct include all of the values in the range, not just the ones in your data. Use levels again:\nf = factor(c(3, 3, 5, 6, 7, 8, 10), levels = 3:10)\nFinally, you may want to get rid of levels that were included in a previous factor but are no longer relevant:\nf = factor(c(\"a\", \"b\", \"c\", \"d\"))\nf2 = f[1:2]\nlevels(f2)\nNote that a character vector is returned displaying the different levels in the factor f.\nf2 = factor(as.character(f2))\nlevels(f2)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIllustrate the effects of the levels command by plotting the factor f=factor(c(3,3,5,6,7,8,10)) as created with and without intermediate levels, i.e. with and without levels c(1:10). For an extra challenge, draw them as two side-by-side subplots. (Use par(mfrow=c(1,1)) to restore a full plot window.)\n\n\n\nDates (time permitting)\nDates and times can be tricky in R, but you can handle your dates as type Date within R rather than using Julian days\nYou can use colClasses=\"Date\" within read.table() to read in dates directly from a file, but only if your dates are in four-digit-year/month/day (e.g. 2005/08/16 or 2005-08-16) format; otherwise R will either butcher your dates or complain\nError in fromchar(x) : character string is not in a standard unambiguous format\nIf your dates are in another format in a single column, read them in as character strings (colClasses=\"character\" or using as.is) and then use as.Date(), which uses a very flexible format argument to convert character formats to dates:\nas.Date(c(\"1jan1960\", \"2jan1960\", \"31mar1960\", \"30jul1960\"),\n        format = \"%d%b%Y\")\nas.Date(c(\"02/27/92\", \"02/27/92\", \"01/14/92\", \"02/28/92\", \"02/01/92\"),\n        format = \"%m/%d/%y\")\nThe most useful format codes are %m for month number, %d for day of month, %j% for Julian date (day of year), %y% for two-digit year (dangerous for dates before 1970!) and %Y% for four-digit year; see ?strftime for many more details. If you have your dates as separate (numeric) day, month, and year columns, you actually have to squash them together into a character format. This can be done with paste(), using sep=\"/\" to specify that the values should be separated by a slash and then convert them to dates:\nyear = c(2004,2004,2004,2005)\nmonth = c(10,11,12,1)\nday = c(20,18,28,17)\ndatestr = paste(year,month,day,sep=\"/\")\ndate = as.Date(datestr)\ndate\nWhen you want to split a date to month, year and day, you can use ‘strsplit’:\ndate.c = as.character(date)\ndate.char = strsplit(date.c, \"-\" )\nWhich you subsequently can turn in to multiple colums through matrix:\ndat.mat = matrix(unlist(date.char), ncol=3, byrow=TRUE)\nAlthough R prints the dates in date out so they look like a vector of character strings, they are really dates: class(date) will give you the answer \"Date\". Note that when using the dat.mat these are characters.\nPitfall #4 quotation marks in character variables If you have character strings in your data set with apostrophes or quotation marks embedded in them, you have to get R to ignore them. I used a data set recently that contained lines like this: Western Canyon|valley|Santa Cruz|313120N|1103145WO'Donnell Canyon\nI used\ndata3 = read.table(\"datafile\", sep = \"|\", quote = \"\")\nto tell R that | was the separator between fields and that it should ignore all apostrophes/single quotations/double quotations in the data set and just read them as part of a string.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#accessing-data",
    "href": "Lab2/no_solution.html#accessing-data",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "2.5 Accessing data",
    "text": "2.5 Accessing data\nTo access individual variables within your data set use mydata$varname or mydata[,n] or mydata[,\"varname\"] where n is the column number and varname is the variable name you want. You can also use attach(mydata) to set things up so that you can refer to the variable names alone (e.g. varname rather than mydata$varname). However, beware: if you then modify a variable, you can end up with two copies of it: one (modified) is a local variable called varname, the other (original) is a column in the data frame called varname: it’s probably better not to attach a data set, or only until after you’ve finished cleaning and modifying it. Furthermore, if you have already created a variable called varname, R will find it before it finds the version of varname that is part of your data set. Attaching multiple copies of a data set is a good way to get confused: try to remember to detach(mydata) when you’re done.\nHere some examples to get the column with name ‘species’\ndata[,\"species\"]\ndata[,1]\ndata$species # recommended! You explictly define the dataframe and name of the column\nTo access data that are built in to R or included in an R package (which you probably won’t need to do often), say\ndata(dataset)\n(data() by itself will list all available data sets.)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#scatter-plot",
    "href": "Lab2/no_solution.html#scatter-plot",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.1 Scatter plot",
    "text": "3.1 Scatter plot\nFrom the previous lab you may remember that the base plot function in R is plot. The function plot takes a number of arguments but at least you need to specify the x and the y. If you refer to x and y by the column name of a data.frame you need to specify the name of the data.frame as well through data.\nplot(taken ~ available,data=data)\nThe graph above may not be very useful as it does not show how many datapoints are underlying every combination of seeds taken and seeds available. The function jitter adds small noise to a numeric vector which makes that observations that have the same combination of seeds available and taken are plotted at a slightly different location.\nplot(jitter(taken)~jitter(available),xlab=\"Seeds available\",\n     ylab=\"Seeds taken\",data=data)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#bubble-plot",
    "href": "Lab2/no_solution.html#bubble-plot",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.2 Bubble plot",
    "text": "3.2 Bubble plot\nSometimes you have multiple variables in the dataset that you want to explore simultaneously. For example, you want to superimpose information on how often certain combinations of the number seeds available versus the number of seeds taken occur. For this a bubble or jitter plot may be useful. The bubble plot is a normal plot with the size of the points representing the number of observations for a given combination of seeds available and seeds taken.\nTo get this information we first need to make a summary table which we can get through:\nt1 = table(data$available, data$taken)\nThis table needs to changed to a long format in order to be useful for the plotting command. Change the table to a long format with seeds available and seeds taken as columns. I recommend that you print every intermediate variable and try to understand what is happening.\nt2 = as.data.frame(t1) # From table to data.frame (this pivots to longer format)\nt2 = rename(t2, `Seeds available` = \"Var1\",\n            `Seeds taken`         = \"Var2\",\n            val                   = \"Freq\")\nNow we can make a plot with the size of the bubbles proportional for the number of observations. Since the columnnames have a space in the string we should use the quotes.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=val)\nThe argument cex controls the size of the points. As you see the size of the bubbles are bit too large, so we need to adjust it. In addition, we need to adjust the scaling of the axis.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\nWe could add the number of observations to the plot (plot) using the command text. The function text needs at least three arguments, the x position and the y position of the text and the text to be printed at this position. text allows vectors for each of those arguments. Therefore we can write:\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ntext(t2[,1],t2[,2],t2[,3])\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the size of the printed numbers and remove the zeros from the graph.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the color of the points and the symbols. Consult Lab 1 how to do this",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#bar-plot-with-error-bars",
    "href": "Lab2/no_solution.html#bar-plot-with-error-bars",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.3 Bar plot (with error bars)",
    "text": "3.3 Bar plot (with error bars)\nThe command to produce the barplot (Figure 3) was:\nbarplot(t(log10(t1 + 1)), beside = TRUE, legend = TRUE, xlab = \"Available\",\n ylab = \"log10(1+# observations)\")\nop = par(xpd = TRUE)\ntext(34.5, 3.05, \"Number taken\")\npar(op)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRestricting your analysis to only the observations with 5 seeds available, create a barplot showing the distribution of number of seeds taken broken down by species. Choose whether you do this with ggplot2 or through the base plot function.\n\n\n\nTo add error bars to the barplot, one need to calculate the standard error of the means. We want to plot the standard error on top of the fraction seeds taken\nFirst, compute the fraction taken:\ndata$frac_taken = data$taken/data$available\nComputing the mean fraction taken for each number of seeds available, using the tapply() function: tapply() (“table apply”, pronounced “t apply”), is an extension of the table() function; it splits a specified vector into groups according to the factors provided, then applies a function (e.g. mean() or sd()) to each group. This idea of applying a function to a set of objects is a very general, very powerful idea in data manipulation with R; in due course we’ll learn about apply() (apply a function to rows and columns of matrices), lapply() (apply a function to lists), sapply() (apply a function to lists and simplify), and mapply() (apply a function to multiple lists). For the present, though,\nmean_frac_by_avail = tapply(data$frac_taken, data$available, mean)\ncomputes the mean of frac_taken for each group defined by a different value of available. R automatically converts available into a factor temporarily for this purpose. If you want to compute the mean by group for more than one variable in a data set, use aggregate(). We can also calculate the standard errors, \\(\\frac{\\sigma}{\\sqrt(n)}\\):\nn_by_avail = table(data$available)\nse_by_avail = tapply(data$frac_taken, data$available, sd)/\n              sqrt(n_by_avail)\nFirst we plot a barplot after which we add the error bars. The error bars can be drawn by using the function arrows that have an angle between the shaft of the angle and the edge of 90 degrees. To position the error bars at the middle of the bars we need to retrieve those positions from the barplot command. This can be done through assigning a name, e.g. bara to the barplot object and using those positions as x coordinates.\nbara = barplot(mean_frac_by_avail,ylim=c(0,0.09))\n# hack: we draw arrows but with very special \"arrowheads\"\narrows(bara[,1],mean_frac_by_avail-se_by_avail,bara[,1],\n       mean_frac_by_avail+se_by_avail, length=0.05, angle=90, code=3)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#histogram-by-species",
    "href": "Lab2/no_solution.html#histogram-by-species",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.4 Histogram by species",
    "text": "3.4 Histogram by species\nTo make a histogram we can use the function hist:\nhist(data$frac_taken)\nTo draw a histogram per species, we need to split the data into a list with each element representing a species.\ndata.s = split(data$frac_taken,list(data$species))\nNext, we use sapply to plot the histograms.\npar(mfrow=c(4,2),oma=c(0,0,0,0),mar=c(4,4,0.1,0.1))\nsapply(data.s,hist,main=\"\")\n# or equivalently\nfor (i in 1:8){\n  hist(data.s[[i]],main=\"\")\n}",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#multi-line-plots",
    "href": "Lab2/no_solution.html#multi-line-plots",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.5 Multi-line plots",
    "text": "3.5 Multi-line plots\nTo illustrate how to make a multiline plot, we will read a different dataset\ndata.meas = read.table(\"ewcitmeas.dat\", header = TRUE, na.strings = \"*\")\nyear, mon, and day were read in as integers: I’ll create a date variable as described above. For convenience, I’m also defining a variable with the city names.\ndate = as.Date(paste(data.meas$year + 1900, data.meas$mon, data.meas$day, sep = \"/\"))\ncity_names = colnames(data.meas)[4:10]\nLater on it will be useful to have the data in long format. As in previous examples we will use the function pivot_longer().\ndata.meas= cbind(data.meas,date)\ndata_long = pivot_longer(select(data.meas, -day, -mon, -year), London:Sheffield,\n                         names_to = \"city\", values_to = \"incidence\")\nWe can make a plot with multiple lines as follows. We first setup the plotting region using plot followed by the function lines to add lines to the existing plot. Note that in the plotting command type=\"l\" is used to specify that lines are drawn instead of point (type=\"p\", the default). A legend can be added by adding the function legend\ndata_long.s = split(data_long,data_long$city)\nplot(incidence ~ date,col=1,type=\"l\",\n     data=data_long[data_long$city == \"London\",])\n\nunique.city = unique(data_long$city)\nfor (i in 2:length(unique.city)){\n  lines(incidence ~ date,type=\"l\",\n        data=data_long[data_long$city == unique.city[i],],col=i)\n}\nlegend(\"topright\",legend=unique.city,col=1:8,lty=1)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#histogram-and-density-plots",
    "href": "Lab2/no_solution.html#histogram-and-density-plots",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.6 Histogram and density plots",
    "text": "3.6 Histogram and density plots\nI’ll start by just collapsing all the incidence data into a single, logged, non-NA vector (in this case I have to use c(as.matrix(x)) to collapse the data and remove all of the data frame information):\nallvals = na.omit(c(as.matrix(data.meas[, 4:10])))\nlogvals = log10(1 + allvals)\nThe histogram (hist() command is fairly easy: the only tricks are to leave room for the other lines that will go on the plot by setting the y limits with ylim, and to specify that we want the data plotted as relative frequencies, not numbers of counts (freq=FALSE or prob=TRUE). This option tells R to divide by total number of counts and then by the bin width, so that the area covered by all the bars adds up to 1. This scaling makes the vertical scale of the histogram compatible with a density plot, or among different histograms with different number of counts or bin widths.\nhist(logvals, col = \"gray\", main = \"\", xlab = \"Log weekly incidence\",\n ylab = \"Density\", freq = FALSE, ylim = c(0, 0.6))\nAdding lines for the density is straightforward, since R knows what to do with a density object - in general, the lines command just adds lines to a plot.\nlines(density(logvals), lwd = 2)\nlines(density(logvals, adjust = 0.5), lwd = 2, lty = 2)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#scaling-data",
    "href": "Lab2/no_solution.html#scaling-data",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.7 Scaling data",
    "text": "3.7 Scaling data\nScaling the incidence in each city by the population size, or by the mean or maximum incidence in that city, begins to get us into some non-trivial data manipulation. This process may actually be easier in the wide format. Several useful commands: * rowMeans(), rowSums(), colMeans(), and colSums() will compute the means or sums of columns efficiently. In this case we would do something like colMeans(data[,4:10]) to get the mean incidence for each city.\n\napply() is the more general command for running some command on each of a set of rows or columns. When you look at the help for apply() you’ll see an argument called MARGIN, which specifies whether you want to operate on rows (1) or columns (2). For example, apply(data[,4:10],1,mean) is the equivalent of rowMeans(data[,4:10]), but we can also easily say (e.g.) apply(data[,4:10],1,max) to get the maxima instead. Later, when you’ve gotten practice defining your own functions, you can apply any function - not just R’s built-in functions.\nscale() is a function for subtracting and dividing specified amounts out of the columns of a matrix. It is fairly flexible: scale(x,center=TRUE,scale=TRUE) will center by subtracting the means and then scale by dividing by the standard errors of the columns. Fairly obviously, setting either to FALSE will turn off that part of the operation. You can also specify a vector for either center or scale, in which case scale() will subtract or divide the columns by those vectors instead.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFigure out how to use apply() and scale() to scale all columns so they have a minimum of 0 and a maximum of 1 (hint: subtract the minimum and divide by (max-min)).",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#box-and-whisker-and-violin-plots",
    "href": "Lab2/no_solution.html#box-and-whisker-and-violin-plots",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.8 Box-and-whisker and violin plots",
    "text": "3.8 Box-and-whisker and violin plots\nBy this time, box-and-whisker and violin plots will (I hope) seem easy. Since the labels get a little crowded (R is not really sophisticated about dealing with axis labels-crowded labels), I’ll use the substr() (substring) command to abbreviate each city’s name to its first three letters.\ncity_abbr = substr(city_names, 1, 3)\nThe boxplot() command uses a formula - the variable before the ~ is the data and the variable after it is the factor to use to split the data up.\nboxplot(log10(1 + incidence) ~ city, data = data_long, ylab = \"Log(incidence+1)\",\n names = city_abbr)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#pair-plot",
    "href": "Lab2/no_solution.html#pair-plot",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "3.9 Pair plot",
    "text": "3.9 Pair plot\nFirst let’s make sure the earthquake data are accessible:\ndata(quakes)\nLuckily, most of the plots I drew in this section are fairly automatic. To draw a scatterplot matrix, just use pairs() (base):\npairs(quakes, pch = \".\")\n(pch=\".\" marks the data with a single-pixel point, which is handy if you are fortunate enough to have a really big data set).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGenerate three new plots based on one of the data sets in this lab, or on your own data.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#scatter-plot-1",
    "href": "Lab2/no_solution.html#scatter-plot-1",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.1 Scatter plot",
    "text": "4.1 Scatter plot\nIn ggplot a scatterplot can be made as follows\nlibrary(ggplot2)\n    ggplot(data=t2)+\n        geom_point(aes(x = `Seeds available`, y = `Seeds taken`,\n                       size = log(val)/2))+\n      ylab(\"Seeds taken\")+\n      xlab(\"Seeds available\")\n    # or alternatively labs(x=\"Seeds taken\",y=\"Seeds available\")\nIn ggplot2 you need to specify the dataset the variables come from. You do this through data=.... Next you specify the type of plot you want. For example, a point plot can be specified through geom_point. Within geom_point, you need to specify the aesthetics (aes(x...,y...)) which determines what is plotted on the axes of the point plot. For example aes(x=x,y=y) put column x on the x-axis and column y on the y-axis.\nIf data is specified in the ggplot statement, it means that all plotting commands below ggplot(...)+ use that dataframe as reference. If the size command is put inside the aes then size is dependent on some variable, if put outside the aes it requires a single value (similarly for e.g. colour, linetype and shape). New commands can be added to the first statement (ggplot) by adding a + after each line. If a column name contains a space you can refer to it by putting it between backticks: ...",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#bar-plot-with-error-bars-1",
    "href": "Lab2/no_solution.html#bar-plot-with-error-bars-1",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.2 Bar plot (with error bars)",
    "text": "4.2 Bar plot (with error bars)\nA barplot can be created with ggplot2 as follows:\nggplot(data=t2)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),\n           stat=\"identity\",position=position_dodge())\nAgain through aes we specify what is on the x and y. Through fill we subdivide the bars by the values in taken. stat=identity expresses that the values assigned to y will be used (compare stat=\"count\"). Through specifying position_dodge() bars are printed side by side instead of stacked bars (position_fill()).\nMore impressively, the ggplot package can automatically plot a barplot of a three-way cross-tabulation (one barplot per species): try\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = as.data.frame(t1.species) %&gt;%\n                rename(`Seeds available` = \"Var1\",\n                       `Seeds taken`     = \"Var2\",\n                       species           = \"Var3\",\n                       val               = \"Freq\")\n\nggplot(data=t1.species)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),stat=\"identity\",\n           position=position_dodge())+\n  facet_wrap(~species)+\n  coord_flip()\nwith facet_wrap a sequence of panels is made a specified by the variable behind the ~. The coord_flip rotates the plot.",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#histogram-by-species-1",
    "href": "Lab2/no_solution.html#histogram-by-species-1",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.3 Histogram by species",
    "text": "4.3 Histogram by species\nTo make a histogram with ggplot2 you can get the frequencies less easily, so will be plot the counts\nggplot(data=data)+\ngeom_bar(aes(x=frac_taken),stat=\"count\")+\n  facet_wrap(~ species)\nggplot(data=data,aes(x=frac_taken))+\n  geom_histogram(aes(y = ..density..))+\n  facet_wrap(~species)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#multiple-line-plots",
    "href": "Lab2/no_solution.html#multiple-line-plots",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.4 Multiple-line plots",
    "text": "4.4 Multiple-line plots\nWith ggplot2 we specify:\nggplot() +\n  geom_line(aes(x=date,y=incidence, colour=city),data=data_long)",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#histogram-and-density-plots-1",
    "href": "Lab2/no_solution.html#histogram-and-density-plots-1",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.5 Histogram and density plots",
    "text": "4.5 Histogram and density plots\nWith ggplot2 we specify:\nggplot()+\n  geom_histogram(aes(x=logvals,y=..density..))+\n  geom_density(aes(x=logvals,y=..density..))",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab2/no_solution.html#box-and-whisker-and-violin-plots-1",
    "href": "Lab2/no_solution.html#box-and-whisker-and-violin-plots-1",
    "title": "Lab 2: Exploratory data analysis and visualization",
    "section": "4.6 Box-and-whisker and violin plots",
    "text": "4.6 Box-and-whisker and violin plots\nWith ggplot2 we specify:\nggplot(data=data_long)+\n  geom_boxplot((aes(x=city,y=log10(incidence+1))))\nIf I want to make a violin plot, you can specify:\nggplot(data=data_long)+\n  geom_violin((aes(x=city,y=log10(incidence+1))))",
    "crumbs": [
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab10/solution.html",
    "href": "Lab10/solution.html",
    "title": "Lab 10",
    "section": "",
    "text": "In this practical you will learn\n\nThe concept of maximum marginal likelihood\nEstimate models with process and measurement errors\nEstimate multilevel non linear models with Gaussian quadrature\nThe Bayesian approach for the two models above\nSpecify informative priors using general knowledge",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/solution.html#model-with-measurement-error",
    "href": "Lab10/solution.html#model-with-measurement-error",
    "title": "Lab 10",
    "section": "3.1 Model with measurement error",
    "text": "3.1 Model with measurement error\nLet’s assume that the growth rates of trees within a forest follow a Gamma distribution and the measurements of this growth rate contain errors that follow a Normal distribution. This is the problem described in section 10.5.2 of the book. This is a model with two levels:\n\\[\n\\begin{align*}\n\\text{Level}~2~:~X_{true} &\\sim Gamma(a, s) \\\\\n\\text{Level}~1~:~X_{obs~~}  &\\sim Normal(X_{true}, \\sigma)\n\\end{align*}\n\\]\nWhere \\(X_{true}\\) are the true growth rates of the trees and \\(X_{obs}\\) are the measured growth rates. Notice that the level 1 uses as input the output of level 2 (\\(X_{true}\\)), which determines the order.\nIn this exercise we will work with synthetic data generated by stochastic simulation (see Chapter 5 of the book):\nset.seed(1001)\nx_true &lt;- rgamma(1000, shape = 3, scale = 10) # True growth rates\nx_obs  &lt;- rnorm(1000, mean = x_true, sd = 10) # Observed growth rates with error\nhist(x_obs, ylim = c(0,300))\nhist(x_true, add = TRUE, col = rgb(1,0,0,0.3))\nAccording to the general procedure described below, we can estimate \\(a\\) and \\(s\\) from level 2 by creating a marginal likelihood (integrating over level 1) and maximizing it (steps 2 - 3). This is shown in the next section. Once that estimation is done, we can estimate each of value of \\(X_{true}\\) by maximizing the conditional likelihood (step 4).\n\n3.1.1 Estimation of parameters\nThe marginal likelihood is defined as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{true,i} | a, s\\right) \\text{Normal}\\left(X_{obs,i} | X_{true,i} , \\sigma \\right) dX_{true,i}\n\\]\nThat is, for each observation we integrate over all the possible true values (that is the latent random variable we do not observe) to obtained a likelihood function that only contains observations and parameters. Because the measurement error is normal, we can reparameterized as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{obs,i} - \\epsilon_i | a, s\\right) \\text{Normal}\\left(\\epsilon_i |0 , \\sigma \\right) d\\epsilon_i}\n\\]\nWhere we replace \\(X_{true} = X_{obs} - \\epsilon\\). This form is how most multilevel models are expressed. In R the product of the two distributions is:\nprodfun &lt;- function(eps, a, s, sigma, x) {\n dgamma(x - eps, shape = a, scale = s)*dnorm(eps, mean = 0, sd =  sigma)\n}\nprodfun(eps = 1, a = 3, s = 10, sigma = 1, x = x_obs[1])\nWe can now integrate prodfun using the function integrate (note: this only works for one latent random variable, more advanced methods are needed when more latent variables are used):\nintegrate(f = prodfun, lower = -Inf, upper = Inf,\n          a = 3, s = 10, sigma = 1, x = x_obs[1],\n          rel.tol = 1e-6, abs.tol = 1e-6)$value\nWe can now build a function to compute the negative log marginal likelihood by applying this integral to each observations:\nNLML &lt;- function(x, a, s, sigma) {\n  # Marginal likelihood of each observation\n  ML &lt;- sapply(x, function(x) integrate(f = prodfun, lower = -Inf, upper = Inf,\n                                        rel.tol = 1e-6, abs.tol = 1e-6,\n               a = a, s = s, sigma = sigma, x = x)$value)\n  # Negative log marginal likelihood\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(x_obs, a = 3, s = 10, sigma = 1)\nNotice that this takes a bit longer than usual because we are doing 1000 numerical integrations. We can minimize the function NLML with mle2:\nlibrary(bbmle)\nfit &lt;- mle2(minuslogl = NLML,\n            start = list(a = 3, s = 10, sigma = 1),\n            lower = c(0,0,0),\n            data = list(x = x_obs), method = \"L-BFGS-B\")\nfit\nWe can now compare the true and estimated models for growth and error to see how well the estimation worked:\npars = coef(fit)\nplot(density(x_obs), ylim = c(0,0.045), xlab = \"Growth rate\",\n     ylab = \"Probability density\", main = \"\", las = 1, col = 3)\ncurve(dgamma(x, shape = 3, scale = 10), add = TRUE)\ncurve(dgamma(x, shape = pars[\"a\"], scale = pars[\"s\"]), add = TRUE, col = 2)\ncurve(dnorm(x, mean = 0, sd = 10), add = TRUE, lty = 2)\ncurve(dnorm(x, mean = 0, sd = pars[\"sigma\"]), add = TRUE, col = 2, lty = 2)\nlegend(\"topright\", c(\"Obs\", \"True growth\", \"Est growth\", \"True error\", \"Est error\"),\n       col = c(3, 1, 2, 1, 2), lty = c(1, 1, 1, 2, 2))\nWe can quickly estimate the confidence intervals using the quadratic approximation:\nconfint(fit, method = \"quad\")\nNotice that the true values (a = 3, s = 10 and sigma = 10) are within the confidence intervals, so the estimation has worked. We could also try building the likelihood profiles but that would take quite a while since this would require many more optimizations so we will skip this part.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with measurement errors\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd an exercise with measurement errors",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/solution.html#model-for-nested-data",
    "href": "Lab10/solution.html#model-for-nested-data",
    "title": "Lab 10",
    "section": "3.2 Model for nested data",
    "text": "3.2 Model for nested data\nTo keep it within the same theme, in this example we will look at a simple dataset of tree growth, but this time using real data. The data describes the growth in height of several individuals of Loblolly pine (Pinus taeda). We can load the data as follows:\nlibrary(ggplot2)\ndata(Loblolly)\nsummary(Loblolly)\n# eyeballing; hm = 60, b = 0.25, c = 12 (check  Figure 3.9 in the book)\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) + geom_point() +\n  stat_function(fun = function(x) 60/(1 + exp(-0.25*(x - 12))), color = \"black\")\nEach tree is identified by the column Seed and six measurements of height over the age of the tree are reported. The growth curve of each tree will be modeled using a logistic curve:\n\\[\nh(t) =  \\frac{h_m}{1 + e^{-b\\left(t - c \\right)}\n\\]\nwhere \\(h\\) is the height of the tree, \\(h_m\\) is the maximum height, \\(b\\) control the steepness of the curve and \\(c\\) is the age at which the half maximum height is reached (see Chapter 3 of the book, I will refer to these parameters as traits from here on) and \\(t\\) is the age of the tree. We want to know the average values for the three traits as well as how much they vary across individuals (variation in traits across individuals is often very relevant in ecology). In a mixed model, \\(h_m\\), \\(b\\) and \\(c\\) are assumed to vary across individuals assuming particular distributions (we will assume normal distributions which is the standard in mixed models).\nAs a first approach, we can try to estimate the curve for each tree independently. This is a good way to get a first estimate of how much the traits vary across individuals as well as the observation error. As we will see in the examples below, non-linear mixed models as simple as these ones already need quite some constraints in order to work.\n\n3.2.1 Stepwise procedure\nThe stepwise procedure is as follows:\n\nFit the model to each individual separately, using maximum likelihood.\nTreat the maximum likelihood estimates from each individuals as if they were observations and analyze them with a separate model.\n\nLet’s setup a function to fit the logistic growth curve to each tree:\nlogistic &lt;- function(b, c, h_m, t) {\n  h_m/(1 + exp(-b*(t - c)))\n}\nmle_fun &lt;- function(b, c, h_m, sigma, t, h) {\n  hmod = logistic(b, c, h_m, t)\n  -sum(dnorm(h, hmod, sigma, log = TRUE))\n}\nWe can test the function for the first tree:\nseeds = unique(Loblolly$Seed)\ntree1 = subset(Loblolly, Seed == seeds[1])\nmle_fun(c = 12, b = 0.25, h_m = 60, sigma = 1, t = tree1$age, h = tree1$height)\nLet’s estimate the parameters for this first tree:\nlibrary(bbmle)\nfit1 = mle2(mle_fun, start = list(c = 12, b = 0.25, h_m = 60, sigma = 1),\n     data = list(t = tree1$age, h = tree1$height))\nfit1\nLet’s repeat it for all trees:\ntraits = matrix(NA, ncol = 4, nrow = length(seeds))\ncolnames(traits) = c(\"b\", \"c\", \"h_m\", \"sigma\")\nfor(i in 1:length(seeds)) {\n  tree = subset(Loblolly, Seed == seeds[i])\n  fit = mle2(mle_fun, start = list(c= 12, b = 0.25, h_m = 60, sigma = 1),\n             lower = c(c = 0, b = 1e-4, h_m = 10, sigma = 1e-4),\n             upper = c(c = 30, b = 1, h_m = 80, sigma = 20),\n            data = list(t = tree$age, h = tree$height), method = \"L-BFGS-B\")\n  traits[i,] = coef(fit)\n}\ntraits\nLet’s look at the individual fits by adding the predicitons to the data:\nLoblolly = transform(Loblolly,\n                     hm1  = rep(traits[,\"h_m\"], each = 6),\n                     c1  = rep(traits[,\"c\"], each = 6),\n                     b1  = rep(traits[,\"b\"], each = 6))\n# Prediction from stepwise model\nLoblolly = transform(Loblolly, pred_height1 = logistic(b1, c1, hm1, age))\n# Plot the data and predictions for each indicidual\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can now look at the (co-)variation of the traits:\nlibrary(GGally)\nggpairs(data = traits[,1:3])\nNotice that b and c are correlated. The standard averages and standard deviations can be used as initial estimates for the mixed model later:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nIn the next sections (and to keep it simple) we will fit a multilevel model where only \\(h_m\\) is allow to vary across trees.\n\n\n3.2.2 Estimating at population level\nTo obtain the mean estimates of \\(a\\), \\(b\\) and \\(h_m\\) we need to construct a marginal likelihood to integrate over the distribution of values across individuals. If we only assume a random effect for \\(h_m\\) we can specify the model as follows:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi}  &\\sim \\text{Normal}(\\mu_{hm}, \\sigma_{hm})\n\\end{align*}\n\\]\nwhere \\(h_{ij}\\) is the height of tree \\(i\\) at age \\(j\\), and \\(mu_hm\\) is the population mean for \\(h_m\\), \\(\\sigma\\) represents the observation error and \\(\\sigma_{hm}\\) represents the variation of \\(h_{mi}\\) across individuals. In some of the literature, the distribution of \\(h_{ij}\\) is referred to as “individual level” and the distribution of \\(h_{mi}\\) is known as “population level” (and this type of models are known as multilevel or hierarchical models).\nWe can decompose the values of \\(h_m\\) for each individual into the average and the deviation with respect to the average (\\(\\epsilon\\)). In some of the literature (where these models are know as mixed effect models) the values of \\(\\epsilon\\) are know as random effects:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi} &= \\mu_{hm} - \\epsilon_{hmi} \\\\\n\\epsilon_{hmi}  &\\sim \\text{Normal}(0, \\sigma_{hm}) \\\\\n\\end{align*}\n\\]\nTo create the marginal likelihood we now need to integrate over the Normal distributions of \\(h_m\\). Let’s first build the function. We do it a bit different from before, because we have multiple observations for one tree (before we only had one). This also means we need to be careful with implementation: (i) the function integrate will pass a vector of eps values to prodfun and we have multiple values of t and h. Therefore, we must use sapply:\nprodfun &lt;- function(eps_hm, b, c, mu_hm, sigma, sigma_hm, t, h) {\n sapply(eps_hm, function(x) exp(dnorm(x, mean = 0, sd =  sigma_hm, log = TRUE) + # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - x, t), sigma, log = TRUE)))) # Individual level\n}\n# Evaluate for first tree using as initial values what we obtain from the stepwise approach\nprodfun(eps_hm = 0, c = 11.8, b = 0.23, mu_hm = 61,  sigma_hm = 2.3, sigma = 2,\n        t = Loblolly$age[1:6], h = Loblolly$height[1:6])\nThe integration for one tree would be:\nintegrate(f = prodfun, lower = -6*2.3, upper = 6*2.3,\n          c = 11.8, b = 0.23, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n          t = Loblolly$age[1:6], h = Loblolly$height[1:6],\n          rel.tol = 1e-12, abs.tol = 1e-12)$value\nWe can now define a function to compute the negative log marginal likelihood by solving the integral for each observation, log transforming and adding them up. Notice that now we are not applying the integration to each observation but to each group of observations that belongs to a tree:\nNLML &lt;- function(b, c, mu_hm, sigma_hm, sigma, t, h) { # data\n  # Every 6 observations is a tree\n  id = seq(1, length(t), by = 6)\n  ML = sapply(id, function(id)\n                   integrate(f = prodfun,lower = -Inf, upper = Inf,\n                        c = c, b = b, mu_hm = mu_hm,\n                        sigma = sigma, sigma_hm = sigma_hm,\n                        t = t[id:(id + 5)], h = h[id:(id + 5)],\n                   rel.tol = 1e-12, abs.tol = 1e-12)$value)\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n     t = Loblolly$age, h = Loblolly$height)\nAnd now we can pass this big boy to bbmle. As usual, I used constrained optimization to avoid negative values or getting too close to zero (and I check later if I hit the boundary):\npar_0 = c(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2)\n\nfit &lt;- mle2(minuslogl = NLML, start = as.list(par_0),\n            data = list(t = Loblolly$age, h = Loblolly$height),\n            method = \"L-BFGS-B\", lower = c(b = 0.01, c = 1, mu_hm = 10,\n                                           # error in lower sigma_hm = 0.01, sigma = 0.01),\n            control = list(parscale = abs(par_0)))\nsummary(fit)\nAs usual we can extract the maximum likelihood estimates and the confidence intervals:\npars = coef(fit)\nci = confint(fit, method = \"quad\")\nprint(pars)\nprint(ci)\nNotice that the estimates are close to what we estimated before with the stepwise approach but not exactly the same\ncbind(means, sds)\n\n\n3.2.3 Estimating at individual level\nThe values of traits for each individual trait can be estimated in the same way that we estimated the true growth rate of trees, by maximizing the conditional likelihood for each tree separately. Let’s build the negative log conditional likelihood of the data of one tree conditional on knowing the population averages and variances:\nNCLL &lt;- function(eps_hm, b, c, mu_hm, sigma_hm, sigma, t, h) {\n   -dnorm(eps_hm, mean = 0, sd =  sigma_hm, log = T) - # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - eps_hm, t), sigma, log = T)) # Individual level\n}\nLet’s the plot NCLL for the first tree\ntree1 = subset(Loblolly, Seed == seeds[1])\nt = tree1$age\nh = tree1$height\neps &lt;- seq(-3,3, by = 0.01)*pars[\"sigma\"]\nNCLL1 &lt;- sapply(eps, function(e)\n                  NCLL(e, c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h))\nplot(eps, NCLL1)\nWe can then optimize this function to obtain the estimated deviation between the maximum height of the first tree and the average of the population and compare with the value estimated from the stepwise procedure:\neps_1 = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"],c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = tree1$age, h = tree1$height)$minimum\nhm1 = pars[\"mu_hm\"] - eps_1\ncat(\"Multivelel: \", hm1, \"Stepwise: \", traits[1,\"h_m\"])\nLet’s do the estimation for all the trees:\neps = numeric(length(seeds))\nfor(i in 1:length(seeds)) {\n  treei = subset(Loblolly, Seed == seeds[i])\n  t = treei$age\n  h = treei$height\n  eps[i] = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"], c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h)$minimum\n}\nhm = pars[\"mu_hm\"] - eps\nplot(traits[,\"h_m\"], hm, ylim = c(52,66), xlim = c(52,66))\nabline(a = 0, b = 1)\nabline(lm(hm~I(traits[,\"h_m\"])), lty = 2)\nWe can see that the individual estimates of hm from the multilevel model and the stepwise approach as correlated (on average they are practically the same) but the multilevel model estimates higher values for the smaller trees and lower values for the higher trees. That is, the estimates for individual trees are pulled towards the population mean. This is a common effect of using multilevel models known as shrinkage. As long as your model for the variation of hm across individuals is reasonable, the estimates with shrinkages are actually better than in the stepwise approach (in the sense that they will be closer to the truth on average).\nLet’t calculate the predictions for each tree\nLoblolly = transform(Loblolly,\n                     hm2 = rep(hm, each = 6))\nLoblolly = transform(Loblolly, pred_height2 = logistic(pars[\"b\"], pars[\"c\"], hm2, age))\n\n# Compare individual predictions for stepwise and multilevel model\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height2))\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can visually see that the multilevel model makes predictions that vary less across trees. Part of it is because of the shrinkage effect, but also because we ignored the variation across trees of a and b. We could write the code to evaluate those two too, but then we have to use for advanced methods of integration and it gets very tedious.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with non-linear mixed model\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd an exercise with non-linear mixed model",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/solution.html#model-with-measurement-error-1",
    "href": "Lab10/solution.html#model-with-measurement-error-1",
    "title": "Lab 10",
    "section": "4.1 Model with measurement error",
    "text": "4.1 Model with measurement error\nWe can estimate this model using Stan as shown in previous tutorials. Remember that the stan model needs to be defined in its own file with the extension .stan or add your code in a Quarto document with the label stan. If you are getting lost in the code, please check the supplement on Stan (also compare with the BUGS code in the book).\ndata {\n  int N; # Number of observations/true values\n  vector[N] x_obs; # The observed growth rate\n  vector[6] hp; # Hyperparameters of the prior distributiobs\n}\n\nparameters {\n  vector&lt;lower = 0&gt;[N] x_true; # Estimated true growth rates\n  real&lt;lower = 0&gt; a; # Shape parameter of Gamma\n  real&lt;lower = 0&gt; s; # Scale parameter of Gamma\n  real&lt;lower = 0&gt; sigma; # Scale parameter of Normal\n}\n\nmodel {\n  # Priors (different from Bolker)\n  a ~ normal(hp[1], hp[2]); # Shape (95% &lt;  26), 10, 10\n  s ~ normal(hp[3], hp[4]); # rate (95% &lt; 40), 15, 15\n  sigma ~ normal(hp[5], hp[6]); # sigma (95% &lt; 40), 10, 10\n  # True growth rate\n  x_true ~ gamma(a, 1/s); # gamma(shape, rate = 1/scale)\n  # Observed growth rate with error\n  x_obs ~ normal(x_true, sigma);\n}\nI made the following changes to the code with respect to Bolker:\n\nAll parameters are given (truncated) Normal distributions as priors because it is easier to reason about prior means and standard deviations.\nI do not hard-code the hyperparameters, so that you can rerun sampling with different priors (for prior sensitivity analysis).\nThe normal distribution is parameterized with the standard deviation rather than precision.\nThere is no need to specify initial values for chains since the priors are not excessively wide (so random samples from them are good starting values).\n\nIn all cases we are using Normal prior distribution that are weakly informative meaning that we only incorporate knowledge of the scale of the parameters. They will be automatically truncated as parameters are all positive. Note that the Bayesian approach will estimate a true value for each observation in addition to the three parameters of the model (so technically we will get 1003 estimates!): The details below are as usual (see Stan supplement). Notice that I increase adapt_delta to 0.95 because this was a harder fit and I want a total of 40 thousand samples:\nlibrary(rstan)\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\nbayesian_fit &lt;- sampling(error_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = length(x_obs), x_obs = x_obs,\n                                     hp = c(10, 10, 15, 15, 10, 10)),\n                         control = list(adapt_delta = 0.95))\nThis ran in about a minute on my computer. Let’s look at the summaries for the population parameters (a, s and sigma):\nprint(bayesian_fit, pars=c(\"a\", \"s\", \"sigma\"), probs=c(.025,.5,.975))\nWe are getting perfect Rhat values (Gelman-Rubin diagnostics) and the effective sample size is in the thousands, which means that our estimations of the posterior distributions from these samples would be very accurate (in fact a bit overkill if you use my originally settings).\nBoth the median and mean are quite similar suggesting a posterior distribution that is fairly symmetric (in fact with this sample size it should be fairly normal and the priors should have a very small effect). We can compare these estimates to the maximum likelihood estimates from before:\npars\nexp(confint(fit, method = \"quad\"))\nWe are getting very similar values to the maximum likelihood approach, as expected from the large sample sizes (so again, the priors did not have much effect). If you check table 10.1 in the book (section 10.5.2) the intervals computed from the posterior distribution are very close to the confidence intervals from the likelihood profile.\nThe Bayesian procedure also gives us the estimates x_true directly. We can convert the samples from the posterior into a matrix:\nposterior &lt;- as.matrix(bayesian_fit);\ndim(posterior)\nposterior[1:4,1:4]\nWe can compare the different estimates of x for the first observation using a kernel density plot:\nplot(density(posterior[,1]), xlab = \"Growth rate\", main = \"\", las = 1)\nabline(v = c(x_obs[1], x_true[1], x_obs[1] - est_eps[1]), col = 1:3)\nabline(v = quantile(posterior[,1], prob = c(0.025,0.975)), lty = 2, col = 4)\nlegend(\"topright\", c(\"Obs\", \"True\", \"Max Lik\", \"CI 95%\"), col = c(1:3,4,4),\n       bty= \"n\", lty = c(1,1,1,2,2))\nWe can see that the Bayesian estimate of the true growth rate for the first observation has a maximum around the same value as the point estimate we obtained using maximum likelihood (and therefore in between observed and true growth rates). However, we actually have quite a bit of uncertainty in this estimate as reflected by the 95% credible interval.\nOf course, given that we know the true values of x in this simulation, we can test how often these 95% intervals include the true values (for a perfect estimation we should cover the true value 95% of the times). Let’s compute the lower and upper bounds of the intervals for each observation:\nlower_ci = quantile(posterior[,1:1000], 2, prob = 0.025)\nupper_ci = quantile(posterior[,1:1000], 2, prob = 0.975)\ninside   = (x_true &gt;= lower_ci) & (x_true &lt;= upper_ci)\ncoverage = sum(inside)/1e3\ncoverage\nThat is pretty much spot on! Getting exactly 95% is very difficult (especially with only 1000 values, we would need more for the coverage estimate to stabilize). The fact that it errors on the side of caution (i.e., simulated coverage is slightly higher than 95%) is also a good thing (we generally prefer to be pessimistic rather than optimistic). So even though our uncertainty about the true growth rate of individual trees remains high given how big the measurement error is, achieving a practically perfect coverage is the best we can do.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd Bayesian version of mixed model exercise",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/solution.html#model-for-nested-data-1",
    "href": "Lab10/solution.html#model-for-nested-data-1",
    "title": "Lab 10",
    "section": "4.2 Model for nested data",
    "text": "4.2 Model for nested data\nLet’s implement the model in Stan. This time we are going to include all traits as varying across replicates but we will ignore correlations as that makes the model much more complex (but it would be possible). The predictions of heights is as before:\n\\[\nh_{ij} \\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b_i\\left(t_j - c_i \\right)}, \\sigma \\right)\n\\] Where the suffix \\(i\\) refers to the tree and \\(j\\) to the time point. The variation in heights across the population is assumed to follow a normal distribution (note that this can technically produce negative values but we will ignore this for simplicity):\n\\[\n\\begin{align*}\nh_{mi} &= \\mu_{hm} + \\sigma_{hm} z_{hmi} \\\\\nz_{hmi}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nNotice that we have further decomposed the Normal distribution based on the fact that \\(\\text{Normal}(\\mu, \\sigma) = \\mu + \\sigma \\text{Normal}(0,1)\\) which tends to make MCMC sampling much faster. We repeat the same decomposition for the rest of the trees:\n\\[\n\\begin{align*}\nb_{i} &= \\mu_{b} + \\sigma_{b} z_{bi} \\\\\nz_{bi}  &\\sim \\text{Normal}(0, 1) \\\\\nc_{i} &= \\mu_{c} + \\sigma_{c} z_{ci} \\\\\nz_{ci}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nThe model in Stan would look at follows\ndata {\n  int N; # Number of observations\n  int Ntree; # Number of trees\n  vector[N] h; # The observed growth rate\n  vector[N] t; # The age associated to each observation\n  int tree[N]; # Id of the tree (not seed, but 1 - 14)\n  vector[14] hp; # Hyperparameters\n}\n\nparameters {\n  # Population-level parameters\n  real&lt;lower = 0&gt; mu_hm; # Mean maximum height\n  real&lt;lower = 0&gt; mu_b;  # Mean parameter b\n  real&lt;lower = 0&gt; mu_c;  # Mean parameter c\n  real&lt;lower = 0&gt; sigma_hm; # Variation in maximum height\n  real&lt;lower = 0&gt; sigma_b;  # Variation in parameter b\n  real&lt;lower = 0&gt; sigma_c;  # Variation in parameter c\n  real&lt;lower = 0&gt; sigma;    # Measurement/observation error\n  vector[Ntree] z_hm; # Standardized deviations of maximum height in population\n  vector[Ntree] z_b;  # Standardized deviations of b in population\n  vector[Ntree] z_c;  # Standardized deviations of c in population\n}\n\n# In this block we can do intermediate calculations (makes it run faster)\ntransformed parameters{\n  # Individual-level parameters\n  vector[Ntree] hm; # Maximum height of each tree\n  vector[Ntree] b; # Trait b of each tree\n  vector[Ntree] c; # Trait c of each tree\n  # Observations\n  vector[N] h_mod;\n  # Compute the trait value of each tree\n  hm = mu_hm + sigma_hm*z_hm;\n  b  = mu_b  + sigma_b*z_b;\n  c  = mu_c  + sigma_c*z_c;\n  # Compute the height of each tree at each timepoint\n  for(i in 1:N) {\n    int id = tree[i]; # Check which tree we are dealing with\n    h_mod[i] = hm[id]/(1 + exp(-b[id]*(t[i] - c[id]))); # Logistic model\n  }\n}\n\n# Here we only list the distributions since we already have done the calculations\n# of growth in the transformed parameters block\nmodel {\n  // Population-level parameters (priors)\n  mu_hm ~ normal(hp[1], hp[2]);\n  mu_b  ~ normal(hp[3], hp[4]);\n  mu_c  ~ normal(hp[5], hp[6]);\n  sigma_hm ~ normal(hp[7], hp[8]);\n  sigma_b  ~ normal(hp[9], hp[10]);\n  sigma_c  ~ normal(hp[11], hp[12]);\n  sigma ~ normal(hp[13], hp[14]);\n  // Standardized deviations from population mean (actual tree-level traits above)\n  z_hm ~ normal(0, 1);\n  z_b ~ normal(0, 1);\n  z_c ~ normal(0, 1);\n  // Observations within tree\n  h ~ normal(h_mod, sigma);\n}\nFor this example, I came up with more informative priors based on general I could find online on the growth of Loblolly pine. I put the whole reasoning to come up with the priors at the end of the lab as an example of how one could come up with reasonable priors and check that they produce sensible predictions. So please go there if you want to check it.\nOnce we have chosen some prior distributions we can sample from the posterior distribution:\nlibrary(rstan)\n\n# Parallelize\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\n\n# See section below on prior elicitation\npriors = c(65, 10,          # mean and sd of mu_hm\n           0.25, 0.25/4,    # mean and sd of mu_b\n           10, 10/4,        # mean and sd of mu_c\n           65/5, 65/10,     # mean and sd of sigma_hm\n           0.25/5, 0.25/10, # mean and sd of sigma_b\n           10/5, 10/10,     # mean and sd of sigma_c\n           1, 1)            # mean and sd of sigma\n\nbayesian_fit &lt;- sampling(logistic_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = nrow(Loblolly),\n                                     Ntree = length(unique(Loblolly$Seed)),\n                                     h = Loblolly$height,\n                                     t = Loblolly$age,\n                                     tree = as.numeric(Loblolly$Seed),\n                                     hp = priors),\n                         control = list(adapt_delta = 0.95))\n\nprint(bayesian_fit, pars=c(\"mu_hm\", \"mu_b\", \"mu_c\",\n                           \"sigma_hm\", \"sigma_b\", \"sigma_c\", \"sigma\"),\n      probs=c(.025,.5,.975))\nCompare this to the estimates we obtained from the stepwise approach:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nThe Bayesian estimate of the standard deviations are a bit smaller than the ones obtained in the stepwise approach, but they are in the right order of magnitude and the 95% credible interval includes the estimates from the stepwise approach. Let’s look at the posterior distributions of this standard deviations:\nposterior = as.matrix(bayesian_fit)\npar(mfrow = c(1,3))\nplot(density(posterior[,\"sigma_hm\"], from = 0), main = \"\", xlab = \"sigma_hm\")\nplot(density(posterior[,\"sigma_b\"], from = 0), main = \"\", xlab = \"sigma_b\")\nplot(density(posterior[,\"sigma_c\"], from = 0), main = \"\", xlab = \"sigma_c\")\nNote how the posterior distributions of standard deviations of b and c are quite small but also highly asymmetric. This is typical of constrained variables that are highly uncertain. Methods based on maximum marginal likelihood will struggle estimating optimal values sigma_b and sigma_c.\nThe samples for hm, b and c of each can be retrieved directly from posterior. For example, for the first tree:\npar(mfrow = c(1,3))\nplot(density(posterior[,\"hm[1]\"]), main = \"\", xlab = \"hm\")\nplot(density(posterior[,\"b[1]\"]), main = \"\",  xlab = \"b\")\nplot(density(posterior[,\"c[1]\"]), main = \"\",  xlab = \"c\")\nWe can also print the summary information directly from othe original fitted object:\nprint(bayesian_fit, pars=c(\"hm\"), probs=c(.025,.5,.975))\nSimilarly, we can also extract the predicted values for each observation of height (I only show it for one observation, otherwise it becomes too long):\nprint(bayesian_fit, pars=c(\"h_mod[1]\"), probs=c(.025,.5,.975))\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd Bayesian version of mixed model exercise",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/solution.html#informative-prior-elicitation",
    "href": "Lab10/solution.html#informative-prior-elicitation",
    "title": "Lab 10",
    "section": "4.3 Informative prior elicitation",
    "text": "4.3 Informative prior elicitation\nLet’s define some reasonable priors. Let’s start with the priors for the means of the populations. The parameter hm is the maximum height of the trees. We are dealing with a pine tree that growth is humid warm areas of North America and typical has heights of 50 to 80 feet with record heights (https://edis.ifas.ufl.edu/publication/ST478). Since it is often between 50 and 80, we can specify our prior to have a mean of 65 feet and a standard deviation of 10:\nlibrary(truncnorm)\nset.seed(1234)\nprior_mu_hm = rtruncnorm(1000, mean = 65, sd = 10, a = 0)\nsum(prior_mu_hm &gt; 50 & prior_mu_hm &lt; 80)/1e3 # 87% of heights within this range\nhist(prior_mu_hm)\nWe know that parameter b is related to growth rates (it is 4 times the maximum growth rate normalized by maximum height, see Chapter 3 of the book). Searching the literature (actually using AI with sources…) tells use that average growth rate in this species is about 2 feet/year. How does b related to this? First let’s compute the derivative of the logistic (which would tell you the growth rate). I will let the computer compute the derivative for me…\nlibrary(Deriv) # Package to compute symbolic derivatives\nderiv_logistic = Deriv(logistic, \"t\")\ncurve(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = x), 0, 35, ylab = \"Growth rate\")\nWe can now calculate the average growth rate by averaging the first 30 years of growth (after that it does not seem to grow much):\navg_growth_rate = mean(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = 1:30))\nOk, that is exactly the value we got from literature (I guess our data is very average, but you will not always be so lucky), meaning that an average growth rate of 2 feet/year corresponds to b = 0.25. We don’t have information on uncertainty, so we can take a conservative estimate of say, quarter of the mean:\nprior_mu_b = rtruncnorm(1000, mean = 0.25, sd = 0.25/4, a = 0)\nhist(prior_mu_b)\nThis would lead to maximum growth rates of 0.5 - 7 feet/year. Finally the parameter c that tells us about the age at which half of maximum height is reached. Again, checking our AI-powered Google and some of the sources within, we see that most of the growth happens in the first 20 years and after that growth rate decrease significantly as the tree matures. Since the logistic curve is symmetric, this would imply a value of c = 10 and we can assume again a standard deviation of a quarter of that:\nprior_mu_c = rtruncnorm(1000, mean = 10, sd = 10/4, a = 0)\nhist(prior_mu_c)\nTo double check that our priors make sense, we can simulate 1000 growth curves based on the parameters we just obtained\n# Simulation prior population means\nt = tree1$age\nprior_mu = sapply(t, function(x)\n                  logistic(b = prior_mu_b, c = prior_mu_c, h_m = prior_mu_hm, x))\nhead(prior_mu) # Each column is a different age\nLet’s summarise all these trajectories:\nmean_prior_mu = colMeans(prior_mu)\nmed_prior_mu = apply(prior_mu, 2, quantile, prob = 0.5)\nlower_prior_mu = apply(prior_mu, 2, quantile, prob = 0.025)\nupper_prior_mu = apply(prior_mu, 2, quantile, prob = 0.975)\nplot(t, mean_prior_mu, ylim = range(prior_mu), t = \"l\")\nlines(t, med_prior_mu, col = 2)\nlines(t, lower_prior_mu, col = 3)\nlines(t, upper_prior_mu, col = 3)\nWe can see that this covers a wide range of trajectories, but it looks very reasonable. Of course later we can test what happens if our priors are wider but if you have information do use it and come up with sensible priors rather than covering an unreasonable range “just to be sure”.\nMore complicated is coming up with reasonable priors for the variances. The reason is that the variation across trees is going to depend on the context. Is this a wild population or a more artificial plantation? If it is a wild population, is it a mixed forest or a very homogeneous system? Unfortunately, most of the published research focuses on averages, so even if you are an expert, it can be hard to justify prior distributions for the variances. Given that we have 14 replicates we can allow to be rather vague and calculate these variances as a function of the better defined means above. For example, we could expect a standard deviation of 20% around the mean for each trait and half the value for our uncertainty of what the exact value should be:\nprior_sigma_hm = rtruncnorm(1000, mean = 65/5, sd = 65/10, a = 0)\nprior_sigma_b = rtruncnorm(1000, mean = 0.25/5, sd = 0.25/10, a = 0)\nprior_sigma_c = rtruncnorm(1000, mean = 10/5, sd = 10/10, a = 0)\nWe can now simulate individual trees by combining the priors for means and standard deviations.\nprior_hm = rtruncnorm(1000, mean = prior_mu_hm, sd = prior_sigma_hm, a = 0)\nprior_b  = rtruncnorm(1000, mean = prior_mu_b, sd = prior_sigma_b, a = 0)\nprior_c  = rtruncnorm(1000, mean = prior_mu_c, sd = prior_sigma_c, a = 0)\nThe resulting population should be more variable than the averages above:\nt = 1:25\nprior_h = sapply(t, function(x)\n                  logistic(b = prior_b, c = prior_c, h_m = prior_hm, x))\nmean_prior_h = colMeans(prior_h)\nmed_prior_h = apply(prior_h, 2, quantile, prob = 0.5)\nlower_prior_h = apply(prior_h, 2, quantile, prob = 0.025)\nupper_prior_h = apply(prior_h, 2, quantile, prob = 0.975)\nplot(t, mean_prior_h, ylim = range(prior_h), t = \"l\", ylab = \"Tree height\")\nlines(t, med_prior_h, col = 2)\nlines(t, lower_prior_h, col = 3)\nlines(t, upper_prior_h, col = 3)\nThe last variable which prior we need to specify is the measurement error. The measurement errors that we should expect depend on how the measurements were taken which of course we do not know. Assuming a visual estimation using trigonometry, the reported errors seem to be 1 - 4% of the tree height. This means a prior we believe an error of 0.2 - 3 m based on the prior predicted heights. We can achieve this as follows:\nprior_sigma = rtruncnorm(1000, mean = 1, sd = 1, a = 0)\nhist(prior_sigma)",
    "crumbs": [
      "Solutions",
      "Lab 10 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/material.html",
    "href": "Lab10/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "In this practical you will learn\n\nThe concept of maximum marginal likelihood\nEstimate models with process and measurement errors\nEstimate multilevel non linear models with Gaussian quadrature\nThe Bayesian approach for the two models above\nSpecify informative priors using general knowledge"
  },
  {
    "objectID": "Lab10/material.html#model-with-measurement-error",
    "href": "Lab10/material.html#model-with-measurement-error",
    "title": "1 Learning goals",
    "section": "3.1 Model with measurement error",
    "text": "3.1 Model with measurement error\nLet’s assume that the growth rates of trees within a forest follow a Gamma distribution and the measurements of this growth rate contain errors that follow a Normal distribution. This is the problem described in section 10.5.2 of the book. This is a model with two levels:\n\\[\n\\begin{align*}\n\\text{Level}~2~:~X_{true} &\\sim Gamma(a, s) \\\\\n\\text{Level}~1~:~X_{obs~~}  &\\sim Normal(X_{true}, \\sigma)\n\\end{align*}\n\\]\nWhere \\(X_{true}\\) are the true growth rates of the trees and \\(X_{obs}\\) are the measured growth rates. Notice that the level 1 uses as input the output of level 2 (\\(X_{true}\\)), which determines the order.\nIn this exercise we will work with synthetic data generated by stochastic simulation (see Chapter 5 of the book):\nset.seed(1001)\nx_true &lt;- rgamma(1000, shape = 3, scale = 10) # True growth rates\nx_obs  &lt;- rnorm(1000, mean = x_true, sd = 10) # Observed growth rates with error\nhist(x_obs, ylim = c(0,300))\nhist(x_true, add = TRUE, col = rgb(1,0,0,0.3))\nAccording to the general procedure described below, we can estimate \\(a\\) and \\(s\\) from level 2 by creating a marginal likelihood (integrating over level 1) and maximizing it (steps 2 - 3). This is shown in the next section. Once that estimation is done, we can estimate each of value of \\(X_{true}\\) by maximizing the conditional likelihood (step 4).\n\n3.1.1 Estimation of parameters\nThe marginal likelihood is defined as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{true,i} | a, s\\right) \\text{Normal}\\left(X_{obs,i} | X_{true,i} , \\sigma \\right) dX_{true,i}\n\\]\nThat is, for each observation we integrate over all the possible true values (that is the latent random variable we do not observe) to obtained a likelihood function that only contains observations and parameters. Because the measurement error is normal, we can reparameterized as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{obs,i} - \\epsilon_i | a, s\\right) \\text{Normal}\\left(\\epsilon_i |0 , \\sigma \\right) d\\epsilon_i}\n\\]\nWhere we replace \\(X_{true} = X_{obs} - \\epsilon\\). This form is how most multilevel models are expressed. In R the product of the two distributions is:\nprodfun &lt;- function(eps, a, s, sigma, x) {\n dgamma(x - eps, shape = a, scale = s)*dnorm(eps, mean = 0, sd =  sigma)\n}\nprodfun(eps = 1, a = 3, s = 10, sigma = 1, x = x_obs[1])\nWe can now integrate prodfun using the function integrate (note: this only works for one latent random variable, more advanced methods are needed when more latent variables are used):\nintegrate(f = prodfun, lower = -Inf, upper = Inf,\n          a = 3, s = 10, sigma = 1, x = x_obs[1],\n          rel.tol = 1e-6, abs.tol = 1e-6)$value\nWe can now build a function to compute the negative log marginal likelihood by applying this integral to each observations:\nNLML &lt;- function(x, a, s, sigma) {\n  # Marginal likelihood of each observation\n  ML &lt;- sapply(x, function(x) integrate(f = prodfun, lower = -Inf, upper = Inf,\n                                        rel.tol = 1e-6, abs.tol = 1e-6,\n               a = a, s = s, sigma = sigma, x = x)$value)\n  # Negative log marginal likelihood\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(x_obs, a = 3, s = 10, sigma = 1)\nNotice that this takes a bit longer than usual because we are doing 1000 numerical integrations. We can minimize the function NLML with mle2:\nlibrary(bbmle)\nfit &lt;- mle2(minuslogl = NLML,\n            start = list(a = 3, s = 10, sigma = 1),\n            lower = c(0,0,0),\n            data = list(x = x_obs), method = \"L-BFGS-B\")\nfit\nWe can now compare the true and estimated models for growth and error to see how well the estimation worked:\npars = coef(fit)\nplot(density(x_obs), ylim = c(0,0.045), xlab = \"Growth rate\",\n     ylab = \"Probability density\", main = \"\", las = 1, col = 3)\ncurve(dgamma(x, shape = 3, scale = 10), add = TRUE)\ncurve(dgamma(x, shape = pars[\"a\"], scale = pars[\"s\"]), add = TRUE, col = 2)\ncurve(dnorm(x, mean = 0, sd = 10), add = TRUE, lty = 2)\ncurve(dnorm(x, mean = 0, sd = pars[\"sigma\"]), add = TRUE, col = 2, lty = 2)\nlegend(\"topright\", c(\"Obs\", \"True growth\", \"Est growth\", \"True error\", \"Est error\"),\n       col = c(3, 1, 2, 1, 2), lty = c(1, 1, 1, 2, 2))\nWe can quickly estimate the confidence intervals using the quadratic approximation:\nconfint(fit, method = \"quad\")\nNotice that the true values (a = 3, s = 10 and sigma = 10) are within the confidence intervals, so the estimation has worked. We could also try building the likelihood profiles but that would take quite a while since this would require many more optimizations so we will skip this part.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with measurement errors"
  },
  {
    "objectID": "Lab10/material.html#model-for-nested-data",
    "href": "Lab10/material.html#model-for-nested-data",
    "title": "1 Learning goals",
    "section": "3.2 Model for nested data",
    "text": "3.2 Model for nested data\nTo keep it within the same theme, in this example we will look at a simple dataset of tree growth, but this time using real data. The data describes the growth in height of several individuals of Loblolly pine (Pinus taeda). We can load the data as follows:\nlibrary(ggplot2)\ndata(Loblolly)\nsummary(Loblolly)\n# eyeballing; hm = 60, b = 0.25, c = 12 (check  Figure 3.9 in the book)\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) + geom_point() +\n  stat_function(fun = function(x) 60/(1 + exp(-0.25*(x - 12))), color = \"black\")\nEach tree is identified by the column Seed and six measurements of height over the age of the tree are reported. The growth curve of each tree will be modeled using a logistic curve:\n\\[\nh(t) =  \\frac{h_m}{1 + e^{-b\\left(t - c \\right)}\n\\]\nwhere \\(h\\) is the height of the tree, \\(h_m\\) is the maximum height, \\(b\\) control the steepness of the curve and \\(c\\) is the age at which the half maximum height is reached (see Chapter 3 of the book, I will refer to these parameters as traits from here on) and \\(t\\) is the age of the tree. We want to know the average values for the three traits as well as how much they vary across individuals (variation in traits across individuals is often very relevant in ecology). In a mixed model, \\(h_m\\), \\(b\\) and \\(c\\) are assumed to vary across individuals assuming particular distributions (we will assume normal distributions which is the standard in mixed models).\nAs a first approach, we can try to estimate the curve for each tree independently. This is a good way to get a first estimate of how much the traits vary across individuals as well as the observation error. As we will see in the examples below, non-linear mixed models as simple as these ones already need quite some constraints in order to work.\n\n3.2.1 Stepwise procedure\nThe stepwise procedure is as follows:\n\nFit the model to each individual separately, using maximum likelihood.\nTreat the maximum likelihood estimates from each individuals as if they were observations and analyze them with a separate model.\n\nLet’s setup a function to fit the logistic growth curve to each tree:\nlogistic &lt;- function(b, c, h_m, t) {\n  h_m/(1 + exp(-b*(t - c)))\n}\nmle_fun &lt;- function(b, c, h_m, sigma, t, h) {\n  hmod = logistic(b, c, h_m, t)\n  -sum(dnorm(h, hmod, sigma, log = TRUE))\n}\nWe can test the function for the first tree:\nseeds = unique(Loblolly$Seed)\ntree1 = subset(Loblolly, Seed == seeds[1])\nmle_fun(c = 12, b = 0.25, h_m = 60, sigma = 1, t = tree1$age, h = tree1$height)\nLet’s estimate the parameters for this first tree:\nlibrary(bbmle)\nfit1 = mle2(mle_fun, start = list(c = 12, b = 0.25, h_m = 60, sigma = 1),\n     data = list(t = tree1$age, h = tree1$height))\nfit1\nLet’s repeat it for all trees:\ntraits = matrix(NA, ncol = 4, nrow = length(seeds))\ncolnames(traits) = c(\"b\", \"c\", \"h_m\", \"sigma\")\nfor(i in 1:length(seeds)) {\n  tree = subset(Loblolly, Seed == seeds[i])\n  fit = mle2(mle_fun, start = list(c= 12, b = 0.25, h_m = 60, sigma = 1),\n             lower = c(c = 0, b = 1e-4, h_m = 10, sigma = 1e-4),\n             upper = c(c = 30, b = 1, h_m = 80, sigma = 20),\n            data = list(t = tree$age, h = tree$height), method = \"L-BFGS-B\")\n  traits[i,] = coef(fit)\n}\ntraits\nLet’s look at the individual fits by adding the predicitons to the data:\nLoblolly = transform(Loblolly,\n                     hm1  = rep(traits[,\"h_m\"], each = 6),\n                     c1  = rep(traits[,\"c\"], each = 6),\n                     b1  = rep(traits[,\"b\"], each = 6))\n# Prediction from stepwise model\nLoblolly = transform(Loblolly, pred_height1 = logistic(b1, c1, hm1, age))\n# Plot the data and predictions for each indicidual\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can now look at the (co-)variation of the traits:\nlibrary(GGally)\nggpairs(data = traits[,1:3])\nNotice that b and c are correlated. The standard averages and standard deviations can be used as initial estimates for the mixed model later:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nIn the next sections (and to keep it simple) we will fit a multilevel model where only \\(h_m\\) is allow to vary across trees.\n\n\n3.2.2 Estimating at population level\nTo obtain the mean estimates of \\(a\\), \\(b\\) and \\(h_m\\) we need to construct a marginal likelihood to integrate over the distribution of values across individuals. If we only assume a random effect for \\(h_m\\) we can specify the model as follows:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi}  &\\sim \\text{Normal}(\\mu_{hm}, \\sigma_{hm})\n\\end{align*}\n\\]\nwhere \\(h_{ij}\\) is the height of tree \\(i\\) at age \\(j\\), and \\(mu_hm\\) is the population mean for \\(h_m\\), \\(\\sigma\\) represents the observation error and \\(\\sigma_{hm}\\) represents the variation of \\(h_{mi}\\) across individuals. In some of the literature, the distribution of \\(h_{ij}\\) is referred to as “individual level” and the distribution of \\(h_{mi}\\) is known as “population level” (and this type of models are known as multilevel or hierarchical models).\nWe can decompose the values of \\(h_m\\) for each individual into the average and the deviation with respect to the average (\\(\\epsilon\\)). In some of the literature (where these models are know as mixed effect models) the values of \\(\\epsilon\\) are know as random effects:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi} &= \\mu_{hm} - \\epsilon_{hmi} \\\\\n\\epsilon_{hmi}  &\\sim \\text{Normal}(0, \\sigma_{hm}) \\\\\n\\end{align*}\n\\]\nTo create the marginal likelihood we now need to integrate over the Normal distributions of \\(h_m\\). Let’s first build the function. We do it a bit different from before, because we have multiple observations for one tree (before we only had one). This also means we need to be careful with implementation: (i) the function integrate will pass a vector of eps values to prodfun and we have multiple values of t and h. Therefore, we must use sapply:\nprodfun &lt;- function(eps_hm, b, c, mu_hm, sigma, sigma_hm, t, h) {\n sapply(eps_hm, function(x) exp(dnorm(x, mean = 0, sd =  sigma_hm, log = TRUE) + # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - x, t), sigma, log = TRUE)))) # Individual level\n}\n# Evaluate for first tree using as initial values what we obtain from the stepwise approach\nprodfun(eps_hm = 0, c = 11.8, b = 0.23, mu_hm = 61,  sigma_hm = 2.3, sigma = 2,\n        t = Loblolly$age[1:6], h = Loblolly$height[1:6])\nThe integration for one tree would be:\nintegrate(f = prodfun, lower = -6*2.3, upper = 6*2.3,\n          c = 11.8, b = 0.23, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n          t = Loblolly$age[1:6], h = Loblolly$height[1:6],\n          rel.tol = 1e-12, abs.tol = 1e-12)$value\nWe can now define a function to compute the negative log marginal likelihood by solving the integral for each observation, log transforming and adding them up. Notice that now we are not applying the integration to each observation but to each group of observations that belongs to a tree:\nNLML &lt;- function(b, c, mu_hm, sigma_hm, sigma, t, h) { # data\n  # Every 6 observations is a tree\n  id = seq(1, length(t), by = 6)\n  ML = sapply(id, function(id)\n                   integrate(f = prodfun,lower = -Inf, upper = Inf,\n                        c = c, b = b, mu_hm = mu_hm,\n                        sigma = sigma, sigma_hm = sigma_hm,\n                        t = t[id:(id + 5)], h = h[id:(id + 5)],\n                   rel.tol = 1e-12, abs.tol = 1e-12)$value)\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n     t = Loblolly$age, h = Loblolly$height)\nAnd now we can pass this big boy to bbmle. As usual, I used constrained optimization to avoid negative values or getting too close to zero (and I check later if I hit the boundary):\npar_0 = c(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2)\n\nfit &lt;- mle2(minuslogl = NLML, start = as.list(par_0),\n            data = list(t = Loblolly$age, h = Loblolly$height),\n            method = \"L-BFGS-B\", lower = c(b = 0.01, c = 1, mu_hm = 10,\n                                           # error in lower sigma_hm = 0.01, sigma = 0.01),\n            control = list(parscale = abs(par_0)))\nsummary(fit)\nAs usual we can extract the maximum likelihood estimates and the confidence intervals:\npars = coef(fit)\nci = confint(fit, method = \"quad\")\nprint(pars)\nprint(ci)\nNotice that the estimates are close to what we estimated before with the stepwise approach but not exactly the same\ncbind(means, sds)\n\n\n3.2.3 Estimating at individual level\nThe values of traits for each individual trait can be estimated in the same way that we estimated the true growth rate of trees, by maximizing the conditional likelihood for each tree separately. Let’s build the negative log conditional likelihood of the data of one tree conditional on knowing the population averages and variances:\nNCLL &lt;- function(eps_hm, b, c, mu_hm, sigma_hm, sigma, t, h) {\n   -dnorm(eps_hm, mean = 0, sd =  sigma_hm, log = T) - # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - eps_hm, t), sigma, log = T)) # Individual level\n}\nLet’s the plot NCLL for the first tree\ntree1 = subset(Loblolly, Seed == seeds[1])\nt = tree1$age\nh = tree1$height\neps &lt;- seq(-3,3, by = 0.01)*pars[\"sigma\"]\nNCLL1 &lt;- sapply(eps, function(e)\n                  NCLL(e, c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h))\nplot(eps, NCLL1)\nWe can then optimize this function to obtain the estimated deviation between the maximum height of the first tree and the average of the population and compare with the value estimated from the stepwise procedure:\neps_1 = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"],c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = tree1$age, h = tree1$height)$minimum\nhm1 = pars[\"mu_hm\"] - eps_1\ncat(\"Multivelel: \", hm1, \"Stepwise: \", traits[1,\"h_m\"])\nLet’s do the estimation for all the trees:\neps = numeric(length(seeds))\nfor(i in 1:length(seeds)) {\n  treei = subset(Loblolly, Seed == seeds[i])\n  t = treei$age\n  h = treei$height\n  eps[i] = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"], c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h)$minimum\n}\nhm = pars[\"mu_hm\"] - eps\nplot(traits[,\"h_m\"], hm, ylim = c(52,66), xlim = c(52,66))\nabline(a = 0, b = 1)\nabline(lm(hm~I(traits[,\"h_m\"])), lty = 2)\nWe can see that the individual estimates of hm from the multilevel model and the stepwise approach as correlated (on average they are practically the same) but the multilevel model estimates higher values for the smaller trees and lower values for the higher trees. That is, the estimates for individual trees are pulled towards the population mean. This is a common effect of using multilevel models known as shrinkage. As long as your model for the variation of hm across individuals is reasonable, the estimates with shrinkages are actually better than in the stepwise approach (in the sense that they will be closer to the truth on average).\nLet’t calculate the predictions for each tree\nLoblolly = transform(Loblolly,\n                     hm2 = rep(hm, each = 6))\nLoblolly = transform(Loblolly, pred_height2 = logistic(pars[\"b\"], pars[\"c\"], hm2, age))\n\n# Compare individual predictions for stepwise and multilevel model\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height2))\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can visually see that the multilevel model makes predictions that vary less across trees. Part of it is because of the shrinkage effect, but also because we ignored the variation across trees of a and b. We could write the code to evaluate those two too, but then we have to use for advanced methods of integration and it gets very tedious.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with non-linear mixed model"
  },
  {
    "objectID": "Lab10/material.html#model-with-measurement-error-1",
    "href": "Lab10/material.html#model-with-measurement-error-1",
    "title": "1 Learning goals",
    "section": "4.1 Model with measurement error",
    "text": "4.1 Model with measurement error\nWe can estimate this model using Stan as shown in previous tutorials. Remember that the stan model needs to be defined in its own file with the extension .stan or add your code in a Quarto document with the label stan. If you are getting lost in the code, please check the supplement on Stan (also compare with the BUGS code in the book).\ndata {\n  int N; # Number of observations/true values\n  vector[N] x_obs; # The observed growth rate\n  vector[6] hp; # Hyperparameters of the prior distributiobs\n}\n\nparameters {\n  vector&lt;lower = 0&gt;[N] x_true; # Estimated true growth rates\n  real&lt;lower = 0&gt; a; # Shape parameter of Gamma\n  real&lt;lower = 0&gt; s; # Scale parameter of Gamma\n  real&lt;lower = 0&gt; sigma; # Scale parameter of Normal\n}\n\nmodel {\n  # Priors (different from Bolker)\n  a ~ normal(hp[1], hp[2]); # Shape (95% &lt;  26), 10, 10\n  s ~ normal(hp[3], hp[4]); # rate (95% &lt; 40), 15, 15\n  sigma ~ normal(hp[5], hp[6]); # sigma (95% &lt; 40), 10, 10\n  # True growth rate\n  x_true ~ gamma(a, 1/s); # gamma(shape, rate = 1/scale)\n  # Observed growth rate with error\n  x_obs ~ normal(x_true, sigma);\n}\nI made the following changes to the code with respect to Bolker:\n\nAll parameters are given (truncated) Normal distributions as priors because it is easier to reason about prior means and standard deviations.\nI do not hard-code the hyperparameters, so that you can rerun sampling with different priors (for prior sensitivity analysis).\nThe normal distribution is parameterized with the standard deviation rather than precision.\nThere is no need to specify initial values for chains since the priors are not excessively wide (so random samples from them are good starting values).\n\nIn all cases we are using Normal prior distribution that are weakly informative meaning that we only incorporate knowledge of the scale of the parameters. They will be automatically truncated as parameters are all positive. Note that the Bayesian approach will estimate a true value for each observation in addition to the three parameters of the model (so technically we will get 1003 estimates!): The details below are as usual (see Stan supplement). Notice that I increase adapt_delta to 0.95 because this was a harder fit and I want a total of 40 thousand samples:\nlibrary(rstan)\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\nbayesian_fit &lt;- sampling(error_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = length(x_obs), x_obs = x_obs,\n                                     hp = c(10, 10, 15, 15, 10, 10)),\n                         control = list(adapt_delta = 0.95))\nThis ran in about a minute on my computer. Let’s look at the summaries for the population parameters (a, s and sigma):\nprint(bayesian_fit, pars=c(\"a\", \"s\", \"sigma\"), probs=c(.025,.5,.975))\nWe are getting perfect Rhat values (Gelman-Rubin diagnostics) and the effective sample size is in the thousands, which means that our estimations of the posterior distributions from these samples would be very accurate (in fact a bit overkill if you use my originally settings).\nBoth the median and mean are quite similar suggesting a posterior distribution that is fairly symmetric (in fact with this sample size it should be fairly normal and the priors should have a very small effect). We can compare these estimates to the maximum likelihood estimates from before:\npars\nexp(confint(fit, method = \"quad\"))\nWe are getting very similar values to the maximum likelihood approach, as expected from the large sample sizes (so again, the priors did not have much effect). If you check table 10.1 in the book (section 10.5.2) the intervals computed from the posterior distribution are very close to the confidence intervals from the likelihood profile.\nThe Bayesian procedure also gives us the estimates x_true directly. We can convert the samples from the posterior into a matrix:\nposterior &lt;- as.matrix(bayesian_fit);\ndim(posterior)\nposterior[1:4,1:4]\nWe can compare the different estimates of x for the first observation using a kernel density plot:\nplot(density(posterior[,1]), xlab = \"Growth rate\", main = \"\", las = 1)\nabline(v = c(x_obs[1], x_true[1], x_obs[1] - est_eps[1]), col = 1:3)\nabline(v = quantile(posterior[,1], prob = c(0.025,0.975)), lty = 2, col = 4)\nlegend(\"topright\", c(\"Obs\", \"True\", \"Max Lik\", \"CI 95%\"), col = c(1:3,4,4),\n       bty= \"n\", lty = c(1,1,1,2,2))\nWe can see that the Bayesian estimate of the true growth rate for the first observation has a maximum around the same value as the point estimate we obtained using maximum likelihood (and therefore in between observed and true growth rates). However, we actually have quite a bit of uncertainty in this estimate as reflected by the 95% credible interval.\nOf course, given that we know the true values of x in this simulation, we can test how often these 95% intervals include the true values (for a perfect estimation we should cover the true value 95% of the times). Let’s compute the lower and upper bounds of the intervals for each observation:\nlower_ci = quantile(posterior[,1:1000], 2, prob = 0.025)\nupper_ci = quantile(posterior[,1:1000], 2, prob = 0.975)\ninside   = (x_true &gt;= lower_ci) & (x_true &lt;= upper_ci)\ncoverage = sum(inside)/1e3\ncoverage\nThat is pretty much spot on! Getting exactly 95% is very difficult (especially with only 1000 values, we would need more for the coverage estimate to stabilize). The fact that it errors on the side of caution (i.e., simulated coverage is slightly higher than 95%) is also a good thing (we generally prefer to be pessimistic rather than optimistic). So even though our uncertainty about the true growth rate of individual trees remains high given how big the measurement error is, achieving a practically perfect coverage is the best we can do.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise"
  },
  {
    "objectID": "Lab10/material.html#model-for-nested-data-1",
    "href": "Lab10/material.html#model-for-nested-data-1",
    "title": "1 Learning goals",
    "section": "4.2 Model for nested data",
    "text": "4.2 Model for nested data\nLet’s implement the model in Stan. This time we are going to include all traits as varying across replicates but we will ignore correlations as that makes the model much more complex (but it would be possible). The predictions of heights is as before:\n\\[\nh_{ij} \\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b_i\\left(t_j - c_i \\right)}, \\sigma \\right)\n\\] Where the suffix \\(i\\) refers to the tree and \\(j\\) to the time point. The variation in heights across the population is assumed to follow a normal distribution (note that this can technically produce negative values but we will ignore this for simplicity):\n\\[\n\\begin{align*}\nh_{mi} &= \\mu_{hm} + \\sigma_{hm} z_{hmi} \\\\\nz_{hmi}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nNotice that we have further decomposed the Normal distribution based on the fact that \\(\\text{Normal}(\\mu, \\sigma) = \\mu + \\sigma \\text{Normal}(0,1)\\) which tends to make MCMC sampling much faster. We repeat the same decomposition for the rest of the trees:\n\\[\n\\begin{align*}\nb_{i} &= \\mu_{b} + \\sigma_{b} z_{bi} \\\\\nz_{bi}  &\\sim \\text{Normal}(0, 1) \\\\\nc_{i} &= \\mu_{c} + \\sigma_{c} z_{ci} \\\\\nz_{ci}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nThe model in Stan would look at follows\ndata {\n  int N; # Number of observations\n  int Ntree; # Number of trees\n  vector[N] h; # The observed growth rate\n  vector[N] t; # The age associated to each observation\n  int tree[N]; # Id of the tree (not seed, but 1 - 14)\n  vector[14] hp; # Hyperparameters\n}\n\nparameters {\n  # Population-level parameters\n  real&lt;lower = 0&gt; mu_hm; # Mean maximum height\n  real&lt;lower = 0&gt; mu_b;  # Mean parameter b\n  real&lt;lower = 0&gt; mu_c;  # Mean parameter c\n  real&lt;lower = 0&gt; sigma_hm; # Variation in maximum height\n  real&lt;lower = 0&gt; sigma_b;  # Variation in parameter b\n  real&lt;lower = 0&gt; sigma_c;  # Variation in parameter c\n  real&lt;lower = 0&gt; sigma;    # Measurement/observation error\n  vector[Ntree] z_hm; # Standardized deviations of maximum height in population\n  vector[Ntree] z_b;  # Standardized deviations of b in population\n  vector[Ntree] z_c;  # Standardized deviations of c in population\n}\n\n# In this block we can do intermediate calculations (makes it run faster)\ntransformed parameters{\n  # Individual-level parameters\n  vector[Ntree] hm; # Maximum height of each tree\n  vector[Ntree] b; # Trait b of each tree\n  vector[Ntree] c; # Trait c of each tree\n  # Observations\n  vector[N] h_mod;\n  # Compute the trait value of each tree\n  hm = mu_hm + sigma_hm*z_hm;\n  b  = mu_b  + sigma_b*z_b;\n  c  = mu_c  + sigma_c*z_c;\n  # Compute the height of each tree at each timepoint\n  for(i in 1:N) {\n    int id = tree[i]; # Check which tree we are dealing with\n    h_mod[i] = hm[id]/(1 + exp(-b[id]*(t[i] - c[id]))); # Logistic model\n  }\n}\n\n# Here we only list the distributions since we already have done the calculations\n# of growth in the transformed parameters block\nmodel {\n  // Population-level parameters (priors)\n  mu_hm ~ normal(hp[1], hp[2]);\n  mu_b  ~ normal(hp[3], hp[4]);\n  mu_c  ~ normal(hp[5], hp[6]);\n  sigma_hm ~ normal(hp[7], hp[8]);\n  sigma_b  ~ normal(hp[9], hp[10]);\n  sigma_c  ~ normal(hp[11], hp[12]);\n  sigma ~ normal(hp[13], hp[14]);\n  // Standardized deviations from population mean (actual tree-level traits above)\n  z_hm ~ normal(0, 1);\n  z_b ~ normal(0, 1);\n  z_c ~ normal(0, 1);\n  // Observations within tree\n  h ~ normal(h_mod, sigma);\n}\nFor this example, I came up with more informative priors based on general I could find online on the growth of Loblolly pine. I put the whole reasoning to come up with the priors at the end of the lab as an example of how one could come up with reasonable priors and check that they produce sensible predictions. So please go there if you want to check it.\nOnce we have chosen some prior distributions we can sample from the posterior distribution:\nlibrary(rstan)\n\n# Parallelize\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\n\n# See section below on prior elicitation\npriors = c(65, 10,          # mean and sd of mu_hm\n           0.25, 0.25/4,    # mean and sd of mu_b\n           10, 10/4,        # mean and sd of mu_c\n           65/5, 65/10,     # mean and sd of sigma_hm\n           0.25/5, 0.25/10, # mean and sd of sigma_b\n           10/5, 10/10,     # mean and sd of sigma_c\n           1, 1)            # mean and sd of sigma\n\nbayesian_fit &lt;- sampling(logistic_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = nrow(Loblolly),\n                                     Ntree = length(unique(Loblolly$Seed)),\n                                     h = Loblolly$height,\n                                     t = Loblolly$age,\n                                     tree = as.numeric(Loblolly$Seed),\n                                     hp = priors),\n                         control = list(adapt_delta = 0.95))\n\nprint(bayesian_fit, pars=c(\"mu_hm\", \"mu_b\", \"mu_c\",\n                           \"sigma_hm\", \"sigma_b\", \"sigma_c\", \"sigma\"),\n      probs=c(.025,.5,.975))\nCompare this to the estimates we obtained from the stepwise approach:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nThe Bayesian estimate of the standard deviations are a bit smaller than the ones obtained in the stepwise approach, but they are in the right order of magnitude and the 95% credible interval includes the estimates from the stepwise approach. Let’s look at the posterior distributions of this standard deviations:\nposterior = as.matrix(bayesian_fit)\npar(mfrow = c(1,3))\nplot(density(posterior[,\"sigma_hm\"], from = 0), main = \"\", xlab = \"sigma_hm\")\nplot(density(posterior[,\"sigma_b\"], from = 0), main = \"\", xlab = \"sigma_b\")\nplot(density(posterior[,\"sigma_c\"], from = 0), main = \"\", xlab = \"sigma_c\")\nNote how the posterior distributions of standard deviations of b and c are quite small but also highly asymmetric. This is typical of constrained variables that are highly uncertain. Methods based on maximum marginal likelihood will struggle estimating optimal values sigma_b and sigma_c.\nThe samples for hm, b and c of each can be retrieved directly from posterior. For example, for the first tree:\npar(mfrow = c(1,3))\nplot(density(posterior[,\"hm[1]\"]), main = \"\", xlab = \"hm\")\nplot(density(posterior[,\"b[1]\"]), main = \"\",  xlab = \"b\")\nplot(density(posterior[,\"c[1]\"]), main = \"\",  xlab = \"c\")\nWe can also print the summary information directly from othe original fitted object:\nprint(bayesian_fit, pars=c(\"hm\"), probs=c(.025,.5,.975))\nSimilarly, we can also extract the predicted values for each observation of height (I only show it for one observation, otherwise it becomes too long):\nprint(bayesian_fit, pars=c(\"h_mod[1]\"), probs=c(.025,.5,.975))\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise"
  },
  {
    "objectID": "Lab10/material.html#informative-prior-elicitation",
    "href": "Lab10/material.html#informative-prior-elicitation",
    "title": "1 Learning goals",
    "section": "4.3 Informative prior elicitation",
    "text": "4.3 Informative prior elicitation\nLet’s define some reasonable priors. Let’s start with the priors for the means of the populations. The parameter hm is the maximum height of the trees. We are dealing with a pine tree that growth is humid warm areas of North America and typical has heights of 50 to 80 feet with record heights (https://edis.ifas.ufl.edu/publication/ST478). Since it is often between 50 and 80, we can specify our prior to have a mean of 65 feet and a standard deviation of 10:\nlibrary(truncnorm)\nset.seed(1234)\nprior_mu_hm = rtruncnorm(1000, mean = 65, sd = 10, a = 0)\nsum(prior_mu_hm &gt; 50 & prior_mu_hm &lt; 80)/1e3 # 87% of heights within this range\nhist(prior_mu_hm)\nWe know that parameter b is related to growth rates (it is 4 times the maximum growth rate normalized by maximum height, see Chapter 3 of the book). Searching the literature (actually using AI with sources…) tells use that average growth rate in this species is about 2 feet/year. How does b related to this? First let’s compute the derivative of the logistic (which would tell you the growth rate). I will let the computer compute the derivative for me…\nlibrary(Deriv) # Package to compute symbolic derivatives\nderiv_logistic = Deriv(logistic, \"t\")\ncurve(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = x), 0, 35, ylab = \"Growth rate\")\nWe can now calculate the average growth rate by averaging the first 30 years of growth (after that it does not seem to grow much):\navg_growth_rate = mean(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = 1:30))\nOk, that is exactly the value we got from literature (I guess our data is very average, but you will not always be so lucky), meaning that an average growth rate of 2 feet/year corresponds to b = 0.25. We don’t have information on uncertainty, so we can take a conservative estimate of say, quarter of the mean:\nprior_mu_b = rtruncnorm(1000, mean = 0.25, sd = 0.25/4, a = 0)\nhist(prior_mu_b)\nThis would lead to maximum growth rates of 0.5 - 7 feet/year. Finally the parameter c that tells us about the age at which half of maximum height is reached. Again, checking our AI-powered Google and some of the sources within, we see that most of the growth happens in the first 20 years and after that growth rate decrease significantly as the tree matures. Since the logistic curve is symmetric, this would imply a value of c = 10 and we can assume again a standard deviation of a quarter of that:\nprior_mu_c = rtruncnorm(1000, mean = 10, sd = 10/4, a = 0)\nhist(prior_mu_c)\nTo double check that our priors make sense, we can simulate 1000 growth curves based on the parameters we just obtained\n# Simulation prior population means\nt = tree1$age\nprior_mu = sapply(t, function(x)\n                  logistic(b = prior_mu_b, c = prior_mu_c, h_m = prior_mu_hm, x))\nhead(prior_mu) # Each column is a different age\nLet’s summarise all these trajectories:\nmean_prior_mu = colMeans(prior_mu)\nmed_prior_mu = apply(prior_mu, 2, quantile, prob = 0.5)\nlower_prior_mu = apply(prior_mu, 2, quantile, prob = 0.025)\nupper_prior_mu = apply(prior_mu, 2, quantile, prob = 0.975)\nplot(t, mean_prior_mu, ylim = range(prior_mu), t = \"l\")\nlines(t, med_prior_mu, col = 2)\nlines(t, lower_prior_mu, col = 3)\nlines(t, upper_prior_mu, col = 3)\nWe can see that this covers a wide range of trajectories, but it looks very reasonable. Of course later we can test what happens if our priors are wider but if you have information do use it and come up with sensible priors rather than covering an unreasonable range “just to be sure”.\nMore complicated is coming up with reasonable priors for the variances. The reason is that the variation across trees is going to depend on the context. Is this a wild population or a more artificial plantation? If it is a wild population, is it a mixed forest or a very homogeneous system? Unfortunately, most of the published research focuses on averages, so even if you are an expert, it can be hard to justify prior distributions for the variances. Given that we have 14 replicates we can allow to be rather vague and calculate these variances as a function of the better defined means above. For example, we could expect a standard deviation of 20% around the mean for each trait and half the value for our uncertainty of what the exact value should be:\nprior_sigma_hm = rtruncnorm(1000, mean = 65/5, sd = 65/10, a = 0)\nprior_sigma_b = rtruncnorm(1000, mean = 0.25/5, sd = 0.25/10, a = 0)\nprior_sigma_c = rtruncnorm(1000, mean = 10/5, sd = 10/10, a = 0)\nWe can now simulate individual trees by combining the priors for means and standard deviations.\nprior_hm = rtruncnorm(1000, mean = prior_mu_hm, sd = prior_sigma_hm, a = 0)\nprior_b  = rtruncnorm(1000, mean = prior_mu_b, sd = prior_sigma_b, a = 0)\nprior_c  = rtruncnorm(1000, mean = prior_mu_c, sd = prior_sigma_c, a = 0)\nThe resulting population should be more variable than the averages above:\nt = 1:25\nprior_h = sapply(t, function(x)\n                  logistic(b = prior_b, c = prior_c, h_m = prior_hm, x))\nmean_prior_h = colMeans(prior_h)\nmed_prior_h = apply(prior_h, 2, quantile, prob = 0.5)\nlower_prior_h = apply(prior_h, 2, quantile, prob = 0.025)\nupper_prior_h = apply(prior_h, 2, quantile, prob = 0.975)\nplot(t, mean_prior_h, ylim = range(prior_h), t = \"l\", ylab = \"Tree height\")\nlines(t, med_prior_h, col = 2)\nlines(t, lower_prior_h, col = 3)\nlines(t, upper_prior_h, col = 3)\nThe last variable which prior we need to specify is the measurement error. The measurement errors that we should expect depend on how the measurements were taken which of course we do not know. Assuming a visual estimation using trigonometry, the reported errors seem to be 1 - 4% of the tree height. This means a prior we believe an error of 0.2 - 3 m based on the prior predicted heights. We can achieve this as follows:\nprior_sigma = rtruncnorm(1000, mean = 1, sd = 1, a = 0)\nhist(prior_sigma)"
  },
  {
    "objectID": "Lab1/no_solution.html",
    "href": "Lab1/no_solution.html",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "",
    "text": "The aim of this tutorial is to learn the basics of R. After completing the tutorial you will be able to:\n\nInstall R and associated packages and Rstudio\nDo interactive calculations in the R console\nConsult the built-in help in R\nCreate, modify and run R scripts\nSetup a typical workflow in R (loading libraries, reading data, doing statistics and saving results)\nWork with the most common data types in R (vectors, matrices, lists and data frames)",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#how-to-use-this-tutorial",
    "href": "Lab1/no_solution.html#how-to-use-this-tutorial",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.1 How to use this tutorial",
    "text": "2.1 How to use this tutorial\n\nThis tutorial contains many sample calculations. It is important to do these yourself—type them in at your keyboard and see what happens on your screen —to get the feel of working in R.\nExercises in the middle of a section should be done immediately, and make sure you have them rightly solved before moving on. Some other, more challenging, exercises appear at the end of some sections, and these can be left until later.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#what-is-r",
    "href": "Lab1/no_solution.html#what-is-r",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.2 What is R?",
    "text": "2.2 What is R?\nR is an object-oriented scripting language that\n\ncan be used for numerical simulation of deterministic and stochastic dynamic models.\nhas an extensive set of functions for classical and modern statistical data analysis and modeling.\nhas graphics functions for visualizing data and model output\n\nR is an open source project, available for free download via the Web. Originally a research project in statistical computing it is now managed by a development team that includes a number of well-regarded statisticians. It is widely used by statistical researchers and a growing number of theoretical ecologists and ecological modelers as a platform for making new methods available to users.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#installing-r-on-your-computer-basics",
    "href": "Lab1/no_solution.html#installing-r-on-your-computer-basics",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.3 Installing R on your computer: basics",
    "text": "2.3 Installing R on your computer: basics\nIf R is already installed on your computer, you can skip this section.\nThe main source for R is the CRAN home page http://cran.r-project.org. You can get the source code, but most users will prefer a precompiled version. To get one of these from CRAN:\n\ngo to http://cran.r-project.org/mirrors.html and find a mirror site that is geographically somewhat near you.\nFind the appropriate page for your operating system — when you get to the download section, go to base rather than contrib. Download the binary file (e.g. base/R-x.y.z-win32.exe for Windows, R-x.y.z.dmg for MacOS, where x.y.z is the version number).\nRead and follow the instructions (which are pretty much “click on the icon”).",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#installing-and-using-rstudio",
    "href": "Lab1/no_solution.html#installing-and-using-rstudio",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.4 Installing and using Rstudio",
    "text": "2.4 Installing and using Rstudio\nNowadays, programs are available that integrate R in a sophisticated way by combining a console, a syntax highlighting editor (giving colors to R commands and allowing you to identify missing parentheses, quotation marks etc.), tools for plotting, debugging, workspace management and connections to versioning systems into one program. Rstudio is currently the most common program around R and is highly recommended if R is the core of your work. Rstudio can be found and downloaded at https://www.rstudio.com.\nIf the wrong R version is launched after you installed Rstudio, you can change the R version that is used in Rstudio by clicking on Tools and Global Options, choose R general and select the location of the R version you want.\nRstudio usually displays four panels. The default setting is a script editor at the top-left of the screen, the console at the bottom-left, the global environment (which shows what is stored in the memory) at the top-right, and a plotting region at bottom-right. As you see some panel have multiple tabs that include other useful features such as the help (bottom-right), the (available and loaded) packages (bottom-right) etc.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#the-r-package-system",
    "href": "Lab1/no_solution.html#the-r-package-system",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.5 The R package system",
    "text": "2.5 The R package system\nThe standard distributions of R include several packages, user-contributed suites of add-on functions. This lab uses some packages that are not part of the standard distribution. In general, you can install additional packages from within R using the Packages menu, or the install.packages command.\nYou may be able to install new packages from a menu within R. Type and it will install the package ggplot2 that you will use in the next tutorial.\ninstall.packages(\"ggplot2\")\n(for example — this installs the ggplot2 package). You can install more than one package at a time:\ninstall.packages(c(\"ggplot2\",\"nlme\"))\n(c stands for “combine”, and is the command for combining multiple things into a single object.)\nSome of the important functions and packages (collections of functions) for statistical modeling and data analysis are summarized in Table 2. Venables and Ripley (2002) give a good practical (although somewhat advanced) overview, and you can find a list of available packages and their contents at CRAN, the main R website (http://www.cran.r-project.org — select a mirror site near you and click on Package sources). For the most part, we will not be concerned here with this side of R.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#interactive-calculations-in-the-console",
    "href": "Lab1/no_solution.html#interactive-calculations-in-the-console",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.6 Interactive calculations in the console",
    "text": "2.6 Interactive calculations in the console\nThe console is where you enter commands for R to execute interactively, meaning that the command is executed and the result is displayed as soon as you hit the Enter key (bottom-left panel in Rstudio). For example, at the command prompt &gt;, type in 2+2 and hit Enter\n2 + 2\nTo do anything complicated, you have to store the results from calculations by assigning them to variables, using = or &lt;-. For example:\na = 2+2\nR automatically creates the variable a and stores the result (4) in it, but it doesn’t print anything. This may seem strange, but you’ll often be creating and manipulating huge sets of data that would fill many screens, so the default is to skip printing the results. To ask R to print the value, just type the variable name by itself at the command prompt:\na\n(the [1] at the beginning of the line is just R printing an index of element numbers; if you print a result that displays on multiple lines, R will put an index at the beginning of each line. print(a) also works to print the value of a variable.) By default, a variable created this way is a vector, and it is numeric because we gave R a number rather than some other type of data (e.g.  a character string like \"pxqr\"). In this case a is a numeric vector of length 1, which acts just like a number.\nYou could also type a=2+2; a, using a semicolon to put two or more commands on a single line. Conversely, you can break lines anywhere that R can tell you haven’t finished your command and R will give you a “continuation” prompt (+) to let you know that it doesn’t think you’re finished yet: try typing\na = 3*(4 + [Enter]\n5)\nto see what happens. You will sometimes see the continuation prompt when you don’t expect it, e.g. if you forget to close parentheses.If you get stuck continuing a command you don’t want—e.g. you opened the wrong parentheses—just hit the Escape key or the stop icon in the menu bar to get out.\nVariable names in R must begin with a letter, followed by letters or digits You can break up long names with a period, as in very.long.variable.number.3, or an underscore (_), but you can’t use blank spaces in variable names (or at least it’s not worth the trouble). Variable names in R are case sensitive, so Abc and abc are different variables. Make variable names long enough to remember, short enough to type.N.per.ha or pop.density are better than x and y (too short) or available.nitrogen.per.hectare (too long). Avoid c, l, q, t, C, D, F, I, and T, which are either built-in R functions or hard to tell apart.\nR does calculations with variables as if they were numbers. It uses +, -, *, /, and ^ for addition, subtraction, multiplication, division and exponentiation, respectively. For example:\nx = 5\ny = 2\nz1 = x*y ## no output\nz2 = x/y ## no output\nz3 = x^y ## no output\nz2\nz3\nEven though R did not display the values of x and y, it “remembers” that it assigned values to them. Type x; y to display the values.\nYou can retrieve and edit previous commands. The up-arrow (\\(\\uparrow\\)) in the console recalls previous commands to the prompt. They also can be found in the top-right tab History. For example, you can bring back the second-to-last command and edit it into\nz3 = 2*x^y\nYou can combine several operations in one calculation:\nA = 3\nC = (A+2*sqrt(A))/(A+5*sqrt(A))\nC\nParentheses specify the order of operations. The command\nC = A + 2*sqrt(A)/A + 5*sqrt(A)\nis not the same as the one above; rather, it is equivalent to C=A + 2*(sqrt(A)/A) + 5*sqrt(A).\nThe default order of operations is: (1) parentheses; (2) exponentiation, or powers, (3) multiplication and division, (4) addition and subtraction.\nb = 12-4/2^3 gives 12 - 4/8 = 12 - 0.5 = 11.5\nb = (12-4)/2^3 gives 8/8 = 1\nb = -1^2 gives -(1^2) = -1\nb = (-1)^2 gives 1\nIn complicated expressions you might start off by using parentheses to specify explicitly what you want, such as b = 12 - (4/(2^3)) or at least b = 12 - 4/(2^3); a few extra sets of parentheses never hurt, although when you get confused it’s better to think through the order of operations rather than flailing around adding parentheses at random. R also has many built-in mathematical functions that operate on variables such as:\n\nabs: Absolute values.\n\ncos, sin and tan: For trigonometry (arguments always in radians).\n\nexp: Exponential function.\n\nlog, log10: Natural logarithm and logarithm base 10.\n\nsqrt: Square root.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUsing editing shortcuts wherever you can, have R compute the values of:\n\n\\(\\frac{2^7}{2^7 - 1}\\) and compare it with \\(\\left({1 - \\frac{1}{2^7}}\\right)^{-1}\\)\n\n\\(1 + 0.2\\)\n\\(1 + 0.2 + 0.2^2/2 + \\cos(2.3)\\)\n\\(\\log(1)\\)\n\\(\\exp(0.2)\\)\n\nThe standard normal probability density, \\(\\frac{1}{\\sqrt{2\\pi}}\\exp^{-x^2/2}\\), for values of \\(x=1\\) and \\(x=2\\) (R knows \\(\\pi\\) as pi). You can check your answers against the built-in function for the normal distribution; dnorm(1) and dnorm(2) should give you the values for the standard normal for \\(x=1\\) and \\(x=2\\).",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#the-help-system",
    "href": "Lab1/no_solution.html#the-help-system",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "2.7 The help system",
    "text": "2.7 The help system\nR has a help system, although it is generally better for providing detail or reminding you how to do things than for basic ``how do I …?’’ questions.\n\nYou can get help on any R function by entering\n\n?foo\nWhere foo is the name of the function you are interested in (e.g., try ?sin).\n\n??topic or help.search(\"topic\") (with quotes) will list information related to topic available in the base system or in any extra installed packages: then use ?topic to see the information, perhaps using library(pkg) to load the appropriate package first. help.search uses “fuzzy matching” — for example, help.search(\"log\") finds 528 entries (on my particular system) including lots of functions with “plot”, which includes the letters “lot”, which are almost like “log”. If you can’t stand it, you can turn this behavior off by specifying the incantation help.search(\"log\",agrep=FALSE) (81 results which still include matches for “logistic”, “myelogenous”, and “phylogeny” …)\nOn-line help resources - just google it. In our experience, the help provided by R requires some experience in the R language to be able to understand it. Therefore, a general, but practical advice is to Google your problem. In 99.9% of the cases someone else had a similar question in the past which were solved by the R community, often in https://stackoverflow.com.\nLarge language models can often help with basic R operations and recent models may handle more advanced modeling functions. However, you should always be aware that (i) the quality of the responses you get depend on how often a particular error or function has been discussed online and (ii) these generative AI can “say” things are factually incorrect because they do not “understand” what they are saying and it can be very hard to distinguish proper answers from these “hallucinations”. Best case scenario the answer is so wrong that when you try it, it just doesn’t run in R. Worst case, the answer “runs” but it does not do what you actually wanted to do. For that reason (and many others) it is better to use these AI tools to help you learn rather than as a virtual slave coder.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#using-scripts-and-data-files",
    "href": "Lab1/no_solution.html#using-scripts-and-data-files",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "3.1 Using scripts and data files",
    "text": "3.1 Using scripts and data files\nModeling and complicated data analysis are often much easier if you use scripts, which are a series of commands stored in a text file. Scripting has a number of advantages and should be standard practice when doing statistics for reasons of transparency (you can see what you have done), repeatability (tomorrow you will get the same result as today) and transferability (a colleague can easily check what you have done and redo your analysis). Even for relatively simple tasks, script files are useful for building up a calculation step-by-step, making sure that each part works before adding on to it. We recommend you making a habit typing all commands in a script editor before sending it the console, otherwise important parts of your analysis may get lost because you did not store them.\nRstudio has an advanced script editor that recognizes R syntax by giving different colors to different R commands and by automatic completion of parentheses. You can also use Windows Notepad or Wordpad but you should not use MS Word.\nMost programs for working with models or analyzing data follow a simple pattern of program parts:\n\n“Setup” statements. For example, load some packages, or run another script file that creates some functions (more on functions later).\nInput some data from a file or the keyboard. For example, read in data from a text file.\nCarry out the calculations that you want. For example, fit several statistical models to the data and compare them.\nPrint the results, graph them, or save them to a file. For example, graph the results, and save the graph to disk for including in your term project.\n\nTo tell R where data and script files are located, you can do any one of the following:\n\nSpell out the path, or file location, explicitly. (Use a single forward slash to separate folders (e.g. \"c:/My Documents/R/script.R\"): this works on all platforms.)\nChange your working directory to wherever the file(s) are located using the setwd (set working directory) function, e.g. setwd(\"c:/temp\") or through clicking on ‘Session’ and ‘set working directory’. Changing your working directory is more efficient in the long run, if you save all the script and data files for a particular project in the same directory and switch to that directory when you start work.\nAssociate an RStudio project to your folder. This means that every time you open this project in RStudio, the working directory will be set to that folder, you can easily refer to all the data and scripts within the folder without having to worry about the absolute location in your computer. It also means that you can share this project with a colleague and they can run your code without having to change",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#typical-workflow-in-r-an-example-using-linear-regression",
    "href": "Lab1/no_solution.html#typical-workflow-in-r-an-example-using-linear-regression",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "3.2 Typical workflow in R: an example using linear regression",
    "text": "3.2 Typical workflow in R: an example using linear regression\nTo get a feel for a typical workflow in R we’ll fit a straight-line model (linear regression) to data.\nStart a blank R script (File -&gt; New File -&gt; R script) and save it on a convenient location.\nBelow are some data on the maximum growth rate \\(r_{max}\\) of laboratory populations of the green alga Chlorella vulgaris as a function of light intensity (\\(\\mu\\)E per m\\(^2\\) per second). These experiments were run during the system-design phase of the study reported by Fussman et al. (2000).\nLight: 20, 20, 20, 20, 21, 24, 44, 60, 90, 94, 101\n\\(r_{max}\\): 1.73, 1.65, 2.02, 1.89, 2.61, 1.36, 2.37, 2.08, 2.69, 2.32, 3.67\nTo analyze these data in R, first enter them as numerical vectors in your script and send them to the console:\nLight = c(20,20,20,20,21,24,44,60,90,94,101)\nrmax = c(1.73,1.65,2.02,1.89,2.61,1.36,2.37,2.08,2.69,2.32,3.67)\nThe function c combines the individual numbers into a vector. Try recalling (with \\(\\uparrow\\)) and modifying the above command to\nLight=20,20,20,20,21,24,44,60,90,94,101\nand see the error message you get: in order to create a vector of specified numbers, you must use the c function. Don’t be afraid of error messages: the answer to “what would happen if I …?” is usually “try it and see!”\nTo see a histogram of the growth rates enter hist(rmax), which opens a graphics window and displays the histogram. There are many other built-in statistics functions: for example mean(rmax) computes you the mean, and sd(rmax) and var(rmax) compute the standard deviation and variance, respectively. Play around with these functions, and any others you can think of.\nTo see how light intensity affects algal rate of increase, type\nplot(rmax ~ Light)\nin the script (and send it the console) to plot rmax (\\(y\\)) against Light (\\(x\\)). The ~ sign implies “as a function of”. Alternatively, type plot(Light,rmax). A linear regression would seem like a reasonable model for these data. We’ll soon be adding to it. The figure below shows several more ways to adjust the appearance of lines and points in R.\n\n\n\n\n\n\n\n\n\nR’s default plotting character is an open circle. Open symbols are generally better than closed symbols for plotting because it is easier to see where they overlap, but you could include pch=16 in the plot command if you wanted closed circles instead.\nTo perform linear regression we create a linear model using the lm (linear model) function:\nfit = lm(rmax~Light)\n(Note that linear model is read as “model \\(r_{max}\\) as a function of light”.)\nThe lm command produces no output at all, but it creates fit as an object, i.e. a data structure consisting of multiple parts, holding the results of a regression analysis with rmax being modeled as a function of Light. Unlike most statistics packages, R rarely produces automatic summary output from an analysis. Statistical analyses in R are done by creating a model, and then giving additional commands to extract desired information about the model or display results graphically.\nTo get a summary of the results, enter the command summary(fit). R sets up model objects (more on this later) so that the function summary “knows” that fit was created by lm, and produces an appropriate summary of results for an lm object:\nsummary(fit)\n[If you’ve had (and remember) a statistics course the output will make sense to you. The table of coefficients gives the estimated regression line as \\(r_{max} = 1.581 + 0.014 \\times Light\\), and associated with each coefficient is the standard error of the estimate, the \\(t\\)-statistic value for testing whether the coefficient is nonzero, and the \\(p\\)-value corresponding to the \\(t\\)-statistic. Below the table, the adjusted R-squared gives the estimated fraction of the variance explained by the regression line, and the \\(p\\)-value in the last line is an overall test for significance of the model against the null hypothesis that the response variable is independent of the predictors.]\nYou can add the regression line to the plot of the data with a function taking fit as its input (if you closed the plot of the data, you will need to create it again in order to add the regression line):\nabline(fit)\n(abline, pronounced “a b line”, is a general-purpose function for adding lines to a plot: you can specify horizontal or vertical lines, a slope and an intercept, or a regression model: ?abline).\nYou can get the coefficients by using the coef function:\ncoef(fit)\nYou can also “interrogate” fit directly. Type names(fit) to get a list of the components of fit, and then use the $ symbol to extract components according to their names.\nnames(fit)\nFor more information (perhaps more than you want) about fit, use str(fit) (for structure). You can get the regression coefficients this way:\nfit$coefficients\nIt’s good to be able to look inside R objects when necessary, but all other things being equal you should prefer (e.g.) coef(x) to x$coefficients.\nUsually data is loaded from a file. To illustrate this, the file ChlorellaGrowth.txt from the course files (Brightspace or Teams). In ChlorellaGrowth.txt the two variables are entered as columns of a data matrix. Then instead of typing these in by hand, the command\nX = read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nreads the file (from the current directory) and puts the data values into the variable X; header=TRUE specifies that the file includes column names. Note that as specified above you need to make sure that R is looking for the data file in the right place … either move the data file to your current working directory, or change the line so that it points to the actual location of the data file.\nExtract the variables from X with the commands\nLight = X[,1]\nrmax = X[,2]\nThink of these as shorthand for “Light = everything in column 1 of X”, and “rmax = everything in column 2 of X” (we’ll learn about working with data matrices later). From there on out it’s the same as before, with some additions that set the axis labels and add a title.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGrab the file Intro2.R from the course files. Make a copy with a different name, and modify the copy so that it does linear regression of algal growth rate on the natural log of light intensity, LogLight=log(Light), and plots the data appropriately.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun Intro2.R, then enter the command plot(fit) in the console and follow the directions in the console. Figure out what just happened.\n\n\n\nR produced a series of diagnostic plots exploring whether or not the fitted linear model is a suitable fit to the data. In each of the plots, the 3 most extreme points (the most likely candidates for “outliers”) have been identified according to their sequence in the data set.\nThe axes in plots are scaled automatically, but the outcome is not always ideal (e.g. if you want several graphs with exactly the same axis limits). You can use the xlim and ylim arguments in plot to control the limits: plot(x,y,xlim=c(x1,x2), [other stuff]) will draw the graph with the \\(x\\)-axis running from x1 to x2, and using ylim=c(y1,y2) within the plot command will do the same for the \\(y\\)-axis.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCreate a plot of growth rate versus light intensity with the \\(x\\)-axis running from 0 to 120 and the \\(y\\)-axis running from 1 to 4.\n\n\n\nYou can place several graphs within a single figure by using the par function (short for “parameter”) to adjust the layout of the plot. For example, the command\npar(mfrow=c(2,3))\ndivides the plotting area into 2 rows and 3 columns. As R draws a series of graphs, it places them along the top row from left to right, then along the next row, and so on. mfcol=c(2,3) has the same effect except that R draws successive graphs down the first column, then down the second column, and so on.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify the script as follows. Use mfcol=c(2,1) to create graphs of growth rate as a function of Light, and of log(growth rate) as a function of log(Light) in the same figure. Do the same again, using mfcol=c(1,2).\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse ?plot and ?par to read about other plot control parameters. The function plot takes a number of arguments such as type that allows you to draw a line through a series of points instead of plotting separate points. With par you can change anything you want to change. For example, you can choose the color of the points, or the shape of the points. You should definitely skim read this help as this is one of the longest help files in the whole R system!).\nThen draw a \\(2 \\times 2\\) set of plots, each showing the line \\(y=5x + 3\\) with \\(x\\) running from 3 to 8, but with 4 different line styles and 4 different line colors.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify one of your scripts so that at the very end it saves the plot to disk. In Windows you can do this with specific functions like jpeg or png. Use ?jpeg or ?png to read about these functions. Note that the argument filename can include the path to a folder; for example, in Windows you can use filename=\"c:/temp/Intro2Figure.png\".\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nDo some online research to figure out:\n\nHow to rotate the y-axis labels\nHow to change the background of the plot to grey",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#vectors",
    "href": "Lab1/no_solution.html#vectors",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "4.1 Vectors",
    "text": "4.1 Vectors\nAn important class of data types are vectors and matrices (1- and 2-dimensional rectangular arrays of numbers). Operations with vectors and matrices may seem a bit abstract now, but we need them to do useful things later. The only properties of vectors are their type (or class) and length, although they can also have an associated list of names.\nWe’ve already seen two ways to create vectors in R:\n\nA command in the console window or a script file listing the values, such as\n\ninitialsize=c(1,3,5,7,9,11)\n\nUsing read.table:\n\ninitialsize=read.table(\"c:/temp/initialdata.txt\")\n(assuming of course that the file exists in the right place).\nYou can then use a vector in calculations as if it were a number (more or less)\nfinalsize=initialsize+1\nfinalsize\nnewsize=sqrt(initialsize)\nnewsize\nNotice that R applied each operation to every element in the vector. Similarly, commands like initialsize-5, 2*initialsize, initialsize/10 apply subtraction, multiplication, and division to each element of the vector. The same is true for\ninitialsize^2\nIn R the default is to apply functions and operations to vectors in an element by element (or “vectorized”) manner. This is an extremely useful property in R.\n\n4.1.1 Functions for creating vectors\nYou can use the seq function to create a set of regularly spaced values. seq’s syntax is x=seq(from,to,by) or x=seq(from,to) or x=seq(from,to,length.out). The first form generates a vector starting with from with the last entry not extending further than than to in steps of by. In the second form the value of by is assumed to be 1 or -1, depending on whether from or to is larger. The third form creates a vector with the desired endpoints and length. The syntax from:to is a shortcut for seq(from,to):\n1:8\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse seq to create the vector v=(1 5 9 13), and to create a vector going from 1 to 5 in increments of 0.2.\n\n\n\nYou can use rep to create a constant vector such as (1 1 1 1); the basic syntax is rep(values,lengths). For example,\nrep(3,5)\ncreates a vector in which the value 3 is repeated 5 times. rep will repeat a whole vector multiple times\nrep(1:3,3)\nor will repeat each of the elements in a vector a given number of times:\nrep(1:3,each=3)\nEven more flexibly, you can repeat each element in the vector a different number of times:\n\nrep( c(3,4),c(2,5) )\n\n[1] 3 3 4 4 4 4 4\n\n\nThe value 3 was repeated 2 times, followed by the value 4 repeated 5 times. rep can be a little bit mind-blowing as you get started, but it will turn out to be useful.\n\n\n4.1.2 Vector indexing\nYou will often want to extract a specific entry or other part of a vector. This procedure is called vector indexing, and uses square brackets ([]):\nz = c(1,3,5,7,9,11)\nz[3]\nz[3] extracts the third item, or element, in the vector z. You can also access a block of elements using the functions for vector construction, e.g.\nz[2:5]\nextracts the second through fifth elements.\nWhat happens if you enter v=z[seq(1,5,2)] ? Try it and see, and make sure you understand what happened.\nYou can extracted irregularly spaced elements of a vector. For example\nz[c(1,2,5)]\nYou can also use indexing to set specific values within a vector. For example,\nz[1]=12\nchanges the value of the first entry in z while leaving all the rest alone, and\nz[c(1,3,5)]=c(22,33,44)\nchanges the first, third, and fifth values (note that we had to use c to create the vector — can you interpret the error message you get if you try z[1,3,5] ?)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a one-line command to extract a vector consisting of the second, first, and third elements of z in that order.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a script file that computes values of \\(y=\\frac{(x-1)}{(x+1)}\\) for \\(x=1,2,\\cdots,10\\), and plots \\(y\\) versus \\(x\\) with the points plotted and connected by a line hint: in ?plot, search for type.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThe sum of the geometric series \\(1 + r + r^2 + r^3 + ... + r^n\\) approaches the limit \\(1/(1-r)\\) for \\(r &lt; 1\\) as \\(n \\rightarrow \\infty\\).\nSet the values \\(r=0.5\\) and \\(n=10\\), and then write a one-line command that creates the vector \\(G = (r^0,r^1,r^2,...,r^n)\\). Compare the sum (using sum) of this vector to the limiting value \\(1/(1-r)\\).\nRepeat for \\(n=50\\). (Note that comparing very similar numeric values can be tricky: rounding can happen, and some numbers cannot be represented exactly in binary (computer) notation. By default R displays 7~significant digits (options(\"digits\")).\nFor example:\nx = 1.999999\nx\nx-2\nx=1.9999999999999\nx\nx-2\nAll the digits are still there, in the second case, but they are not shown. Also note that x-2 is not exactly \\(-1 \\times 10^{-13}\\); this is unavoidable.)\n\n\n\n\n\n4.1.3 Logical operators\nLogical operators return a value of TRUE or FALSE. For example, try:\na=1\nb=3\nd=a&lt;b\ne=(a&gt;b)\nd\ne\nThe parentheses around (a&gt;b) are optional but make the code easier to read. One special case where you do need parentheses (or spaces) is when you make comparisons with negative values; a&lt;-1 will surprise you by setting a=1, because &lt;- (representing a left-pointing arrow) is equivalent to = in R. Use a&lt; -1, or more safely a&lt;(-1), to make this comparison.\nSome comparison operators in R:\n\nx &lt; y: x is smaller than y.\n\nx &lt; y: x is greater than y.\n\nx &lt;= y: x is smaller or equal than y.\n\nx &gt;= y: x is greater or equal than y.\n\nx == y: x is equal to y.\n\nx != y: x is not equal to y.\n\nWhen we compare two vectors or matrices of the same size, or compare a number with a vector or matrix, comparisons are done element-by-element. For example,\nx = 1:5\nb = (x &lt;= 3)\nSo if x and y are vectors, then (x == y) will return a vector of values giving the element-by-element comparisons. If you want to know whether x and y are identical vectors, use identical(x,y) which returns a single value: TRUE if each entry in x equals the corresponding entry in y, otherwise FALSE. You can use ?Logical to read more about logical operators. Note the difference between = and ==\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun the code block below. Can you figure out why a==b gives different results the first and second time you execute them?\na =  1:3\nb =  2:4\na == b\na =  b\na == b\n\n\n\nExclamation marks ! are used in R to mean “not”; != (not ==) means “not equal to”.\nR also does arithmetic on logical values, treating TRUE as 1 and FALSE as 0. So sum(b) returns the value 3, telling us that three entries of x satisfied the condition (x&lt;=3). This is useful for (e.g.) seeing how many of the elements of a vector are larger than a cutoff value. Build more complicated conditions by using logical operators to combine comparisons:\n\n!: Negation\n&, &&: AND\n|, ||: OR\n\nOR is non-exclusive, meaning that x|y is true if either x or y or both are true (a ham-and-cheese sandwich would satisfy the condition “ham OR cheese”). For example, try\na = c(1,2,3,4)\nb = c(1,1,5,5)\n(a &lt; b) & (a &gt; 3)\n(a &lt; b) | (a &gt; 3)\nand make sure you understand what happened. If it’s confusing, try breaking up the expression and looking at the results of a&lt;b and a&gt;3 separately first. The two forms of AND and OR differ in how they handle vectors. The shorter one does element-by-element comparisons; the longer one only looks at the first element in each vector.\nWe can also use logical vectors (lists of TRUE and FALSE values) to pick elements out of vectors. This is important, e.g., for subsetting data (getting rid of those pesky outliers!)\nAs a simple example, we might want to focus on just the low-light values of \\(r_{max}\\) in the Chlorella example:\nX=read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight=X[,1]\nrmax=X[,2]\nlowLight = Light[Light&lt;50]\nlowLightrmax = rmax[Light&lt;50]\nlowLight\nlowLightrmax\nWhat is really happening here (think about it for a minute) is that Light&lt;50 generates a logical vector the same length as Light (TRUE TRUE TRUE ...) which is then used to select the appropriate values.\nIf you want the positions at which Light is lower than 50, you could say (1:length(Light))[Light&lt;50], but you can also use a built-in function: which(Light&lt;50). If you wanted the position at which the maximum value of Light occurs, you could say which(Light==max(Light)). (This normally results in a vector of length 1; when could it give a longer vector?) There is even a built-in command for this specific function, which.max (although which.max always returns just the first position at which the maximum occurs).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWhat would happen if instead of setting lowLight you replaced Light by saying Light=Light[Light&lt;50], and then rmax=rmax[Light&lt;50]?\nWhy would that be wrong?\nTry it with some temporary variables — set Light2=Light and rmax2=rmax and then play with Light2 and rmax2 so you dont mess up your working variables — and work out what happened…\nWe can also combine logical operators (making sure to use the element-by-element & and | versions of AND and OR):\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nrunif(n) is a function (more on it soon) that generates a vector of n random, uniformly distributed numbers between 0 and 1. Create a vector of 20 numbers, then select the subset of those numbers that is less than the mean. (If you want your answers to match mine exactly, use set.seed(273) to set the random-number generator to a particular starting point before you use runif. [273 is an arbitrary number I chose].)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind the positions of the elements that are less than the mean of the vector you just created (e.g. if your vector were (0.1 0.9. 0.7 0.3) the answer would be (1 4)).\n\n\n\nAs I mentioned in passing above, vectors can have names associated with their elements: if they do, you can also extract elements by name (use names to find out the names).\nx = c(first=7,second=5,third=2)\nnames(x)\nx[\"first\"]\nx[c(\"third\",\"first\")]\nFinally, it is sometimes handy to be able to drop a particular set of elements, rather than taking a particular set: you can do this with negative indices. For example, x[-1] extracts all but the first element of a vector.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nSpecify two ways to take only the elements in the odd positions (first, third, …) of a vector of arbitrary length.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#matrices",
    "href": "Lab1/no_solution.html#matrices",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\n\n4.2.1 Creating matrices\nA matrix is a two-dimensional array, and has the same kind of variables in every column. You can create matrices of numbers by creating a vector of the matrix entries, and then reshaping them to the desired number of rows and columns using the function matrix. For example\n(X = matrix(1:6,nrow=2,ncol=3))\ntakes the values 1 to 6 and reshapes them into a 2 by 3 matrix.\nBy default R loads the values into the matrix column-wise (this is probably counter-intuitive since we’re used to reading tables row-wise). Use the optional parameter byrow to change this behavior. For example:\n(A = matrix(1:9,nrow=3,ncol=3,byrow=TRUE))\nR will re-cycle through entries in the data vector, if necessary to fill a matrix of the specified size. So for example\nmatrix(1,nrow=10,ncol=10)\ncreates a \\(10 \\times 10\\) matrix of ones.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse a command of the form X = matrix(v,nrow=2,ncol=4) where v is a data vector, to create the following matrix X:\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n\n\nIf you can, try to use R commands to construct the vector rather than typing out all of the individual values.\nR will also collapse a matrix to behave like a vector whenever it makes sense: for example sum(X) above is 12.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse rnorm (which is like runif, but generates Gaussian (normally distributed) numbers with a specified mean and standard deviation instead) and matrix to create a \\(5 \\times 7\\) matrix of Gaussian random numbers with mean 1 and standard deviation 2. (Use set.seed(273) again for consistency).\n\n\n\nAnother useful function for creating matrices is diag. diag(v,n) creates an \\(n \\times n\\) matrix with data vector \\(v\\) on its diagonal. So for example diag(1,5) creates the \\(5 \\times 5\\) identity matrix, which has 1’s on the diagonal and 0 everywhere else. Try diag(1:5,5) and diag(1:2,5); Observe what is happening. Is this desired behaviour?\nFinally, you can use the data.entry function. This function can only edit existing matrices, but for example\nA=matrix(0,nrow=3,ncol=4)\ndata.entry(A)\nwill create A as a \\(3 \\times 4\\) matrix, and then call up a spreadsheet-like interface in which you can change the values to whatever you need.\n\n\n4.2.2 cbind and rbind\nIf their sizes match, you can combine vectors to form matrices, and stick matrices together with vectors or other matrices. cbind (“column bind”) and rbind (“row bind”) are the functions to use.\ncbind binds together columns of two objects. One thing it can do is put vectors together to form a matrix:\n(C = cbind(1:3,4:6,5:7))\nR interprets vectors as row or column vectors according to what you’re doing with them. Here it treats them as column vectors so that columns exist to be bound together. On the other hand,\n(D = rbind(1:3,4:6))\ntreats them as rows. Now we have two matrices that can be combined.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nVerify that rbind(C,D) works, cbind(C,C) works, but cbind(C,D) doesn’t. Why not?\n\n\n\n\n\n4.2.3 Matrix indexing\nMatrix indexing is like vector indexing except that you have to specify both the row and column, or range of rows and columns. For example z=A[2,3] sets z equal to 6, which is the (2nd row, 3rd column) entry of the matrix A that you recently created, and\nA[2,2:3]\n(B=A[2:3,1:2])\nThere is an easy shortcut to extract entire rows or columns: leave out the l imits, leaving a blank before or after the comma.\n(first.row=A[1,])\n(second.column=A[,2])\n(What does A[,] do?)\nAs with vectors, indexing also works in reverse for assigning values to matrix entries. For example,\n(A[1,1]=12)\nYou can do the same with blocks, rows, or columns, for example\n(A[1,]=c(2,4,5))\nIf you use which on a matrix, R will normally treat the matrix as a vector — so for example which(A==8) will give the answer 6 (figure out why). However, which does have an option that will treat its argument as a matrix:\nwhich(A==8,arr.ind=TRUE)",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#lists",
    "href": "Lab1/no_solution.html#lists",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "4.3 Lists",
    "text": "4.3 Lists\nWhile vectors and matrices may seem familiar, lists are probably new to you. Vectors and matrices have to contain elements that are all the same type: lists in R can contain anything — vectors, matrices, other lists… Indexing lists is a little different too: use double square brackets [[ ]] (rather than single square brackets as for a vector) to extract an element of a list by number or name, or $ to extract an element by name (only). Given a list like this:\nL = list(A=x,B=y,C=c(\"a\",\"b\",\"c\"))\nThen L$A, L[[\"A\"]], and L[[1]] will all grab the first element of the list.\nYou won’t use lists too much at the beginning, but many of R’s own results are structured in the form of lists.",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "Lab1/no_solution.html#data-frames",
    "href": "Lab1/no_solution.html#data-frames",
    "title": "Lab 1: An introduction to R for ecological modeling",
    "section": "4.4 Data frames",
    "text": "4.4 Data frames\nData frames are the solution to the problem that most data sets have several different kinds of variables observed for each sample (e.g. categorical site location and continuous rainfall), but matrices can only contain a single type of data. Data frames are a hybrid of lists and vectors; internally, they are a list of vectors that may be of different types but must all be the same length, but you can do most of the same things with them (e.g., extracting a subset of rows) that you can do with matrices. You can index them either the way you would index a list, using [[ ]] or $ — where each variable is a different item in the list — or the way you would index a matrix. Use as.matrix if you have a data frame (where all variables are the same type) that you really want to be a matrix, e.g. if you need to transpose it (use as.data.frame to go the other way).",
    "crumbs": [
      "Lab 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Labs CSA-40306 Ecological Modelling and Data Analysis",
    "section": "",
    "text": "Practical labs for the course CSA-40306 Ecological Modelling and Data Analysis at Wageningen University.\nBased on Ben Bolker’s book Ecological Models and Data in R modified over the years by Bob Douma and Alejandro Morales.\nHow to use these practical labs\n\nUse the sidebar to navigate through the labs. These follow the different chapters in the book (but not all chapters are covered in the course).\nExercises are included throughout the labs as subheadings denoted Exercise\nA copy of each lab is provided at the end of the sidebar with possible solutions. The solutions are given after each Exercise in their own headings denoted Solution.\nThe output of the different code chunks are not given and this is on purpose to prevent that you just read the material. You are meant to execute the code yourself and judge the output. Please type the code yourself rather than copy-pasting. This may seem like a waste of time but somehow the learning process is more effective when you actually type the code yourself.\nIf you suspect that something went wrong, please ask your colleagues and see if you can figure out the problem together. Do not immediately ask the teachers.\n\nNote on how the labs have evolved over time\nThe book by Benjamin Bolker is one of a kind and still very valuable to learn the foundations of likelihood functions and the role they play in model-based inference in ecology.\nHowever, the code use in the book and R supplement are about 20 years old at the time of writing. In the labs we have made our best efforts over the year to update the material while at the same time maintaining some level of consistency.\nBiggest changes include:\n\nUse of Stan for all the Bayesian modeling (labs 6, 7 and 9).\nUse of ggplot2 and other tools from the tidyverse in parts of the labs."
  },
  {
    "objectID": "Lab1/material.html",
    "href": "Lab1/material.html",
    "title": "1 Learning outcomes",
    "section": "",
    "text": "The aim of this tutorial is to learn the basics of R. After completing the tutorial you will be able to:\n\nInstall R and associated packages and Rstudio\nDo interactive calculations in the R console\nConsult the built-in help in R\nCreate, modify and run R scripts\nSetup a typical workflow in R (loading libraries, reading data, doing statistics and saving results)\nWork with the most common data types in R (vectors, matrices, lists and data frames)"
  },
  {
    "objectID": "Lab1/material.html#how-to-use-this-tutorial",
    "href": "Lab1/material.html#how-to-use-this-tutorial",
    "title": "1 Learning outcomes",
    "section": "2.1 How to use this tutorial",
    "text": "2.1 How to use this tutorial\n\nThis tutorial contains many sample calculations. It is important to do these yourself—type them in at your keyboard and see what happens on your screen —to get the feel of working in R.\nExercises in the middle of a section should be done immediately, and make sure you have them rightly solved before moving on. Some other, more challenging, exercises appear at the end of some sections, and these can be left until later."
  },
  {
    "objectID": "Lab1/material.html#what-is-r",
    "href": "Lab1/material.html#what-is-r",
    "title": "1 Learning outcomes",
    "section": "2.2 What is R?",
    "text": "2.2 What is R?\nR is an object-oriented scripting language that\n\ncan be used for numerical simulation of deterministic and stochastic dynamic models.\nhas an extensive set of functions for classical and modern statistical data analysis and modeling.\nhas graphics functions for visualizing data and model output\n\nR is an open source project, available for free download via the Web. Originally a research project in statistical computing it is now managed by a development team that includes a number of well-regarded statisticians. It is widely used by statistical researchers and a growing number of theoretical ecologists and ecological modelers as a platform for making new methods available to users."
  },
  {
    "objectID": "Lab1/material.html#installing-r-on-your-computer-basics",
    "href": "Lab1/material.html#installing-r-on-your-computer-basics",
    "title": "1 Learning outcomes",
    "section": "2.3 Installing R on your computer: basics",
    "text": "2.3 Installing R on your computer: basics\nIf R is already installed on your computer, you can skip this section.\nThe main source for R is the CRAN home page http://cran.r-project.org. You can get the source code, but most users will prefer a precompiled version. To get one of these from CRAN:\n\ngo to http://cran.r-project.org/mirrors.html and find a mirror site that is geographically somewhat near you.\nFind the appropriate page for your operating system — when you get to the download section, go to base rather than contrib. Download the binary file (e.g. base/R-x.y.z-win32.exe for Windows, R-x.y.z.dmg for MacOS, where x.y.z is the version number).\nRead and follow the instructions (which are pretty much “click on the icon”)."
  },
  {
    "objectID": "Lab1/material.html#installing-and-using-rstudio",
    "href": "Lab1/material.html#installing-and-using-rstudio",
    "title": "1 Learning outcomes",
    "section": "2.4 Installing and using Rstudio",
    "text": "2.4 Installing and using Rstudio\nNowadays, programs are available that integrate R in a sophisticated way by combining a console, a syntax highlighting editor (giving colors to R commands and allowing you to identify missing parentheses, quotation marks etc.), tools for plotting, debugging, workspace management and connections to versioning systems into one program. Rstudio is currently the most common program around R and is highly recommended if R is the core of your work. Rstudio can be found and downloaded at https://www.rstudio.com.\nIf the wrong R version is launched after you installed Rstudio, you can change the R version that is used in Rstudio by clicking on Tools and Global Options, choose R general and select the location of the R version you want.\nRstudio usually displays four panels. The default setting is a script editor at the top-left of the screen, the console at the bottom-left, the global environment (which shows what is stored in the memory) at the top-right, and a plotting region at bottom-right. As you see some panel have multiple tabs that include other useful features such as the help (bottom-right), the (available and loaded) packages (bottom-right) etc."
  },
  {
    "objectID": "Lab1/material.html#the-r-package-system",
    "href": "Lab1/material.html#the-r-package-system",
    "title": "1 Learning outcomes",
    "section": "2.5 The R package system",
    "text": "2.5 The R package system\nThe standard distributions of R include several packages, user-contributed suites of add-on functions. This lab uses some packages that are not part of the standard distribution. In general, you can install additional packages from within R using the Packages menu, or the install.packages command.\nYou may be able to install new packages from a menu within R. Type and it will install the package ggplot2 that you will use in the next tutorial.\ninstall.packages(\"ggplot2\")\n(for example — this installs the ggplot2 package). You can install more than one package at a time:\ninstall.packages(c(\"ggplot2\",\"nlme\"))\n(c stands for “combine”, and is the command for combining multiple things into a single object.)\nSome of the important functions and packages (collections of functions) for statistical modeling and data analysis are summarized in Table 2. Venables and Ripley (2002) give a good practical (although somewhat advanced) overview, and you can find a list of available packages and their contents at CRAN, the main R website (http://www.cran.r-project.org — select a mirror site near you and click on Package sources). For the most part, we will not be concerned here with this side of R."
  },
  {
    "objectID": "Lab1/material.html#interactive-calculations-in-the-console",
    "href": "Lab1/material.html#interactive-calculations-in-the-console",
    "title": "1 Learning outcomes",
    "section": "2.6 Interactive calculations in the console",
    "text": "2.6 Interactive calculations in the console\nThe console is where you enter commands for R to execute interactively, meaning that the command is executed and the result is displayed as soon as you hit the Enter key (bottom-left panel in Rstudio). For example, at the command prompt &gt;, type in 2+2 and hit Enter\n2 + 2\nTo do anything complicated, you have to store the results from calculations by assigning them to variables, using = or &lt;-. For example:\na = 2+2\nR automatically creates the variable a and stores the result (4) in it, but it doesn’t print anything. This may seem strange, but you’ll often be creating and manipulating huge sets of data that would fill many screens, so the default is to skip printing the results. To ask R to print the value, just type the variable name by itself at the command prompt:\na\n(the [1] at the beginning of the line is just R printing an index of element numbers; if you print a result that displays on multiple lines, R will put an index at the beginning of each line. print(a) also works to print the value of a variable.) By default, a variable created this way is a vector, and it is numeric because we gave R a number rather than some other type of data (e.g.  a character string like \"pxqr\"). In this case a is a numeric vector of length 1, which acts just like a number.\nYou could also type a=2+2; a, using a semicolon to put two or more commands on a single line. Conversely, you can break lines anywhere that R can tell you haven’t finished your command and R will give you a “continuation” prompt (+) to let you know that it doesn’t think you’re finished yet: try typing\na = 3*(4 + [Enter]\n5)\nto see what happens. You will sometimes see the continuation prompt when you don’t expect it, e.g. if you forget to close parentheses.If you get stuck continuing a command you don’t want—e.g. you opened the wrong parentheses—just hit the Escape key or the stop icon in the menu bar to get out.\nVariable names in R must begin with a letter, followed by letters or digits You can break up long names with a period, as in very.long.variable.number.3, or an underscore (_), but you can’t use blank spaces in variable names (or at least it’s not worth the trouble). Variable names in R are case sensitive, so Abc and abc are different variables. Make variable names long enough to remember, short enough to type.N.per.ha or pop.density are better than x and y (too short) or available.nitrogen.per.hectare (too long). Avoid c, l, q, t, C, D, F, I, and T, which are either built-in R functions or hard to tell apart.\nR does calculations with variables as if they were numbers. It uses +, -, *, /, and ^ for addition, subtraction, multiplication, division and exponentiation, respectively. For example:\nx = 5\ny = 2\nz1 = x*y ## no output\nz2 = x/y ## no output\nz3 = x^y ## no output\nz2\nz3\nEven though R did not display the values of x and y, it “remembers” that it assigned values to them. Type x; y to display the values.\nYou can retrieve and edit previous commands. The up-arrow (\\(\\uparrow\\)) in the console recalls previous commands to the prompt. They also can be found in the top-right tab History. For example, you can bring back the second-to-last command and edit it into\nz3 = 2*x^y\nYou can combine several operations in one calculation:\nA = 3\nC = (A+2*sqrt(A))/(A+5*sqrt(A))\nC\nParentheses specify the order of operations. The command\nC = A + 2*sqrt(A)/A + 5*sqrt(A)\nis not the same as the one above; rather, it is equivalent to C=A + 2*(sqrt(A)/A) + 5*sqrt(A).\nThe default order of operations is: (1) parentheses; (2) exponentiation, or powers, (3) multiplication and division, (4) addition and subtraction.\nb = 12-4/2^3 gives 12 - 4/8 = 12 - 0.5 = 11.5\nb = (12-4)/2^3 gives 8/8 = 1\nb = -1^2 gives -(1^2) = -1\nb = (-1)^2 gives 1\nIn complicated expressions you might start off by using parentheses to specify explicitly what you want, such as b = 12 - (4/(2^3)) or at least b = 12 - 4/(2^3); a few extra sets of parentheses never hurt, although when you get confused it’s better to think through the order of operations rather than flailing around adding parentheses at random. R also has many built-in mathematical functions that operate on variables such as:\n\nabs: Absolute values.\n\ncos, sin and tan: For trigonometry (arguments always in radians).\n\nexp: Exponential function.\n\nlog, log10: Natural logarithm and logarithm base 10.\n\nsqrt: Square root.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUsing editing shortcuts wherever you can, have R compute the values of:\n\n\\(\\frac{2^7}{2^7 - 1}\\) and compare it with \\(\\left({1 - \\frac{1}{2^7}}\\right)^{-1}\\)\n\n\\(1 + 0.2\\)\n\\(1 + 0.2 + 0.2^2/2 + \\cos(2.3)\\)\n\\(\\log(1)\\)\n\\(\\exp(0.2)\\)\n\nThe standard normal probability density, \\(\\frac{1}{\\sqrt{2\\pi}}\\exp^{-x^2/2}\\), for values of \\(x=1\\) and \\(x=2\\) (R knows \\(\\pi\\) as pi). You can check your answers against the built-in function for the normal distribution; dnorm(1) and dnorm(2) should give you the values for the standard normal for \\(x=1\\) and \\(x=2\\)."
  },
  {
    "objectID": "Lab1/material.html#the-help-system",
    "href": "Lab1/material.html#the-help-system",
    "title": "1 Learning outcomes",
    "section": "2.7 The help system",
    "text": "2.7 The help system\nR has a help system, although it is generally better for providing detail or reminding you how to do things than for basic ``how do I …?’’ questions.\n\nYou can get help on any R function by entering\n\n?foo\nWhere foo is the name of the function you are interested in (e.g., try ?sin).\n\n??topic or help.search(\"topic\") (with quotes) will list information related to topic available in the base system or in any extra installed packages: then use ?topic to see the information, perhaps using library(pkg) to load the appropriate package first. help.search uses “fuzzy matching” — for example, help.search(\"log\") finds 528 entries (on my particular system) including lots of functions with “plot”, which includes the letters “lot”, which are almost like “log”. If you can’t stand it, you can turn this behavior off by specifying the incantation help.search(\"log\",agrep=FALSE) (81 results which still include matches for “logistic”, “myelogenous”, and “phylogeny” …)\nOn-line help resources - just google it. In our experience, the help provided by R requires some experience in the R language to be able to understand it. Therefore, a general, but practical advice is to Google your problem. In 99.9% of the cases someone else had a similar question in the past which were solved by the R community, often in https://stackoverflow.com.\nLarge language models can often help with basic R operations and recent models may handle more advanced modeling functions. However, you should always be aware that (i) the quality of the responses you get depend on how often a particular error or function has been discussed online and (ii) these generative AI can “say” things are factually incorrect because they do not “understand” what they are saying and it can be very hard to distinguish proper answers from these “hallucinations”. Best case scenario the answer is so wrong that when you try it, it just doesn’t run in R. Worst case, the answer “runs” but it does not do what you actually wanted to do. For that reason (and many others) it is better to use these AI tools to help you learn rather than as a virtual slave coder."
  },
  {
    "objectID": "Lab1/material.html#using-scripts-and-data-files",
    "href": "Lab1/material.html#using-scripts-and-data-files",
    "title": "1 Learning outcomes",
    "section": "3.1 Using scripts and data files",
    "text": "3.1 Using scripts and data files\nModeling and complicated data analysis are often much easier if you use scripts, which are a series of commands stored in a text file. Scripting has a number of advantages and should be standard practice when doing statistics for reasons of transparency (you can see what you have done), repeatability (tomorrow you will get the same result as today) and transferability (a colleague can easily check what you have done and redo your analysis). Even for relatively simple tasks, script files are useful for building up a calculation step-by-step, making sure that each part works before adding on to it. We recommend you making a habit typing all commands in a script editor before sending it the console, otherwise important parts of your analysis may get lost because you did not store them.\nRstudio has an advanced script editor that recognizes R syntax by giving different colors to different R commands and by automatic completion of parentheses. You can also use Windows Notepad or Wordpad but you should not use MS Word.\nMost programs for working with models or analyzing data follow a simple pattern of program parts:\n\n“Setup” statements. For example, load some packages, or run another script file that creates some functions (more on functions later).\nInput some data from a file or the keyboard. For example, read in data from a text file.\nCarry out the calculations that you want. For example, fit several statistical models to the data and compare them.\nPrint the results, graph them, or save them to a file. For example, graph the results, and save the graph to disk for including in your term project.\n\nTo tell R where data and script files are located, you can do any one of the following:\n\nSpell out the path, or file location, explicitly. (Use a single forward slash to separate folders (e.g. \"c:/My Documents/R/script.R\"): this works on all platforms.)\nChange your working directory to wherever the file(s) are located using the setwd (set working directory) function, e.g. setwd(\"c:/temp\") or through clicking on ‘Session’ and ‘set working directory’. Changing your working directory is more efficient in the long run, if you save all the script and data files for a particular project in the same directory and switch to that directory when you start work.\nAssociate an RStudio project to your folder. This means that every time you open this project in RStudio, the working directory will be set to that folder, you can easily refer to all the data and scripts within the folder without having to worry about the absolute location in your computer. It also means that you can share this project with a colleague and they can run your code without having to change"
  },
  {
    "objectID": "Lab1/material.html#typical-workflow-in-r-an-example-using-linear-regression",
    "href": "Lab1/material.html#typical-workflow-in-r-an-example-using-linear-regression",
    "title": "1 Learning outcomes",
    "section": "3.2 Typical workflow in R: an example using linear regression",
    "text": "3.2 Typical workflow in R: an example using linear regression\nTo get a feel for a typical workflow in R we’ll fit a straight-line model (linear regression) to data.\nStart a blank R script (File -&gt; New File -&gt; R script) and save it on a convenient location.\nBelow are some data on the maximum growth rate \\(r_{max}\\) of laboratory populations of the green alga Chlorella vulgaris as a function of light intensity (\\(\\mu\\)E per m\\(^2\\) per second). These experiments were run during the system-design phase of the study reported by Fussman et al. (2000).\nLight: 20, 20, 20, 20, 21, 24, 44, 60, 90, 94, 101\n\\(r_{max}\\): 1.73, 1.65, 2.02, 1.89, 2.61, 1.36, 2.37, 2.08, 2.69, 2.32, 3.67\nTo analyze these data in R, first enter them as numerical vectors in your script and send them to the console:\nLight = c(20,20,20,20,21,24,44,60,90,94,101)\nrmax = c(1.73,1.65,2.02,1.89,2.61,1.36,2.37,2.08,2.69,2.32,3.67)\nThe function c combines the individual numbers into a vector. Try recalling (with \\(\\uparrow\\)) and modifying the above command to\nLight=20,20,20,20,21,24,44,60,90,94,101\nand see the error message you get: in order to create a vector of specified numbers, you must use the c function. Don’t be afraid of error messages: the answer to “what would happen if I …?” is usually “try it and see!”\nTo see a histogram of the growth rates enter hist(rmax), which opens a graphics window and displays the histogram. There are many other built-in statistics functions: for example mean(rmax) computes you the mean, and sd(rmax) and var(rmax) compute the standard deviation and variance, respectively. Play around with these functions, and any others you can think of.\nTo see how light intensity affects algal rate of increase, type\nplot(rmax ~ Light)\nin the script (and send it the console) to plot rmax (\\(y\\)) against Light (\\(x\\)). The ~ sign implies “as a function of”. Alternatively, type plot(Light,rmax). A linear regression would seem like a reasonable model for these data. We’ll soon be adding to it. The figure below shows several more ways to adjust the appearance of lines and points in R.\n\n\n\n\n\n\n\n\n\nR’s default plotting character is an open circle. Open symbols are generally better than closed symbols for plotting because it is easier to see where they overlap, but you could include pch=16 in the plot command if you wanted closed circles instead.\nTo perform linear regression we create a linear model using the lm (linear model) function:\nfit = lm(rmax~Light)\n(Note that linear model is read as “model \\(r_{max}\\) as a function of light”.)\nThe lm command produces no output at all, but it creates fit as an object, i.e. a data structure consisting of multiple parts, holding the results of a regression analysis with rmax being modeled as a function of Light. Unlike most statistics packages, R rarely produces automatic summary output from an analysis. Statistical analyses in R are done by creating a model, and then giving additional commands to extract desired information about the model or display results graphically.\nTo get a summary of the results, enter the command summary(fit). R sets up model objects (more on this later) so that the function summary “knows” that fit was created by lm, and produces an appropriate summary of results for an lm object:\nsummary(fit)\n[If you’ve had (and remember) a statistics course the output will make sense to you. The table of coefficients gives the estimated regression line as \\(r_{max} = 1.581 + 0.014 \\times Light\\), and associated with each coefficient is the standard error of the estimate, the \\(t\\)-statistic value for testing whether the coefficient is nonzero, and the \\(p\\)-value corresponding to the \\(t\\)-statistic. Below the table, the adjusted R-squared gives the estimated fraction of the variance explained by the regression line, and the \\(p\\)-value in the last line is an overall test for significance of the model against the null hypothesis that the response variable is independent of the predictors.]\nYou can add the regression line to the plot of the data with a function taking fit as its input (if you closed the plot of the data, you will need to create it again in order to add the regression line):\nabline(fit)\n(abline, pronounced “a b line”, is a general-purpose function for adding lines to a plot: you can specify horizontal or vertical lines, a slope and an intercept, or a regression model: ?abline).\nYou can get the coefficients by using the coef function:\ncoef(fit)\nYou can also “interrogate” fit directly. Type names(fit) to get a list of the components of fit, and then use the $ symbol to extract components according to their names.\nnames(fit)\nFor more information (perhaps more than you want) about fit, use str(fit) (for structure). You can get the regression coefficients this way:\nfit$coefficients\nIt’s good to be able to look inside R objects when necessary, but all other things being equal you should prefer (e.g.) coef(x) to x$coefficients.\nUsually data is loaded from a file. To illustrate this, the file ChlorellaGrowth.txt from the course files (Brightspace or Teams). In ChlorellaGrowth.txt the two variables are entered as columns of a data matrix. Then instead of typing these in by hand, the command\nX = read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nreads the file (from the current directory) and puts the data values into the variable X; header=TRUE specifies that the file includes column names. Note that as specified above you need to make sure that R is looking for the data file in the right place … either move the data file to your current working directory, or change the line so that it points to the actual location of the data file.\nExtract the variables from X with the commands\nLight = X[,1]\nrmax = X[,2]\nThink of these as shorthand for “Light = everything in column 1 of X”, and “rmax = everything in column 2 of X” (we’ll learn about working with data matrices later). From there on out it’s the same as before, with some additions that set the axis labels and add a title.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGrab the file Intro2.R from the course files. Make a copy with a different name, and modify the copy so that it does linear regression of algal growth rate on the natural log of light intensity, LogLight=log(Light), and plots the data appropriately.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun Intro2.R, then enter the command plot(fit) in the console and follow the directions in the console. Figure out what just happened.\n\n\n\nR produced a series of diagnostic plots exploring whether or not the fitted linear model is a suitable fit to the data. In each of the plots, the 3 most extreme points (the most likely candidates for “outliers”) have been identified according to their sequence in the data set.\nThe axes in plots are scaled automatically, but the outcome is not always ideal (e.g. if you want several graphs with exactly the same axis limits). You can use the xlim and ylim arguments in plot to control the limits: plot(x,y,xlim=c(x1,x2), [other stuff]) will draw the graph with the \\(x\\)-axis running from x1 to x2, and using ylim=c(y1,y2) within the plot command will do the same for the \\(y\\)-axis.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCreate a plot of growth rate versus light intensity with the \\(x\\)-axis running from 0 to 120 and the \\(y\\)-axis running from 1 to 4.\n\n\n\nYou can place several graphs within a single figure by using the par function (short for “parameter”) to adjust the layout of the plot. For example, the command\npar(mfrow=c(2,3))\ndivides the plotting area into 2 rows and 3 columns. As R draws a series of graphs, it places them along the top row from left to right, then along the next row, and so on. mfcol=c(2,3) has the same effect except that R draws successive graphs down the first column, then down the second column, and so on.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify the script as follows. Use mfcol=c(2,1) to create graphs of growth rate as a function of Light, and of log(growth rate) as a function of log(Light) in the same figure. Do the same again, using mfcol=c(1,2).\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse ?plot and ?par to read about other plot control parameters. The function plot takes a number of arguments such as type that allows you to draw a line through a series of points instead of plotting separate points. With par you can change anything you want to change. For example, you can choose the color of the points, or the shape of the points. You should definitely skim read this help as this is one of the longest help files in the whole R system!).\nThen draw a \\(2 \\times 2\\) set of plots, each showing the line \\(y=5x + 3\\) with \\(x\\) running from 3 to 8, but with 4 different line styles and 4 different line colors.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify one of your scripts so that at the very end it saves the plot to disk. In Windows you can do this with specific functions like jpeg or png. Use ?jpeg or ?png to read about these functions. Note that the argument filename can include the path to a folder; for example, in Windows you can use filename=\"c:/temp/Intro2Figure.png\".\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nDo some online research to figure out:\n\nHow to rotate the y-axis labels\nHow to change the background of the plot to grey"
  },
  {
    "objectID": "Lab1/material.html#vectors",
    "href": "Lab1/material.html#vectors",
    "title": "1 Learning outcomes",
    "section": "4.1 Vectors",
    "text": "4.1 Vectors\nAn important class of data types are vectors and matrices (1- and 2-dimensional rectangular arrays of numbers). Operations with vectors and matrices may seem a bit abstract now, but we need them to do useful things later. The only properties of vectors are their type (or class) and length, although they can also have an associated list of names.\nWe’ve already seen two ways to create vectors in R:\n\nA command in the console window or a script file listing the values, such as\n\ninitialsize=c(1,3,5,7,9,11)\n\nUsing read.table:\n\ninitialsize=read.table(\"c:/temp/initialdata.txt\")\n(assuming of course that the file exists in the right place).\nYou can then use a vector in calculations as if it were a number (more or less)\nfinalsize=initialsize+1\nfinalsize\nnewsize=sqrt(initialsize)\nnewsize\nNotice that R applied each operation to every element in the vector. Similarly, commands like initialsize-5, 2*initialsize, initialsize/10 apply subtraction, multiplication, and division to each element of the vector. The same is true for\ninitialsize^2\nIn R the default is to apply functions and operations to vectors in an element by element (or “vectorized”) manner. This is an extremely useful property in R.\n\n4.1.1 Functions for creating vectors\nYou can use the seq function to create a set of regularly spaced values. seq’s syntax is x=seq(from,to,by) or x=seq(from,to) or x=seq(from,to,length.out). The first form generates a vector starting with from with the last entry not extending further than than to in steps of by. In the second form the value of by is assumed to be 1 or -1, depending on whether from or to is larger. The third form creates a vector with the desired endpoints and length. The syntax from:to is a shortcut for seq(from,to):\n1:8\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse seq to create the vector v=(1 5 9 13), and to create a vector going from 1 to 5 in increments of 0.2.\n\n\n\nYou can use rep to create a constant vector such as (1 1 1 1); the basic syntax is rep(values,lengths). For example,\nrep(3,5)\ncreates a vector in which the value 3 is repeated 5 times. rep will repeat a whole vector multiple times\nrep(1:3,3)\nor will repeat each of the elements in a vector a given number of times:\nrep(1:3,each=3)\nEven more flexibly, you can repeat each element in the vector a different number of times:\n\nrep( c(3,4),c(2,5) )\n\n[1] 3 3 4 4 4 4 4\n\n\nThe value 3 was repeated 2 times, followed by the value 4 repeated 5 times. rep can be a little bit mind-blowing as you get started, but it will turn out to be useful.\n\n\n4.1.2 Vector indexing\nYou will often want to extract a specific entry or other part of a vector. This procedure is called vector indexing, and uses square brackets ([]):\nz = c(1,3,5,7,9,11)\nz[3]\nz[3] extracts the third item, or element, in the vector z. You can also access a block of elements using the functions for vector construction, e.g.\nz[2:5]\nextracts the second through fifth elements.\nWhat happens if you enter v=z[seq(1,5,2)] ? Try it and see, and make sure you understand what happened.\nYou can extracted irregularly spaced elements of a vector. For example\nz[c(1,2,5)]\nYou can also use indexing to set specific values within a vector. For example,\nz[1]=12\nchanges the value of the first entry in z while leaving all the rest alone, and\nz[c(1,3,5)]=c(22,33,44)\nchanges the first, third, and fifth values (note that we had to use c to create the vector — can you interpret the error message you get if you try z[1,3,5] ?)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a one-line command to extract a vector consisting of the second, first, and third elements of z in that order.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a script file that computes values of \\(y=\\frac{(x-1)}{(x+1)}\\) for \\(x=1,2,\\cdots,10\\), and plots \\(y\\) versus \\(x\\) with the points plotted and connected by a line hint: in ?plot, search for type.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThe sum of the geometric series \\(1 + r + r^2 + r^3 + ... + r^n\\) approaches the limit \\(1/(1-r)\\) for \\(r &lt; 1\\) as \\(n \\rightarrow \\infty\\).\nSet the values \\(r=0.5\\) and \\(n=10\\), and then write a one-line command that creates the vector \\(G = (r^0,r^1,r^2,...,r^n)\\). Compare the sum (using sum) of this vector to the limiting value \\(1/(1-r)\\).\nRepeat for \\(n=50\\). (Note that comparing very similar numeric values can be tricky: rounding can happen, and some numbers cannot be represented exactly in binary (computer) notation. By default R displays 7~significant digits (options(\"digits\")).\nFor example:\nx = 1.999999\nx\nx-2\nx=1.9999999999999\nx\nx-2\nAll the digits are still there, in the second case, but they are not shown. Also note that x-2 is not exactly \\(-1 \\times 10^{-13}\\); this is unavoidable.)\n\n\n\n\n\n4.1.3 Logical operators\nLogical operators return a value of TRUE or FALSE. For example, try:\na=1\nb=3\nd=a&lt;b\ne=(a&gt;b)\nd\ne\nThe parentheses around (a&gt;b) are optional but make the code easier to read. One special case where you do need parentheses (or spaces) is when you make comparisons with negative values; a&lt;-1 will surprise you by setting a=1, because &lt;- (representing a left-pointing arrow) is equivalent to = in R. Use a&lt; -1, or more safely a&lt;(-1), to make this comparison.\nSome comparison operators in R:\n\nx &lt; y: x is smaller than y.\n\nx &lt; y: x is greater than y.\n\nx &lt;= y: x is smaller or equal than y.\n\nx &gt;= y: x is greater or equal than y.\n\nx == y: x is equal to y.\n\nx != y: x is not equal to y.\n\nWhen we compare two vectors or matrices of the same size, or compare a number with a vector or matrix, comparisons are done element-by-element. For example,\nx = 1:5\nb = (x &lt;= 3)\nSo if x and y are vectors, then (x == y) will return a vector of values giving the element-by-element comparisons. If you want to know whether x and y are identical vectors, use identical(x,y) which returns a single value: TRUE if each entry in x equals the corresponding entry in y, otherwise FALSE. You can use ?Logical to read more about logical operators. Note the difference between = and ==\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun the code block below. Can you figure out why a==b gives different results the first and second time you execute them?\na =  1:3\nb =  2:4\na == b\na =  b\na == b\n\n\n\nExclamation marks ! are used in R to mean “not”; != (not ==) means “not equal to”.\nR also does arithmetic on logical values, treating TRUE as 1 and FALSE as 0. So sum(b) returns the value 3, telling us that three entries of x satisfied the condition (x&lt;=3). This is useful for (e.g.) seeing how many of the elements of a vector are larger than a cutoff value. Build more complicated conditions by using logical operators to combine comparisons:\n\n!: Negation\n&, &&: AND\n|, ||: OR\n\nOR is non-exclusive, meaning that x|y is true if either x or y or both are true (a ham-and-cheese sandwich would satisfy the condition “ham OR cheese”). For example, try\na = c(1,2,3,4)\nb = c(1,1,5,5)\n(a &lt; b) & (a &gt; 3)\n(a &lt; b) | (a &gt; 3)\nand make sure you understand what happened. If it’s confusing, try breaking up the expression and looking at the results of a&lt;b and a&gt;3 separately first. The two forms of AND and OR differ in how they handle vectors. The shorter one does element-by-element comparisons; the longer one only looks at the first element in each vector.\nWe can also use logical vectors (lists of TRUE and FALSE values) to pick elements out of vectors. This is important, e.g., for subsetting data (getting rid of those pesky outliers!)\nAs a simple example, we might want to focus on just the low-light values of \\(r_{max}\\) in the Chlorella example:\nX=read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight=X[,1]\nrmax=X[,2]\nlowLight = Light[Light&lt;50]\nlowLightrmax = rmax[Light&lt;50]\nlowLight\nlowLightrmax\nWhat is really happening here (think about it for a minute) is that Light&lt;50 generates a logical vector the same length as Light (TRUE TRUE TRUE ...) which is then used to select the appropriate values.\nIf you want the positions at which Light is lower than 50, you could say (1:length(Light))[Light&lt;50], but you can also use a built-in function: which(Light&lt;50). If you wanted the position at which the maximum value of Light occurs, you could say which(Light==max(Light)). (This normally results in a vector of length 1; when could it give a longer vector?) There is even a built-in command for this specific function, which.max (although which.max always returns just the first position at which the maximum occurs).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWhat would happen if instead of setting lowLight you replaced Light by saying Light=Light[Light&lt;50], and then rmax=rmax[Light&lt;50]?\nWhy would that be wrong?\nTry it with some temporary variables — set Light2=Light and rmax2=rmax and then play with Light2 and rmax2 so you dont mess up your working variables — and work out what happened…\nWe can also combine logical operators (making sure to use the element-by-element & and | versions of AND and OR):\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nrunif(n) is a function (more on it soon) that generates a vector of n random, uniformly distributed numbers between 0 and 1. Create a vector of 20 numbers, then select the subset of those numbers that is less than the mean. (If you want your answers to match mine exactly, use set.seed(273) to set the random-number generator to a particular starting point before you use runif. [273 is an arbitrary number I chose].)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind the positions of the elements that are less than the mean of the vector you just created (e.g. if your vector were (0.1 0.9. 0.7 0.3) the answer would be (1 4)).\n\n\n\nAs I mentioned in passing above, vectors can have names associated with their elements: if they do, you can also extract elements by name (use names to find out the names).\nx = c(first=7,second=5,third=2)\nnames(x)\nx[\"first\"]\nx[c(\"third\",\"first\")]\nFinally, it is sometimes handy to be able to drop a particular set of elements, rather than taking a particular set: you can do this with negative indices. For example, x[-1] extracts all but the first element of a vector.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nSpecify two ways to take only the elements in the odd positions (first, third, …) of a vector of arbitrary length."
  },
  {
    "objectID": "Lab1/material.html#matrices",
    "href": "Lab1/material.html#matrices",
    "title": "1 Learning outcomes",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\n\n4.2.1 Creating matrices\nA matrix is a two-dimensional array, and has the same kind of variables in every column. You can create matrices of numbers by creating a vector of the matrix entries, and then reshaping them to the desired number of rows and columns using the function matrix. For example\n(X = matrix(1:6,nrow=2,ncol=3))\ntakes the values 1 to 6 and reshapes them into a 2 by 3 matrix.\nBy default R loads the values into the matrix column-wise (this is probably counter-intuitive since we’re used to reading tables row-wise). Use the optional parameter byrow to change this behavior. For example:\n(A = matrix(1:9,nrow=3,ncol=3,byrow=TRUE))\nR will re-cycle through entries in the data vector, if necessary to fill a matrix of the specified size. So for example\nmatrix(1,nrow=10,ncol=10)\ncreates a \\(10 \\times 10\\) matrix of ones.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse a command of the form X = matrix(v,nrow=2,ncol=4) where v is a data vector, to create the following matrix X:\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n\n\nIf you can, try to use R commands to construct the vector rather than typing out all of the individual values.\nR will also collapse a matrix to behave like a vector whenever it makes sense: for example sum(X) above is 12.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse rnorm (which is like runif, but generates Gaussian (normally distributed) numbers with a specified mean and standard deviation instead) and matrix to create a \\(5 \\times 7\\) matrix of Gaussian random numbers with mean 1 and standard deviation 2. (Use set.seed(273) again for consistency).\n\n\n\nAnother useful function for creating matrices is diag. diag(v,n) creates an \\(n \\times n\\) matrix with data vector \\(v\\) on its diagonal. So for example diag(1,5) creates the \\(5 \\times 5\\) identity matrix, which has 1’s on the diagonal and 0 everywhere else. Try diag(1:5,5) and diag(1:2,5); Observe what is happening. Is this desired behaviour?\nFinally, you can use the data.entry function. This function can only edit existing matrices, but for example\nA=matrix(0,nrow=3,ncol=4)\ndata.entry(A)\nwill create A as a \\(3 \\times 4\\) matrix, and then call up a spreadsheet-like interface in which you can change the values to whatever you need.\n\n\n4.2.2 cbind and rbind\nIf their sizes match, you can combine vectors to form matrices, and stick matrices together with vectors or other matrices. cbind (“column bind”) and rbind (“row bind”) are the functions to use.\ncbind binds together columns of two objects. One thing it can do is put vectors together to form a matrix:\n(C = cbind(1:3,4:6,5:7))\nR interprets vectors as row or column vectors according to what you’re doing with them. Here it treats them as column vectors so that columns exist to be bound together. On the other hand,\n(D = rbind(1:3,4:6))\ntreats them as rows. Now we have two matrices that can be combined.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nVerify that rbind(C,D) works, cbind(C,C) works, but cbind(C,D) doesn’t. Why not?\n\n\n\n\n\n4.2.3 Matrix indexing\nMatrix indexing is like vector indexing except that you have to specify both the row and column, or range of rows and columns. For example z=A[2,3] sets z equal to 6, which is the (2nd row, 3rd column) entry of the matrix A that you recently created, and\nA[2,2:3]\n(B=A[2:3,1:2])\nThere is an easy shortcut to extract entire rows or columns: leave out the l imits, leaving a blank before or after the comma.\n(first.row=A[1,])\n(second.column=A[,2])\n(What does A[,] do?)\nAs with vectors, indexing also works in reverse for assigning values to matrix entries. For example,\n(A[1,1]=12)\nYou can do the same with blocks, rows, or columns, for example\n(A[1,]=c(2,4,5))\nIf you use which on a matrix, R will normally treat the matrix as a vector — so for example which(A==8) will give the answer 6 (figure out why). However, which does have an option that will treat its argument as a matrix:\nwhich(A==8,arr.ind=TRUE)"
  },
  {
    "objectID": "Lab1/material.html#lists",
    "href": "Lab1/material.html#lists",
    "title": "1 Learning outcomes",
    "section": "4.3 Lists",
    "text": "4.3 Lists\nWhile vectors and matrices may seem familiar, lists are probably new to you. Vectors and matrices have to contain elements that are all the same type: lists in R can contain anything — vectors, matrices, other lists… Indexing lists is a little different too: use double square brackets [[ ]] (rather than single square brackets as for a vector) to extract an element of a list by number or name, or $ to extract an element by name (only). Given a list like this:\nL = list(A=x,B=y,C=c(\"a\",\"b\",\"c\"))\nThen L$A, L[[\"A\"]], and L[[1]] will all grab the first element of the list.\nYou won’t use lists too much at the beginning, but many of R’s own results are structured in the form of lists."
  },
  {
    "objectID": "Lab1/material.html#data-frames",
    "href": "Lab1/material.html#data-frames",
    "title": "1 Learning outcomes",
    "section": "4.4 Data frames",
    "text": "4.4 Data frames\nData frames are the solution to the problem that most data sets have several different kinds of variables observed for each sample (e.g. categorical site location and continuous rainfall), but matrices can only contain a single type of data. Data frames are a hybrid of lists and vectors; internally, they are a list of vectors that may be of different types but must all be the same length, but you can do most of the same things with them (e.g., extracting a subset of rows) that you can do with matrices. You can index them either the way you would index a list, using [[ ]] or $ — where each variable is a different item in the list — or the way you would index a matrix. Use as.matrix if you have a data frame (where all variables are the same type) that you really want to be a matrix, e.g. if you need to transpose it (use as.data.frame to go the other way)."
  },
  {
    "objectID": "Lab1/solution.html",
    "href": "Lab1/solution.html",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "",
    "text": "The aim of this tutorial is to learn the basics of R. After completing the tutorial you will be able to:\n\nInstall R and associated packages and Rstudio\nDo interactive calculations in the R console\nConsult the built-in help in R\nCreate, modify and run R scripts\nSetup a typical workflow in R (loading libraries, reading data, doing statistics and saving results)\nWork with the most common data types in R (vectors, matrices, lists and data frames)",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#how-to-use-this-tutorial",
    "href": "Lab1/solution.html#how-to-use-this-tutorial",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.1 How to use this tutorial",
    "text": "2.1 How to use this tutorial\n\nThis tutorial contains many sample calculations. It is important to do these yourself—type them in at your keyboard and see what happens on your screen —to get the feel of working in R.\nExercises in the middle of a section should be done immediately, and make sure you have them rightly solved before moving on. Some other, more challenging, exercises appear at the end of some sections, and these can be left until later.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#what-is-r",
    "href": "Lab1/solution.html#what-is-r",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.2 What is R?",
    "text": "2.2 What is R?\nR is an object-oriented scripting language that\n\ncan be used for numerical simulation of deterministic and stochastic dynamic models.\nhas an extensive set of functions for classical and modern statistical data analysis and modeling.\nhas graphics functions for visualizing data and model output\n\nR is an open source project, available for free download via the Web. Originally a research project in statistical computing it is now managed by a development team that includes a number of well-regarded statisticians. It is widely used by statistical researchers and a growing number of theoretical ecologists and ecological modelers as a platform for making new methods available to users.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#installing-r-on-your-computer-basics",
    "href": "Lab1/solution.html#installing-r-on-your-computer-basics",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.3 Installing R on your computer: basics",
    "text": "2.3 Installing R on your computer: basics\nIf R is already installed on your computer, you can skip this section.\nThe main source for R is the CRAN home page http://cran.r-project.org. You can get the source code, but most users will prefer a precompiled version. To get one of these from CRAN:\n\ngo to http://cran.r-project.org/mirrors.html and find a mirror site that is geographically somewhat near you.\nFind the appropriate page for your operating system — when you get to the download section, go to base rather than contrib. Download the binary file (e.g. base/R-x.y.z-win32.exe for Windows, R-x.y.z.dmg for MacOS, where x.y.z is the version number).\nRead and follow the instructions (which are pretty much “click on the icon”).",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#installing-and-using-rstudio",
    "href": "Lab1/solution.html#installing-and-using-rstudio",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.4 Installing and using Rstudio",
    "text": "2.4 Installing and using Rstudio\nNowadays, programs are available that integrate R in a sophisticated way by combining a console, a syntax highlighting editor (giving colors to R commands and allowing you to identify missing parentheses, quotation marks etc.), tools for plotting, debugging, workspace management and connections to versioning systems into one program. Rstudio is currently the most common program around R and is highly recommended if R is the core of your work. Rstudio can be found and downloaded at https://www.rstudio.com.\nIf the wrong R version is launched after you installed Rstudio, you can change the R version that is used in Rstudio by clicking on Tools and Global Options, choose R general and select the location of the R version you want.\nRstudio usually displays four panels. The default setting is a script editor at the top-left of the screen, the console at the bottom-left, the global environment (which shows what is stored in the memory) at the top-right, and a plotting region at bottom-right. As you see some panel have multiple tabs that include other useful features such as the help (bottom-right), the (available and loaded) packages (bottom-right) etc.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#the-r-package-system",
    "href": "Lab1/solution.html#the-r-package-system",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.5 The R package system",
    "text": "2.5 The R package system\nThe standard distributions of R include several packages, user-contributed suites of add-on functions. This lab uses some packages that are not part of the standard distribution. In general, you can install additional packages from within R using the Packages menu, or the install.packages command.\nYou may be able to install new packages from a menu within R. Type and it will install the package ggplot2 that you will use in the next tutorial.\ninstall.packages(\"ggplot2\")\n(for example — this installs the ggplot2 package). You can install more than one package at a time:\ninstall.packages(c(\"ggplot2\",\"nlme\"))\n(c stands for “combine”, and is the command for combining multiple things into a single object.)\nSome of the important functions and packages (collections of functions) for statistical modeling and data analysis are summarized in Table 2. Venables and Ripley (2002) give a good practical (although somewhat advanced) overview, and you can find a list of available packages and their contents at CRAN, the main R website (http://www.cran.r-project.org — select a mirror site near you and click on Package sources). For the most part, we will not be concerned here with this side of R.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#interactive-calculations-in-the-console",
    "href": "Lab1/solution.html#interactive-calculations-in-the-console",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.6 Interactive calculations in the console",
    "text": "2.6 Interactive calculations in the console\nThe console is where you enter commands for R to execute interactively, meaning that the command is executed and the result is displayed as soon as you hit the Enter key (bottom-left panel in Rstudio). For example, at the command prompt &gt;, type in 2+2 and hit Enter\n2 + 2\nTo do anything complicated, you have to store the results from calculations by assigning them to variables, using = or &lt;-. For example:\na = 2+2\nR automatically creates the variable a and stores the result (4) in it, but it doesn’t print anything. This may seem strange, but you’ll often be creating and manipulating huge sets of data that would fill many screens, so the default is to skip printing the results. To ask R to print the value, just type the variable name by itself at the command prompt:\na\n(the [1] at the beginning of the line is just R printing an index of element numbers; if you print a result that displays on multiple lines, R will put an index at the beginning of each line. print(a) also works to print the value of a variable.) By default, a variable created this way is a vector, and it is numeric because we gave R a number rather than some other type of data (e.g.  a character string like \"pxqr\"). In this case a is a numeric vector of length 1, which acts just like a number.\nYou could also type a=2+2; a, using a semicolon to put two or more commands on a single line. Conversely, you can break lines anywhere that R can tell you haven’t finished your command and R will give you a “continuation” prompt (+) to let you know that it doesn’t think you’re finished yet: try typing\na = 3*(4 + [Enter]\n5)\nto see what happens. You will sometimes see the continuation prompt when you don’t expect it, e.g. if you forget to close parentheses.If you get stuck continuing a command you don’t want—e.g. you opened the wrong parentheses—just hit the Escape key or the stop icon in the menu bar to get out.\nVariable names in R must begin with a letter, followed by letters or digits You can break up long names with a period, as in very.long.variable.number.3, or an underscore (_), but you can’t use blank spaces in variable names (or at least it’s not worth the trouble). Variable names in R are case sensitive, so Abc and abc are different variables. Make variable names long enough to remember, short enough to type.N.per.ha or pop.density are better than x and y (too short) or available.nitrogen.per.hectare (too long). Avoid c, l, q, t, C, D, F, I, and T, which are either built-in R functions or hard to tell apart.\nR does calculations with variables as if they were numbers. It uses +, -, *, /, and ^ for addition, subtraction, multiplication, division and exponentiation, respectively. For example:\nx = 5\ny = 2\nz1 = x*y ## no output\nz2 = x/y ## no output\nz3 = x^y ## no output\nz2\nz3\nEven though R did not display the values of x and y, it “remembers” that it assigned values to them. Type x; y to display the values.\nYou can retrieve and edit previous commands. The up-arrow (\\(\\uparrow\\)) in the console recalls previous commands to the prompt. They also can be found in the top-right tab History. For example, you can bring back the second-to-last command and edit it into\nz3 = 2*x^y\nYou can combine several operations in one calculation:\nA = 3\nC = (A+2*sqrt(A))/(A+5*sqrt(A))\nC\nParentheses specify the order of operations. The command\nC = A + 2*sqrt(A)/A + 5*sqrt(A)\nis not the same as the one above; rather, it is equivalent to C=A + 2*(sqrt(A)/A) + 5*sqrt(A).\nThe default order of operations is: (1) parentheses; (2) exponentiation, or powers, (3) multiplication and division, (4) addition and subtraction.\nb = 12-4/2^3 gives 12 - 4/8 = 12 - 0.5 = 11.5\nb = (12-4)/2^3 gives 8/8 = 1\nb = -1^2 gives -(1^2) = -1\nb = (-1)^2 gives 1\nIn complicated expressions you might start off by using parentheses to specify explicitly what you want, such as b = 12 - (4/(2^3)) or at least b = 12 - 4/(2^3); a few extra sets of parentheses never hurt, although when you get confused it’s better to think through the order of operations rather than flailing around adding parentheses at random. R also has many built-in mathematical functions that operate on variables such as:\n\nabs: Absolute values.\n\ncos, sin and tan: For trigonometry (arguments always in radians).\n\nexp: Exponential function.\n\nlog, log10: Natural logarithm and logarithm base 10.\n\nsqrt: Square root.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUsing editing shortcuts wherever you can, have R compute the values of:\n\n\\(\\frac{2^7}{2^7 - 1}\\) and compare it with \\(\\left({1 - \\frac{1}{2^7}}\\right)^{-1}\\)\n\n\\(1 + 0.2\\)\n\\(1 + 0.2 + 0.2^2/2 + \\cos(2.3)\\)\n\\(\\log(1)\\)\n\\(\\exp(0.2)\\)\n\nThe standard normal probability density, \\(\\frac{1}{\\sqrt{2\\pi}}\\exp^{-x^2/2}\\), for values of \\(x=1\\) and \\(x=2\\) (R knows \\(\\pi\\) as pi). You can check your answers against the built-in function for the normal distribution; dnorm(1) and dnorm(2) should give you the values for the standard normal for \\(x=1\\) and \\(x=2\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n(2^7)/(2^7-1)\n(1-2^7)^(-1)\n\n\n\n1 + 0.2\n1 + 0.2 + 0.2^2/2 + cos(2.3)\nlog(1)\nexp(0.2)\n\n\n\nx = 1\n1/sqrt(2*pi)*exp(-x^2/2)\ndnorm(1)\nx = 2\n1/sqrt(2*pi)*exp(-x^2/2)\ndnorm(2)",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#the-help-system",
    "href": "Lab1/solution.html#the-help-system",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "2.7 The help system",
    "text": "2.7 The help system\nR has a help system, although it is generally better for providing detail or reminding you how to do things than for basic ``how do I …?’’ questions.\n\nYou can get help on any R function by entering\n\n?foo\nWhere foo is the name of the function you are interested in (e.g., try ?sin).\n\n??topic or help.search(\"topic\") (with quotes) will list information related to topic available in the base system or in any extra installed packages: then use ?topic to see the information, perhaps using library(pkg) to load the appropriate package first. help.search uses “fuzzy matching” — for example, help.search(\"log\") finds 528 entries (on my particular system) including lots of functions with “plot”, which includes the letters “lot”, which are almost like “log”. If you can’t stand it, you can turn this behavior off by specifying the incantation help.search(\"log\",agrep=FALSE) (81 results which still include matches for “logistic”, “myelogenous”, and “phylogeny” …)\nOn-line help resources - just google it. In our experience, the help provided by R requires some experience in the R language to be able to understand it. Therefore, a general, but practical advice is to Google your problem. In 99.9% of the cases someone else had a similar question in the past which were solved by the R community, often in https://stackoverflow.com.\nLarge language models can often help with basic R operations and recent models may handle more advanced modeling functions. However, you should always be aware that (i) the quality of the responses you get depend on how often a particular error or function has been discussed online and (ii) these generative AI can “say” things are factually incorrect because they do not “understand” what they are saying and it can be very hard to distinguish proper answers from these “hallucinations”. Best case scenario the answer is so wrong that when you try it, it just doesn’t run in R. Worst case, the answer “runs” but it does not do what you actually wanted to do. For that reason (and many others) it is better to use these AI tools to help you learn rather than as a virtual slave coder.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#using-scripts-and-data-files",
    "href": "Lab1/solution.html#using-scripts-and-data-files",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "3.1 Using scripts and data files",
    "text": "3.1 Using scripts and data files\nModeling and complicated data analysis are often much easier if you use scripts, which are a series of commands stored in a text file. Scripting has a number of advantages and should be standard practice when doing statistics for reasons of transparency (you can see what you have done), repeatability (tomorrow you will get the same result as today) and transferability (a colleague can easily check what you have done and redo your analysis). Even for relatively simple tasks, script files are useful for building up a calculation step-by-step, making sure that each part works before adding on to it. We recommend you making a habit typing all commands in a script editor before sending it the console, otherwise important parts of your analysis may get lost because you did not store them.\nRstudio has an advanced script editor that recognizes R syntax by giving different colors to different R commands and by automatic completion of parentheses. You can also use Windows Notepad or Wordpad but you should not use MS Word.\nMost programs for working with models or analyzing data follow a simple pattern of program parts:\n\n“Setup” statements. For example, load some packages, or run another script file that creates some functions (more on functions later).\nInput some data from a file or the keyboard. For example, read in data from a text file.\nCarry out the calculations that you want. For example, fit several statistical models to the data and compare them.\nPrint the results, graph them, or save them to a file. For example, graph the results, and save the graph to disk for including in your term project.\n\nTo tell R where data and script files are located, you can do any one of the following:\n\nSpell out the path, or file location, explicitly. (Use a single forward slash to separate folders (e.g. \"c:/My Documents/R/script.R\"): this works on all platforms.)\nChange your working directory to wherever the file(s) are located using the setwd (set working directory) function, e.g. setwd(\"c:/temp\") or through clicking on ‘Session’ and ‘set working directory’. Changing your working directory is more efficient in the long run, if you save all the script and data files for a particular project in the same directory and switch to that directory when you start work.\nAssociate an RStudio project to your folder. This means that every time you open this project in RStudio, the working directory will be set to that folder, you can easily refer to all the data and scripts within the folder without having to worry about the absolute location in your computer. It also means that you can share this project with a colleague and they can run your code without having to change",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#typical-workflow-in-r-an-example-using-linear-regression",
    "href": "Lab1/solution.html#typical-workflow-in-r-an-example-using-linear-regression",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "3.2 Typical workflow in R: an example using linear regression",
    "text": "3.2 Typical workflow in R: an example using linear regression\nTo get a feel for a typical workflow in R we’ll fit a straight-line model (linear regression) to data.\nStart a blank R script (File -&gt; New File -&gt; R script) and save it on a convenient location.\nBelow are some data on the maximum growth rate \\(r_{max}\\) of laboratory populations of the green alga Chlorella vulgaris as a function of light intensity (\\(\\mu\\)E per m\\(^2\\) per second). These experiments were run during the system-design phase of the study reported by Fussman et al. (2000).\nLight: 20, 20, 20, 20, 21, 24, 44, 60, 90, 94, 101\n\\(r_{max}\\): 1.73, 1.65, 2.02, 1.89, 2.61, 1.36, 2.37, 2.08, 2.69, 2.32, 3.67\nTo analyze these data in R, first enter them as numerical vectors in your script and send them to the console:\nLight = c(20,20,20,20,21,24,44,60,90,94,101)\nrmax = c(1.73,1.65,2.02,1.89,2.61,1.36,2.37,2.08,2.69,2.32,3.67)\nThe function c combines the individual numbers into a vector. Try recalling (with \\(\\uparrow\\)) and modifying the above command to\nLight=20,20,20,20,21,24,44,60,90,94,101\nand see the error message you get: in order to create a vector of specified numbers, you must use the c function. Don’t be afraid of error messages: the answer to “what would happen if I …?” is usually “try it and see!”\nTo see a histogram of the growth rates enter hist(rmax), which opens a graphics window and displays the histogram. There are many other built-in statistics functions: for example mean(rmax) computes you the mean, and sd(rmax) and var(rmax) compute the standard deviation and variance, respectively. Play around with these functions, and any others you can think of.\nTo see how light intensity affects algal rate of increase, type\nplot(rmax ~ Light)\nin the script (and send it the console) to plot rmax (\\(y\\)) against Light (\\(x\\)). The ~ sign implies “as a function of”. Alternatively, type plot(Light,rmax). A linear regression would seem like a reasonable model for these data. We’ll soon be adding to it. The figure below shows several more ways to adjust the appearance of lines and points in R.\n\n\n\n\n\n\n\n\n\nR’s default plotting character is an open circle. Open symbols are generally better than closed symbols for plotting because it is easier to see where they overlap, but you could include pch=16 in the plot command if you wanted closed circles instead.\nTo perform linear regression we create a linear model using the lm (linear model) function:\nfit = lm(rmax~Light)\n(Note that linear model is read as “model \\(r_{max}\\) as a function of light”.)\nThe lm command produces no output at all, but it creates fit as an object, i.e. a data structure consisting of multiple parts, holding the results of a regression analysis with rmax being modeled as a function of Light. Unlike most statistics packages, R rarely produces automatic summary output from an analysis. Statistical analyses in R are done by creating a model, and then giving additional commands to extract desired information about the model or display results graphically.\nTo get a summary of the results, enter the command summary(fit). R sets up model objects (more on this later) so that the function summary “knows” that fit was created by lm, and produces an appropriate summary of results for an lm object:\nsummary(fit)\n[If you’ve had (and remember) a statistics course the output will make sense to you. The table of coefficients gives the estimated regression line as \\(r_{max} = 1.581 + 0.014 \\times Light\\), and associated with each coefficient is the standard error of the estimate, the \\(t\\)-statistic value for testing whether the coefficient is nonzero, and the \\(p\\)-value corresponding to the \\(t\\)-statistic. Below the table, the adjusted R-squared gives the estimated fraction of the variance explained by the regression line, and the \\(p\\)-value in the last line is an overall test for significance of the model against the null hypothesis that the response variable is independent of the predictors.]\nYou can add the regression line to the plot of the data with a function taking fit as its input (if you closed the plot of the data, you will need to create it again in order to add the regression line):\nabline(fit)\n(abline, pronounced “a b line”, is a general-purpose function for adding lines to a plot: you can specify horizontal or vertical lines, a slope and an intercept, or a regression model: ?abline).\nYou can get the coefficients by using the coef function:\ncoef(fit)\nYou can also “interrogate” fit directly. Type names(fit) to get a list of the components of fit, and then use the $ symbol to extract components according to their names.\nnames(fit)\nFor more information (perhaps more than you want) about fit, use str(fit) (for structure). You can get the regression coefficients this way:\nfit$coefficients\nIt’s good to be able to look inside R objects when necessary, but all other things being equal you should prefer (e.g.) coef(x) to x$coefficients.\nUsually data is loaded from a file. To illustrate this, the file ChlorellaGrowth.txt from the course files (Brightspace or Teams). In ChlorellaGrowth.txt the two variables are entered as columns of a data matrix. Then instead of typing these in by hand, the command\nX = read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nreads the file (from the current directory) and puts the data values into the variable X; header=TRUE specifies that the file includes column names. Note that as specified above you need to make sure that R is looking for the data file in the right place … either move the data file to your current working directory, or change the line so that it points to the actual location of the data file.\nExtract the variables from X with the commands\nLight = X[,1]\nrmax = X[,2]\nThink of these as shorthand for “Light = everything in column 1 of X”, and “rmax = everything in column 2 of X” (we’ll learn about working with data matrices later). From there on out it’s the same as before, with some additions that set the axis labels and add a title.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGrab the file Intro2.R from the course files. Make a copy with a different name, and modify the copy so that it does linear regression of algal growth rate on the natural log of light intensity, LogLight=log(Light), and plots the data appropriately.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA possible solution is as follows:\nX = read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight = X[,1]\nrmax = X[,2]\nlogLight = log(Light) # natural logarithm \\n\nop = par(cex=1,cex.main=0.9, mar = c(4,4,4,0.5), las = 1)\nplot(logLight,rmax,\n    xlab=expression(Log~light~intensity~(mu*E/m^2/s)),\n    ylab=\"Maximum growth rate rmax (1/d)\",pch=16)\ntitle(main=\"Data from Fussmann et al. (2000)\")\nfit = lm(rmax~logLight)\nsummary(fit)\nabline(fit)\nrcoef = round(coef(fit),digits=3)\ntext(3.7,3.5,paste(\"rmax=\",rcoef[1],\"+\",rcoef[2],\"log(Light)\"))\npar(op)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun Intro2.R, then enter the command plot(fit) in the console and follow the directions in the console. Figure out what just happened.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy entering ?plot.lm R brings up the Help page for the function plot.lm that carries out a plot command for an object produced by lm. This is one example of how R uses the fact that statistical analyses are stored as model objects. fit “knows” what kind of object it is (in this case an object of type lm, and so plot(fit) invokes a function that produces plots suitable for an lm object.\n\n\n\n\n\n\nR produced a series of diagnostic plots exploring whether or not the fitted linear model is a suitable fit to the data. In each of the plots, the 3 most extreme points (the most likely candidates for “outliers”) have been identified according to their sequence in the data set.\nThe axes in plots are scaled automatically, but the outcome is not always ideal (e.g. if you want several graphs with exactly the same axis limits). You can use the xlim and ylim arguments in plot to control the limits: plot(x,y,xlim=c(x1,x2), [other stuff]) will draw the graph with the \\(x\\)-axis running from x1 to x2, and using ylim=c(y1,y2) within the plot command will do the same for the \\(y\\)-axis.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCreate a plot of growth rate versus light intensity with the \\(x\\)-axis running from 0 to 120 and the \\(y\\)-axis running from 1 to 4.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA possible solution would be:\nplot(Light,rmax,xlim=c(0,120),ylim=c(1,4))\n\n\n\n\n\n\nYou can place several graphs within a single figure by using the par function (short for “parameter”) to adjust the layout of the plot. For example, the command\npar(mfrow=c(2,3))\ndivides the plotting area into 2 rows and 3 columns. As R draws a series of graphs, it places them along the top row from left to right, then along the next row, and so on. mfcol=c(2,3) has the same effect except that R draws successive graphs down the first column, then down the second column, and so on.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify the script as follows. Use mfcol=c(2,1) to create graphs of growth rate as a function of Light, and of log(growth rate) as a function of log(Light) in the same figure. Do the same again, using mfcol=c(1,2).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmfcol=c(2,1)\nplot(rmax ~ Light)\nplot(rmax ~ logLight)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse ?plot and ?par to read about other plot control parameters. The function plot takes a number of arguments such as type that allows you to draw a line through a series of points instead of plotting separate points. With par you can change anything you want to change. For example, you can choose the color of the points, or the shape of the points. You should definitely skim read this help as this is one of the longest help files in the whole R system!).\nThen draw a \\(2 \\times 2\\) set of plots, each showing the line \\(y=5x + 3\\) with \\(x\\) running from 3 to 8, but with 4 different line styles and 4 different line colors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA possible solution would be:\npar(mfrow=c(2,2))\nx = c(3:8)\ny = 5*x+3\nplot(y~x,type=\"l\")\nplot(y~x,type=\"l\",lty=1,col=\"red\")\nplot(y~x,type=\"l\",lty=2,col=\"blue\")\nplot(y~x,type=\"l\",lty=4,col=\"orange\")\npar(mfrow = c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nModify one of your scripts so that at the very end it saves the plot to disk. In Windows you can do this with specific functions like jpeg or png. Use ?jpeg or ?png to read about these functions. Note that the argument filename can include the path to a folder; for example, in Windows you can use filename=\"c:/temp/Intro2Figure.png\".\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA possible solution to save the figure in PNG format\npng(\"test.png\")\npar(mfrow=c(2,2))\nx = c(3:8)\ny = 5*x+3\nplot(y~x,type=\"l\")\nplot(y~x,type=\"l\",lty=1,col=\"red\")\nplot(y~x,type=\"l\",lty=2,col=\"blue\")\nplot(y~x,type=\"l\",lty=4,col=\"orange\")\ndev.off()\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nDo some online research to figure out:\n\nHow to rotate the y-axis labels\nHow to change the background of the plot to grey\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can rotate the y-axis labels by setting las = 2.\nYou can change the background by setting par(bg = \"grey\")",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#vectors",
    "href": "Lab1/solution.html#vectors",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "4.1 Vectors",
    "text": "4.1 Vectors\nAn important class of data types are vectors and matrices (1- and 2-dimensional rectangular arrays of numbers). Operations with vectors and matrices may seem a bit abstract now, but we need them to do useful things later. The only properties of vectors are their type (or class) and length, although they can also have an associated list of names.\nWe’ve already seen two ways to create vectors in R:\n\nA command in the console window or a script file listing the values, such as\n\ninitialsize=c(1,3,5,7,9,11)\n\nUsing read.table:\n\ninitialsize=read.table(\"c:/temp/initialdata.txt\")\n(assuming of course that the file exists in the right place).\nYou can then use a vector in calculations as if it were a number (more or less)\nfinalsize=initialsize+1\nfinalsize\nnewsize=sqrt(initialsize)\nnewsize\nNotice that R applied each operation to every element in the vector. Similarly, commands like initialsize-5, 2*initialsize, initialsize/10 apply subtraction, multiplication, and division to each element of the vector. The same is true for\ninitialsize^2\nIn R the default is to apply functions and operations to vectors in an element by element (or “vectorized”) manner. This is an extremely useful property in R.\n\n4.1.1 Functions for creating vectors\nYou can use the seq function to create a set of regularly spaced values. seq’s syntax is x=seq(from,to,by) or x=seq(from,to) or x=seq(from,to,length.out). The first form generates a vector starting with from with the last entry not extending further than than to in steps of by. In the second form the value of by is assumed to be 1 or -1, depending on whether from or to is larger. The third form creates a vector with the desired endpoints and length. The syntax from:to is a shortcut for seq(from,to):\n1:8\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse seq to create the vector v=(1 5 9 13), and to create a vector going from 1 to 5 in increments of 0.2.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nseq(1,13,4)\nseq(1,5,0.2)\n\n\n\n\n\n\nYou can use rep to create a constant vector such as (1 1 1 1); the basic syntax is rep(values,lengths). For example,\nrep(3,5)\ncreates a vector in which the value 3 is repeated 5 times. rep will repeat a whole vector multiple times\nrep(1:3,3)\nor will repeat each of the elements in a vector a given number of times:\nrep(1:3,each=3)\nEven more flexibly, you can repeat each element in the vector a different number of times:\n\nrep( c(3,4),c(2,5) )\n\n[1] 3 3 4 4 4 4 4\n\n\nThe value 3 was repeated 2 times, followed by the value 4 repeated 5 times. rep can be a little bit mind-blowing as you get started, but it will turn out to be useful.\n\n\n4.1.2 Vector indexing\nYou will often want to extract a specific entry or other part of a vector. This procedure is called vector indexing, and uses square brackets ([]):\nz = c(1,3,5,7,9,11)\nz[3]\nz[3] extracts the third item, or element, in the vector z. You can also access a block of elements using the functions for vector construction, e.g.\nz[2:5]\nextracts the second through fifth elements.\nWhat happens if you enter v=z[seq(1,5,2)] ? Try it and see, and make sure you understand what happened.\nYou can extracted irregularly spaced elements of a vector. For example\nz[c(1,2,5)]\nYou can also use indexing to set specific values within a vector. For example,\nz[1]=12\nchanges the value of the first entry in z while leaving all the rest alone, and\nz[c(1,3,5)]=c(22,33,44)\nchanges the first, third, and fifth values (note that we had to use c to create the vector — can you interpret the error message you get if you try z[1,3,5] ?)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a one-line command to extract a vector consisting of the second, first, and third elements of z in that order.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nz = c(1,3,5,7,9,11)\nz[c(2,1,3)]\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWrite a script file that computes values of \\(y=\\frac{(x-1)}{(x+1)}\\) for \\(x=1,2,\\cdots,10\\), and plots \\(y\\) versus \\(x\\) with the points plotted and connected by a line hint: in ?plot, search for type.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nx = c(1:10)\ny = (x-1)/(x+1)\nplot(y~x,type=\"b\")\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThe sum of the geometric series \\(1 + r + r^2 + r^3 + ... + r^n\\) approaches the limit \\(1/(1-r)\\) for \\(r &lt; 1\\) as \\(n \\rightarrow \\infty\\).\nSet the values \\(r=0.5\\) and \\(n=10\\), and then write a one-line command that creates the vector \\(G = (r^0,r^1,r^2,...,r^n)\\). Compare the sum (using sum) of this vector to the limiting value \\(1/(1-r)\\).\nRepeat for \\(n=50\\). (Note that comparing very similar numeric values can be tricky: rounding can happen, and some numbers cannot be represented exactly in binary (computer) notation. By default R displays 7~significant digits (options(\"digits\")).\nFor example:\nx = 1.999999\nx\nx-2\nx=1.9999999999999\nx\nx-2\nAll the digits are still there, in the second case, but they are not shown. Also note that x-2 is not exactly \\(-1 \\times 10^{-13}\\); this is unavoidable.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nr = 0.5\nn = 10\nsum(r^c(0:n))\nn = 50\nsum(r^c(0:n))\n\n\n\n\n\n\n\n\n4.1.3 Logical operators\nLogical operators return a value of TRUE or FALSE. For example, try:\na=1\nb=3\nd=a&lt;b\ne=(a&gt;b)\nd\ne\nThe parentheses around (a&gt;b) are optional but make the code easier to read. One special case where you do need parentheses (or spaces) is when you make comparisons with negative values; a&lt;-1 will surprise you by setting a=1, because &lt;- (representing a left-pointing arrow) is equivalent to = in R. Use a&lt; -1, or more safely a&lt;(-1), to make this comparison.\nSome comparison operators in R:\n\nx &lt; y: x is smaller than y.\n\nx &lt; y: x is greater than y.\n\nx &lt;= y: x is smaller or equal than y.\n\nx &gt;= y: x is greater or equal than y.\n\nx == y: x is equal to y.\n\nx != y: x is not equal to y.\n\nWhen we compare two vectors or matrices of the same size, or compare a number with a vector or matrix, comparisons are done element-by-element. For example,\nx = 1:5\nb = (x &lt;= 3)\nSo if x and y are vectors, then (x == y) will return a vector of values giving the element-by-element comparisons. If you want to know whether x and y are identical vectors, use identical(x,y) which returns a single value: TRUE if each entry in x equals the corresponding entry in y, otherwise FALSE. You can use ?Logical to read more about logical operators. Note the difference between = and ==\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun the code block below. Can you figure out why a==b gives different results the first and second time you execute them?\na =  1:3\nb =  2:4\na == b\na =  b\na == b\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe overwrote a by b in the command a = b and so the last statement confirms that they are identical.\n\n\n\n\n\n\nExclamation marks ! are used in R to mean “not”; != (not ==) means “not equal to”.\nR also does arithmetic on logical values, treating TRUE as 1 and FALSE as 0. So sum(b) returns the value 3, telling us that three entries of x satisfied the condition (x&lt;=3). This is useful for (e.g.) seeing how many of the elements of a vector are larger than a cutoff value. Build more complicated conditions by using logical operators to combine comparisons:\n\n!: Negation\n&, &&: AND\n|, ||: OR\n\nOR is non-exclusive, meaning that x|y is true if either x or y or both are true (a ham-and-cheese sandwich would satisfy the condition “ham OR cheese”). For example, try\na = c(1,2,3,4)\nb = c(1,1,5,5)\n(a &lt; b) & (a &gt; 3)\n(a &lt; b) | (a &gt; 3)\nand make sure you understand what happened. If it’s confusing, try breaking up the expression and looking at the results of a&lt;b and a&gt;3 separately first. The two forms of AND and OR differ in how they handle vectors. The shorter one does element-by-element comparisons; the longer one only looks at the first element in each vector.\nWe can also use logical vectors (lists of TRUE and FALSE values) to pick elements out of vectors. This is important, e.g., for subsetting data (getting rid of those pesky outliers!)\nAs a simple example, we might want to focus on just the low-light values of \\(r_{max}\\) in the Chlorella example:\nX=read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight=X[,1]\nrmax=X[,2]\nlowLight = Light[Light&lt;50]\nlowLightrmax = rmax[Light&lt;50]\nlowLight\nlowLightrmax\nWhat is really happening here (think about it for a minute) is that Light&lt;50 generates a logical vector the same length as Light (TRUE TRUE TRUE ...) which is then used to select the appropriate values.\nIf you want the positions at which Light is lower than 50, you could say (1:length(Light))[Light&lt;50], but you can also use a built-in function: which(Light&lt;50). If you wanted the position at which the maximum value of Light occurs, you could say which(Light==max(Light)). (This normally results in a vector of length 1; when could it give a longer vector?) There is even a built-in command for this specific function, which.max (although which.max always returns just the first position at which the maximum occurs).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nWhat would happen if instead of setting lowLight you replaced Light by saying Light=Light[Light&lt;50], and then rmax=rmax[Light&lt;50]?\nWhy would that be wrong?\nTry it with some temporary variables — set Light2=Light and rmax2=rmax and then play with Light2 and rmax2 so you dont mess up your working variables — and work out what happened…\nWe can also combine logical operators (making sure to use the element-by-element & and | versions of AND and OR):\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we combine logical operators we do not need intermediate variables:\nLight[Light&lt;50 & rmax &lt;= 2.0]\nrmax[Light&lt;50 & rmax &lt;= 2.0]\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nrunif(n) is a function (more on it soon) that generates a vector of n random, uniformly distributed numbers between 0 and 1. Create a vector of 20 numbers, then select the subset of those numbers that is less than the mean. (If you want your answers to match mine exactly, use set.seed(273) to set the random-number generator to a particular starting point before you use runif. [273 is an arbitrary number I chose].)\n\n\n\n\n\n\nSolution\n\n\n\n\n\na = runif(20)\na[a &lt; mean(a)]\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind the positions of the elements that are less than the mean of the vector you just created (e.g. if your vector were (0.1 0.9. 0.7 0.3) the answer would be (1 4)).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nwhich(a &lt; mean(a))\n\n\n\n\n\n\nAs I mentioned in passing above, vectors can have names associated with their elements: if they do, you can also extract elements by name (use names to find out the names).\nx = c(first=7,second=5,third=2)\nnames(x)\nx[\"first\"]\nx[c(\"third\",\"first\")]\nFinally, it is sometimes handy to be able to drop a particular set of elements, rather than taking a particular set: you can do this with negative indices. For example, x[-1] extracts all but the first element of a vector.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nSpecify two ways to take only the elements in the odd positions (first, third, …) of a vector of arbitrary length.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nn = 20\na = c(1:n)\na1 = a[seq(1,n,2)]\na2 = a[-seq(2,n,2)]",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#matrices",
    "href": "Lab1/solution.html#matrices",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\n\n4.2.1 Creating matrices\nA matrix is a two-dimensional array, and has the same kind of variables in every column. You can create matrices of numbers by creating a vector of the matrix entries, and then reshaping them to the desired number of rows and columns using the function matrix. For example\n(X = matrix(1:6,nrow=2,ncol=3))\ntakes the values 1 to 6 and reshapes them into a 2 by 3 matrix.\nBy default R loads the values into the matrix column-wise (this is probably counter-intuitive since we’re used to reading tables row-wise). Use the optional parameter byrow to change this behavior. For example:\n(A = matrix(1:9,nrow=3,ncol=3,byrow=TRUE))\nR will re-cycle through entries in the data vector, if necessary to fill a matrix of the specified size. So for example\nmatrix(1,nrow=10,ncol=10)\ncreates a \\(10 \\times 10\\) matrix of ones.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse a command of the form X = matrix(v,nrow=2,ncol=4) where v is a data vector, to create the following matrix X:\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n\n\nIf you can, try to use R commands to construct the vector rather than typing out all of the individual values.\nR will also collapse a matrix to behave like a vector whenever it makes sense: for example sum(X) above is 12.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmatrix(rep(1:2,4),nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUse rnorm (which is like runif, but generates Gaussian (normally distributed) numbers with a specified mean and standard deviation instead) and matrix to create a \\(5 \\times 7\\) matrix of Gaussian random numbers with mean 1 and standard deviation 2. (Use set.seed(273) again for consistency).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nset.seed(273)\nmatrix(rnorm(35,mean=1,sd=2),nrow=5,ncol=7)\n\n\n\n\n\n\nAnother useful function for creating matrices is diag. diag(v,n) creates an \\(n \\times n\\) matrix with data vector \\(v\\) on its diagonal. So for example diag(1,5) creates the \\(5 \\times 5\\) identity matrix, which has 1’s on the diagonal and 0 everywhere else. Try diag(1:5,5) and diag(1:2,5); Observe what is happening. Is this desired behaviour?\nFinally, you can use the data.entry function. This function can only edit existing matrices, but for example\nA=matrix(0,nrow=3,ncol=4)\ndata.entry(A)\nwill create A as a \\(3 \\times 4\\) matrix, and then call up a spreadsheet-like interface in which you can change the values to whatever you need.\n\n\n4.2.2 cbind and rbind\nIf their sizes match, you can combine vectors to form matrices, and stick matrices together with vectors or other matrices. cbind (“column bind”) and rbind (“row bind”) are the functions to use.\ncbind binds together columns of two objects. One thing it can do is put vectors together to form a matrix:\n(C = cbind(1:3,4:6,5:7))\nR interprets vectors as row or column vectors according to what you’re doing with them. Here it treats them as column vectors so that columns exist to be bound together. On the other hand,\n(D = rbind(1:3,4:6))\ntreats them as rows. Now we have two matrices that can be combined.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nVerify that rbind(C,D) works, cbind(C,C) works, but cbind(C,D) doesn’t. Why not?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nC=cbind(1:3,4:6,5:7)\nD=rbind(1:3,4:6)\nrbind(C,D)\ncbind(C,D)\n\n\n\n\n\n\n\n\n4.2.3 Matrix indexing\nMatrix indexing is like vector indexing except that you have to specify both the row and column, or range of rows and columns. For example z=A[2,3] sets z equal to 6, which is the (2nd row, 3rd column) entry of the matrix A that you recently created, and\nA[2,2:3]\n(B=A[2:3,1:2])\nThere is an easy shortcut to extract entire rows or columns: leave out the l imits, leaving a blank before or after the comma.\n(first.row=A[1,])\n(second.column=A[,2])\n(What does A[,] do?)\nAs with vectors, indexing also works in reverse for assigning values to matrix entries. For example,\n(A[1,1]=12)\nYou can do the same with blocks, rows, or columns, for example\n(A[1,]=c(2,4,5))\nIf you use which on a matrix, R will normally treat the matrix as a vector — so for example which(A==8) will give the answer 6 (figure out why). However, which does have an option that will treat its argument as a matrix:\nwhich(A==8,arr.ind=TRUE)",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#lists",
    "href": "Lab1/solution.html#lists",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "4.3 Lists",
    "text": "4.3 Lists\nWhile vectors and matrices may seem familiar, lists are probably new to you. Vectors and matrices have to contain elements that are all the same type: lists in R can contain anything — vectors, matrices, other lists… Indexing lists is a little different too: use double square brackets [[ ]] (rather than single square brackets as for a vector) to extract an element of a list by number or name, or $ to extract an element by name (only). Given a list like this:\nL = list(A=x,B=y,C=c(\"a\",\"b\",\"c\"))\nThen L$A, L[[\"A\"]], and L[[1]] will all grab the first element of the list.\nYou won’t use lists too much at the beginning, but many of R’s own results are structured in the form of lists.",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab1/solution.html#data-frames",
    "href": "Lab1/solution.html#data-frames",
    "title": "Lab 1: An introduction to R for ecological modeling (solutions)",
    "section": "4.4 Data frames",
    "text": "4.4 Data frames\nData frames are the solution to the problem that most data sets have several different kinds of variables observed for each sample (e.g. categorical site location and continuous rainfall), but matrices can only contain a single type of data. Data frames are a hybrid of lists and vectors; internally, they are a list of vectors that may be of different types but must all be the same length, but you can do most of the same things with them (e.g., extracting a subset of rows) that you can do with matrices. You can index them either the way you would index a list, using [[ ]] or $ — where each variable is a different item in the list — or the way you would index a matrix. Use as.matrix if you have a data frame (where all variables are the same type) that you really want to be a matrix, e.g. if you need to transpose it (use as.data.frame to go the other way).",
    "crumbs": [
      "Solutions",
      "Lab 1 (solutions)"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html",
    "href": "Lab10/no_solution.html",
    "title": "Lab 10",
    "section": "",
    "text": "In this practical you will learn\n\nThe concept of maximum marginal likelihood\nEstimate models with process and measurement errors\nEstimate multilevel non linear models with Gaussian quadrature\nThe Bayesian approach for the two models above\nSpecify informative priors using general knowledge",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html#model-with-measurement-error",
    "href": "Lab10/no_solution.html#model-with-measurement-error",
    "title": "Lab 10",
    "section": "3.1 Model with measurement error",
    "text": "3.1 Model with measurement error\nLet’s assume that the growth rates of trees within a forest follow a Gamma distribution and the measurements of this growth rate contain errors that follow a Normal distribution. This is the problem described in section 10.5.2 of the book. This is a model with two levels:\n\\[\n\\begin{align*}\n\\text{Level}~2~:~X_{true} &\\sim Gamma(a, s) \\\\\n\\text{Level}~1~:~X_{obs~~}  &\\sim Normal(X_{true}, \\sigma)\n\\end{align*}\n\\]\nWhere \\(X_{true}\\) are the true growth rates of the trees and \\(X_{obs}\\) are the measured growth rates. Notice that the level 1 uses as input the output of level 2 (\\(X_{true}\\)), which determines the order.\nIn this exercise we will work with synthetic data generated by stochastic simulation (see Chapter 5 of the book):\nset.seed(1001)\nx_true &lt;- rgamma(1000, shape = 3, scale = 10) # True growth rates\nx_obs  &lt;- rnorm(1000, mean = x_true, sd = 10) # Observed growth rates with error\nhist(x_obs, ylim = c(0,300))\nhist(x_true, add = TRUE, col = rgb(1,0,0,0.3))\nAccording to the general procedure described below, we can estimate \\(a\\) and \\(s\\) from level 2 by creating a marginal likelihood (integrating over level 1) and maximizing it (steps 2 - 3). This is shown in the next section. Once that estimation is done, we can estimate each of value of \\(X_{true}\\) by maximizing the conditional likelihood (step 4).\n\n3.1.1 Estimation of parameters\nThe marginal likelihood is defined as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{true,i} | a, s\\right) \\text{Normal}\\left(X_{obs,i} | X_{true,i} , \\sigma \\right) dX_{true,i}\n\\]\nThat is, for each observation we integrate over all the possible true values (that is the latent random variable we do not observe) to obtained a likelihood function that only contains observations and parameters. Because the measurement error is normal, we can reparameterized as:\n\\[\nL_m \\left(X_{obs,i} | a, s , \\sigma \\right) = \\prod_{i=1}^{n} \\int{\\text{Gamma}\\left(X_{obs,i} - \\epsilon_i | a, s\\right) \\text{Normal}\\left(\\epsilon_i |0 , \\sigma \\right) d\\epsilon_i}\n\\]\nWhere we replace \\(X_{true} = X_{obs} - \\epsilon\\). This form is how most multilevel models are expressed. In R the product of the two distributions is:\nprodfun &lt;- function(eps, a, s, sigma, x) {\n dgamma(x - eps, shape = a, scale = s)*dnorm(eps, mean = 0, sd =  sigma)\n}\nprodfun(eps = 1, a = 3, s = 10, sigma = 1, x = x_obs[1])\nWe can now integrate prodfun using the function integrate (note: this only works for one latent random variable, more advanced methods are needed when more latent variables are used):\nintegrate(f = prodfun, lower = -Inf, upper = Inf,\n          a = 3, s = 10, sigma = 1, x = x_obs[1],\n          rel.tol = 1e-6, abs.tol = 1e-6)$value\nWe can now build a function to compute the negative log marginal likelihood by applying this integral to each observations:\nNLML &lt;- function(x, a, s, sigma) {\n  # Marginal likelihood of each observation\n  ML &lt;- sapply(x, function(x) integrate(f = prodfun, lower = -Inf, upper = Inf,\n                                        rel.tol = 1e-6, abs.tol = 1e-6,\n               a = a, s = s, sigma = sigma, x = x)$value)\n  # Negative log marginal likelihood\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(x_obs, a = 3, s = 10, sigma = 1)\nNotice that this takes a bit longer than usual because we are doing 1000 numerical integrations. We can minimize the function NLML with mle2:\nlibrary(bbmle)\nfit &lt;- mle2(minuslogl = NLML,\n            start = list(a = 3, s = 10, sigma = 1),\n            lower = c(0,0,0),\n            data = list(x = x_obs), method = \"L-BFGS-B\")\nfit\nWe can now compare the true and estimated models for growth and error to see how well the estimation worked:\npars = coef(fit)\nplot(density(x_obs), ylim = c(0,0.045), xlab = \"Growth rate\",\n     ylab = \"Probability density\", main = \"\", las = 1, col = 3)\ncurve(dgamma(x, shape = 3, scale = 10), add = TRUE)\ncurve(dgamma(x, shape = pars[\"a\"], scale = pars[\"s\"]), add = TRUE, col = 2)\ncurve(dnorm(x, mean = 0, sd = 10), add = TRUE, lty = 2)\ncurve(dnorm(x, mean = 0, sd = pars[\"sigma\"]), add = TRUE, col = 2, lty = 2)\nlegend(\"topright\", c(\"Obs\", \"True growth\", \"Est growth\", \"True error\", \"Est error\"),\n       col = c(3, 1, 2, 1, 2), lty = c(1, 1, 1, 2, 2))\nWe can quickly estimate the confidence intervals using the quadratic approximation:\nconfint(fit, method = \"quad\")\nNotice that the true values (a = 3, s = 10 and sigma = 10) are within the confidence intervals, so the estimation has worked. We could also try building the likelihood profiles but that would take quite a while since this would require many more optimizations so we will skip this part.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with measurement errors",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html#model-for-nested-data",
    "href": "Lab10/no_solution.html#model-for-nested-data",
    "title": "Lab 10",
    "section": "3.2 Model for nested data",
    "text": "3.2 Model for nested data\nTo keep it within the same theme, in this example we will look at a simple dataset of tree growth, but this time using real data. The data describes the growth in height of several individuals of Loblolly pine (Pinus taeda). We can load the data as follows:\nlibrary(ggplot2)\ndata(Loblolly)\nsummary(Loblolly)\n# eyeballing; hm = 60, b = 0.25, c = 12 (check  Figure 3.9 in the book)\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) + geom_point() +\n  stat_function(fun = function(x) 60/(1 + exp(-0.25*(x - 12))), color = \"black\")\nEach tree is identified by the column Seed and six measurements of height over the age of the tree are reported. The growth curve of each tree will be modeled using a logistic curve:\n\\[\nh(t) =  \\frac{h_m}{1 + e^{-b\\left(t - c \\right)}\n\\]\nwhere \\(h\\) is the height of the tree, \\(h_m\\) is the maximum height, \\(b\\) control the steepness of the curve and \\(c\\) is the age at which the half maximum height is reached (see Chapter 3 of the book, I will refer to these parameters as traits from here on) and \\(t\\) is the age of the tree. We want to know the average values for the three traits as well as how much they vary across individuals (variation in traits across individuals is often very relevant in ecology). In a mixed model, \\(h_m\\), \\(b\\) and \\(c\\) are assumed to vary across individuals assuming particular distributions (we will assume normal distributions which is the standard in mixed models).\nAs a first approach, we can try to estimate the curve for each tree independently. This is a good way to get a first estimate of how much the traits vary across individuals as well as the observation error. As we will see in the examples below, non-linear mixed models as simple as these ones already need quite some constraints in order to work.\n\n3.2.1 Stepwise procedure\nThe stepwise procedure is as follows:\n\nFit the model to each individual separately, using maximum likelihood.\nTreat the maximum likelihood estimates from each individuals as if they were observations and analyze them with a separate model.\n\nLet’s setup a function to fit the logistic growth curve to each tree:\nlogistic &lt;- function(b, c, h_m, t) {\n  h_m/(1 + exp(-b*(t - c)))\n}\nmle_fun &lt;- function(b, c, h_m, sigma, t, h) {\n  hmod = logistic(b, c, h_m, t)\n  -sum(dnorm(h, hmod, sigma, log = TRUE))\n}\nWe can test the function for the first tree:\nseeds = unique(Loblolly$Seed)\ntree1 = subset(Loblolly, Seed == seeds[1])\nmle_fun(c = 12, b = 0.25, h_m = 60, sigma = 1, t = tree1$age, h = tree1$height)\nLet’s estimate the parameters for this first tree:\nlibrary(bbmle)\nfit1 = mle2(mle_fun, start = list(c = 12, b = 0.25, h_m = 60, sigma = 1),\n     data = list(t = tree1$age, h = tree1$height))\nfit1\nLet’s repeat it for all trees:\ntraits = matrix(NA, ncol = 4, nrow = length(seeds))\ncolnames(traits) = c(\"b\", \"c\", \"h_m\", \"sigma\")\nfor(i in 1:length(seeds)) {\n  tree = subset(Loblolly, Seed == seeds[i])\n  fit = mle2(mle_fun, start = list(c= 12, b = 0.25, h_m = 60, sigma = 1),\n             lower = c(c = 0, b = 1e-4, h_m = 10, sigma = 1e-4),\n             upper = c(c = 30, b = 1, h_m = 80, sigma = 20),\n            data = list(t = tree$age, h = tree$height), method = \"L-BFGS-B\")\n  traits[i,] = coef(fit)\n}\ntraits\nLet’s look at the individual fits by adding the predicitons to the data:\nLoblolly = transform(Loblolly,\n                     hm1  = rep(traits[,\"h_m\"], each = 6),\n                     c1  = rep(traits[,\"c\"], each = 6),\n                     b1  = rep(traits[,\"b\"], each = 6))\n# Prediction from stepwise model\nLoblolly = transform(Loblolly, pred_height1 = logistic(b1, c1, hm1, age))\n# Plot the data and predictions for each indicidual\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can now look at the (co-)variation of the traits:\nlibrary(GGally)\nggpairs(data = traits[,1:3])\nNotice that b and c are correlated. The standard averages and standard deviations can be used as initial estimates for the mixed model later:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nIn the next sections (and to keep it simple) we will fit a multilevel model where only \\(h_m\\) is allow to vary across trees.\n\n\n3.2.2 Estimating at population level\nTo obtain the mean estimates of \\(a\\), \\(b\\) and \\(h_m\\) we need to construct a marginal likelihood to integrate over the distribution of values across individuals. If we only assume a random effect for \\(h_m\\) we can specify the model as follows:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi}  &\\sim \\text{Normal}(\\mu_{hm}, \\sigma_{hm})\n\\end{align*}\n\\]\nwhere \\(h_{ij}\\) is the height of tree \\(i\\) at age \\(j\\), and \\(mu_hm\\) is the population mean for \\(h_m\\), \\(\\sigma\\) represents the observation error and \\(\\sigma_{hm}\\) represents the variation of \\(h_{mi}\\) across individuals. In some of the literature, the distribution of \\(h_{ij}\\) is referred to as “individual level” and the distribution of \\(h_{mi}\\) is known as “population level” (and this type of models are known as multilevel or hierarchical models).\nWe can decompose the values of \\(h_m\\) for each individual into the average and the deviation with respect to the average (\\(\\epsilon\\)). In some of the literature (where these models are know as mixed effect models) the values of \\(\\epsilon\\) are know as random effects:\n\\[\n\\begin{align*}\nh_{ij} &\\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b\\left(t_j - c \\right)}, \\sigma \\right) \\\\\nh_{mi} &= \\mu_{hm} - \\epsilon_{hmi} \\\\\n\\epsilon_{hmi}  &\\sim \\text{Normal}(0, \\sigma_{hm}) \\\\\n\\end{align*}\n\\]\nTo create the marginal likelihood we now need to integrate over the Normal distributions of \\(h_m\\). Let’s first build the function. We do it a bit different from before, because we have multiple observations for one tree (before we only had one). This also means we need to be careful with implementation: (i) the function integrate will pass a vector of eps values to prodfun and we have multiple values of t and h. Therefore, we must use sapply:\nprodfun &lt;- function(eps_hm, b, c, mu_hm, sigma, sigma_hm, t, h) {\n sapply(eps_hm, function(x) exp(dnorm(x, mean = 0, sd =  sigma_hm, log = TRUE) + # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - x, t), sigma, log = TRUE)))) # Individual level\n}\n# Evaluate for first tree using as initial values what we obtain from the stepwise approach\nprodfun(eps_hm = 0, c = 11.8, b = 0.23, mu_hm = 61,  sigma_hm = 2.3, sigma = 2,\n        t = Loblolly$age[1:6], h = Loblolly$height[1:6])\nThe integration for one tree would be:\nintegrate(f = prodfun, lower = -6*2.3, upper = 6*2.3,\n          c = 11.8, b = 0.23, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n          t = Loblolly$age[1:6], h = Loblolly$height[1:6],\n          rel.tol = 1e-12, abs.tol = 1e-12)$value\nWe can now define a function to compute the negative log marginal likelihood by solving the integral for each observation, log transforming and adding them up. Notice that now we are not applying the integration to each observation but to each group of observations that belongs to a tree:\nNLML &lt;- function(b, c, mu_hm, sigma_hm, sigma, t, h) { # data\n  # Every 6 observations is a tree\n  id = seq(1, length(t), by = 6)\n  ML = sapply(id, function(id)\n                   integrate(f = prodfun,lower = -Inf, upper = Inf,\n                        c = c, b = b, mu_hm = mu_hm,\n                        sigma = sigma, sigma_hm = sigma_hm,\n                        t = t[id:(id + 5)], h = h[id:(id + 5)],\n                   rel.tol = 1e-12, abs.tol = 1e-12)$value)\n  NLML &lt;- -sum(log(ML))\n  NLML\n}\nNLML(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2,\n     t = Loblolly$age, h = Loblolly$height)\nAnd now we can pass this big boy to bbmle. As usual, I used constrained optimization to avoid negative values or getting too close to zero (and I check later if I hit the boundary):\npar_0 = c(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2)\n\nfit &lt;- mle2(minuslogl = NLML, start = as.list(par_0),\n            data = list(t = Loblolly$age, h = Loblolly$height),\n            method = \"L-BFGS-B\", lower = c(b = 0.01, c = 1, mu_hm = 10,\n                                           # error in lower sigma_hm = 0.01, sigma = 0.01),\n            control = list(parscale = abs(par_0)))\nsummary(fit)\nAs usual we can extract the maximum likelihood estimates and the confidence intervals:\npars = coef(fit)\nci = confint(fit, method = \"quad\")\nprint(pars)\nprint(ci)\nNotice that the estimates are close to what we estimated before with the stepwise approach but not exactly the same\ncbind(means, sds)\n\n\n3.2.3 Estimating at individual level\nThe values of traits for each individual trait can be estimated in the same way that we estimated the true growth rate of trees, by maximizing the conditional likelihood for each tree separately. Let’s build the negative log conditional likelihood of the data of one tree conditional on knowing the population averages and variances:\nNCLL &lt;- function(eps_hm, b, c, mu_hm, sigma_hm, sigma, t, h) {\n   -dnorm(eps_hm, mean = 0, sd =  sigma_hm, log = T) - # Population level\n     sum(dnorm(h, logistic(b, c, mu_hm - eps_hm, t), sigma, log = T)) # Individual level\n}\nLet’s the plot NCLL for the first tree\ntree1 = subset(Loblolly, Seed == seeds[1])\nt = tree1$age\nh = tree1$height\neps &lt;- seq(-3,3, by = 0.01)*pars[\"sigma\"]\nNCLL1 &lt;- sapply(eps, function(e)\n                  NCLL(e, c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h))\nplot(eps, NCLL1)\nWe can then optimize this function to obtain the estimated deviation between the maximum height of the first tree and the average of the population and compare with the value estimated from the stepwise procedure:\neps_1 = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"],c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = tree1$age, h = tree1$height)$minimum\nhm1 = pars[\"mu_hm\"] - eps_1\ncat(\"Multivelel: \", hm1, \"Stepwise: \", traits[1,\"h_m\"])\nLet’s do the estimation for all the trees:\neps = numeric(length(seeds))\nfor(i in 1:length(seeds)) {\n  treei = subset(Loblolly, Seed == seeds[i])\n  t = treei$age\n  h = treei$height\n  eps[i] = optimize(NCLL, c(-3,3)*pars[\"sigma_hm\"], c = pars[\"c\"], b = pars[\"b\"],\n                       mu_hm = pars[\"mu_hm\"], sigma_hm = pars[\"sigma_hm\"],\n                       sigma = pars[\"sigma\"], t = t, h = h)$minimum\n}\nhm = pars[\"mu_hm\"] - eps\nplot(traits[,\"h_m\"], hm, ylim = c(52,66), xlim = c(52,66))\nabline(a = 0, b = 1)\nabline(lm(hm~I(traits[,\"h_m\"])), lty = 2)\nWe can see that the individual estimates of hm from the multilevel model and the stepwise approach as correlated (on average they are practically the same) but the multilevel model estimates higher values for the smaller trees and lower values for the higher trees. That is, the estimates for individual trees are pulled towards the population mean. This is a common effect of using multilevel models known as shrinkage. As long as your model for the variation of hm across individuals is reasonable, the estimates with shrinkages are actually better than in the stepwise approach (in the sense that they will be closer to the truth on average).\nLet’t calculate the predictions for each tree\nLoblolly = transform(Loblolly,\n                     hm2 = rep(hm, each = 6))\nLoblolly = transform(Loblolly, pred_height2 = logistic(pars[\"b\"], pars[\"c\"], hm2, age))\n\n# Compare individual predictions for stepwise and multilevel model\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height2))\nggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +\n  geom_point() +\n  geom_line(mapping = aes(y = pred_height1))\nWe can visually see that the multilevel model makes predictions that vary less across trees. Part of it is because of the shrinkage effect, but also because we ignored the variation across trees of a and b. We could write the code to evaluate those two too, but then we have to use for advanced methods of integration and it gets very tedious.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd an exercise with non-linear mixed model",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html#model-with-measurement-error-1",
    "href": "Lab10/no_solution.html#model-with-measurement-error-1",
    "title": "Lab 10",
    "section": "4.1 Model with measurement error",
    "text": "4.1 Model with measurement error\nWe can estimate this model using Stan as shown in previous tutorials. Remember that the stan model needs to be defined in its own file with the extension .stan or add your code in a Quarto document with the label stan. If you are getting lost in the code, please check the supplement on Stan (also compare with the BUGS code in the book).\ndata {\n  int N; # Number of observations/true values\n  vector[N] x_obs; # The observed growth rate\n  vector[6] hp; # Hyperparameters of the prior distributiobs\n}\n\nparameters {\n  vector&lt;lower = 0&gt;[N] x_true; # Estimated true growth rates\n  real&lt;lower = 0&gt; a; # Shape parameter of Gamma\n  real&lt;lower = 0&gt; s; # Scale parameter of Gamma\n  real&lt;lower = 0&gt; sigma; # Scale parameter of Normal\n}\n\nmodel {\n  # Priors (different from Bolker)\n  a ~ normal(hp[1], hp[2]); # Shape (95% &lt;  26), 10, 10\n  s ~ normal(hp[3], hp[4]); # rate (95% &lt; 40), 15, 15\n  sigma ~ normal(hp[5], hp[6]); # sigma (95% &lt; 40), 10, 10\n  # True growth rate\n  x_true ~ gamma(a, 1/s); # gamma(shape, rate = 1/scale)\n  # Observed growth rate with error\n  x_obs ~ normal(x_true, sigma);\n}\nI made the following changes to the code with respect to Bolker:\n\nAll parameters are given (truncated) Normal distributions as priors because it is easier to reason about prior means and standard deviations.\nI do not hard-code the hyperparameters, so that you can rerun sampling with different priors (for prior sensitivity analysis).\nThe normal distribution is parameterized with the standard deviation rather than precision.\nThere is no need to specify initial values for chains since the priors are not excessively wide (so random samples from them are good starting values).\n\nIn all cases we are using Normal prior distribution that are weakly informative meaning that we only incorporate knowledge of the scale of the parameters. They will be automatically truncated as parameters are all positive. Note that the Bayesian approach will estimate a true value for each observation in addition to the three parameters of the model (so technically we will get 1003 estimates!): The details below are as usual (see Stan supplement). Notice that I increase adapt_delta to 0.95 because this was a harder fit and I want a total of 40 thousand samples:\nlibrary(rstan)\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\nbayesian_fit &lt;- sampling(error_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = length(x_obs), x_obs = x_obs,\n                                     hp = c(10, 10, 15, 15, 10, 10)),\n                         control = list(adapt_delta = 0.95))\nThis ran in about a minute on my computer. Let’s look at the summaries for the population parameters (a, s and sigma):\nprint(bayesian_fit, pars=c(\"a\", \"s\", \"sigma\"), probs=c(.025,.5,.975))\nWe are getting perfect Rhat values (Gelman-Rubin diagnostics) and the effective sample size is in the thousands, which means that our estimations of the posterior distributions from these samples would be very accurate (in fact a bit overkill if you use my originally settings).\nBoth the median and mean are quite similar suggesting a posterior distribution that is fairly symmetric (in fact with this sample size it should be fairly normal and the priors should have a very small effect). We can compare these estimates to the maximum likelihood estimates from before:\npars\nexp(confint(fit, method = \"quad\"))\nWe are getting very similar values to the maximum likelihood approach, as expected from the large sample sizes (so again, the priors did not have much effect). If you check table 10.1 in the book (section 10.5.2) the intervals computed from the posterior distribution are very close to the confidence intervals from the likelihood profile.\nThe Bayesian procedure also gives us the estimates x_true directly. We can convert the samples from the posterior into a matrix:\nposterior &lt;- as.matrix(bayesian_fit);\ndim(posterior)\nposterior[1:4,1:4]\nWe can compare the different estimates of x for the first observation using a kernel density plot:\nplot(density(posterior[,1]), xlab = \"Growth rate\", main = \"\", las = 1)\nabline(v = c(x_obs[1], x_true[1], x_obs[1] - est_eps[1]), col = 1:3)\nabline(v = quantile(posterior[,1], prob = c(0.025,0.975)), lty = 2, col = 4)\nlegend(\"topright\", c(\"Obs\", \"True\", \"Max Lik\", \"CI 95%\"), col = c(1:3,4,4),\n       bty= \"n\", lty = c(1,1,1,2,2))\nWe can see that the Bayesian estimate of the true growth rate for the first observation has a maximum around the same value as the point estimate we obtained using maximum likelihood (and therefore in between observed and true growth rates). However, we actually have quite a bit of uncertainty in this estimate as reflected by the 95% credible interval.\nOf course, given that we know the true values of x in this simulation, we can test how often these 95% intervals include the true values (for a perfect estimation we should cover the true value 95% of the times). Let’s compute the lower and upper bounds of the intervals for each observation:\nlower_ci = quantile(posterior[,1:1000], 2, prob = 0.025)\nupper_ci = quantile(posterior[,1:1000], 2, prob = 0.975)\ninside   = (x_true &gt;= lower_ci) & (x_true &lt;= upper_ci)\ncoverage = sum(inside)/1e3\ncoverage\nThat is pretty much spot on! Getting exactly 95% is very difficult (especially with only 1000 values, we would need more for the coverage estimate to stabilize). The fact that it errors on the side of caution (i.e., simulated coverage is slightly higher than 95%) is also a good thing (we generally prefer to be pessimistic rather than optimistic). So even though our uncertainty about the true growth rate of individual trees remains high given how big the measurement error is, achieving a practically perfect coverage is the best we can do.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html#model-for-nested-data-1",
    "href": "Lab10/no_solution.html#model-for-nested-data-1",
    "title": "Lab 10",
    "section": "4.2 Model for nested data",
    "text": "4.2 Model for nested data\nLet’s implement the model in Stan. This time we are going to include all traits as varying across replicates but we will ignore correlations as that makes the model much more complex (but it would be possible). The predictions of heights is as before:\n\\[\nh_{ij} \\sim \\text{Normal} \\left(\\frac{h_{mi}{1 + e^{-b_i\\left(t_j - c_i \\right)}, \\sigma \\right)\n\\] Where the suffix \\(i\\) refers to the tree and \\(j\\) to the time point. The variation in heights across the population is assumed to follow a normal distribution (note that this can technically produce negative values but we will ignore this for simplicity):\n\\[\n\\begin{align*}\nh_{mi} &= \\mu_{hm} + \\sigma_{hm} z_{hmi} \\\\\nz_{hmi}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nNotice that we have further decomposed the Normal distribution based on the fact that \\(\\text{Normal}(\\mu, \\sigma) = \\mu + \\sigma \\text{Normal}(0,1)\\) which tends to make MCMC sampling much faster. We repeat the same decomposition for the rest of the trees:\n\\[\n\\begin{align*}\nb_{i} &= \\mu_{b} + \\sigma_{b} z_{bi} \\\\\nz_{bi}  &\\sim \\text{Normal}(0, 1) \\\\\nc_{i} &= \\mu_{c} + \\sigma_{c} z_{ci} \\\\\nz_{ci}  &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align*}\n\\]\nThe model in Stan would look at follows\ndata {\n  int N; # Number of observations\n  int Ntree; # Number of trees\n  vector[N] h; # The observed growth rate\n  vector[N] t; # The age associated to each observation\n  int tree[N]; # Id of the tree (not seed, but 1 - 14)\n  vector[14] hp; # Hyperparameters\n}\n\nparameters {\n  # Population-level parameters\n  real&lt;lower = 0&gt; mu_hm; # Mean maximum height\n  real&lt;lower = 0&gt; mu_b;  # Mean parameter b\n  real&lt;lower = 0&gt; mu_c;  # Mean parameter c\n  real&lt;lower = 0&gt; sigma_hm; # Variation in maximum height\n  real&lt;lower = 0&gt; sigma_b;  # Variation in parameter b\n  real&lt;lower = 0&gt; sigma_c;  # Variation in parameter c\n  real&lt;lower = 0&gt; sigma;    # Measurement/observation error\n  vector[Ntree] z_hm; # Standardized deviations of maximum height in population\n  vector[Ntree] z_b;  # Standardized deviations of b in population\n  vector[Ntree] z_c;  # Standardized deviations of c in population\n}\n\n# In this block we can do intermediate calculations (makes it run faster)\ntransformed parameters{\n  # Individual-level parameters\n  vector[Ntree] hm; # Maximum height of each tree\n  vector[Ntree] b; # Trait b of each tree\n  vector[Ntree] c; # Trait c of each tree\n  # Observations\n  vector[N] h_mod;\n  # Compute the trait value of each tree\n  hm = mu_hm + sigma_hm*z_hm;\n  b  = mu_b  + sigma_b*z_b;\n  c  = mu_c  + sigma_c*z_c;\n  # Compute the height of each tree at each timepoint\n  for(i in 1:N) {\n    int id = tree[i]; # Check which tree we are dealing with\n    h_mod[i] = hm[id]/(1 + exp(-b[id]*(t[i] - c[id]))); # Logistic model\n  }\n}\n\n# Here we only list the distributions since we already have done the calculations\n# of growth in the transformed parameters block\nmodel {\n  // Population-level parameters (priors)\n  mu_hm ~ normal(hp[1], hp[2]);\n  mu_b  ~ normal(hp[3], hp[4]);\n  mu_c  ~ normal(hp[5], hp[6]);\n  sigma_hm ~ normal(hp[7], hp[8]);\n  sigma_b  ~ normal(hp[9], hp[10]);\n  sigma_c  ~ normal(hp[11], hp[12]);\n  sigma ~ normal(hp[13], hp[14]);\n  // Standardized deviations from population mean (actual tree-level traits above)\n  z_hm ~ normal(0, 1);\n  z_b ~ normal(0, 1);\n  z_c ~ normal(0, 1);\n  // Observations within tree\n  h ~ normal(h_mod, sigma);\n}\nFor this example, I came up with more informative priors based on general I could find online on the growth of Loblolly pine. I put the whole reasoning to come up with the priors at the end of the lab as an example of how one could come up with reasonable priors and check that they produce sensible predictions. So please go there if you want to check it.\nOnce we have chosen some prior distributions we can sample from the posterior distribution:\nlibrary(rstan)\n\n# Parallelize\nncores = parallel::detectCores()\nnchains = min(8, ncores)\nniter = 1000 + round(40e3/nchains, 0)\noptions(mc.cores = ncores)\n\n# See section below on prior elicitation\npriors = c(65, 10,          # mean and sd of mu_hm\n           0.25, 0.25/4,    # mean and sd of mu_b\n           10, 10/4,        # mean and sd of mu_c\n           65/5, 65/10,     # mean and sd of sigma_hm\n           0.25/5, 0.25/10, # mean and sd of sigma_b\n           10/5, 10/10,     # mean and sd of sigma_c\n           1, 1)            # mean and sd of sigma\n\nbayesian_fit &lt;- sampling(logistic_growth_model,\n                         cores = nchains, chains = nchains,\n                         iter = niter, warmup = 1000,\n                         data = list(N = nrow(Loblolly),\n                                     Ntree = length(unique(Loblolly$Seed)),\n                                     h = Loblolly$height,\n                                     t = Loblolly$age,\n                                     tree = as.numeric(Loblolly$Seed),\n                                     hp = priors),\n                         control = list(adapt_delta = 0.95))\n\nprint(bayesian_fit, pars=c(\"mu_hm\", \"mu_b\", \"mu_c\",\n                           \"sigma_hm\", \"sigma_b\", \"sigma_c\", \"sigma\"),\n      probs=c(.025,.5,.975))\nCompare this to the estimates we obtained from the stepwise approach:\nmeans = colMeans(traits)\nsds   = apply(traits, 2, sd)\ncbind(means, sds)\nThe Bayesian estimate of the standard deviations are a bit smaller than the ones obtained in the stepwise approach, but they are in the right order of magnitude and the 95% credible interval includes the estimates from the stepwise approach. Let’s look at the posterior distributions of this standard deviations:\nposterior = as.matrix(bayesian_fit)\npar(mfrow = c(1,3))\nplot(density(posterior[,\"sigma_hm\"], from = 0), main = \"\", xlab = \"sigma_hm\")\nplot(density(posterior[,\"sigma_b\"], from = 0), main = \"\", xlab = \"sigma_b\")\nplot(density(posterior[,\"sigma_c\"], from = 0), main = \"\", xlab = \"sigma_c\")\nNote how the posterior distributions of standard deviations of b and c are quite small but also highly asymmetric. This is typical of constrained variables that are highly uncertain. Methods based on maximum marginal likelihood will struggle estimating optimal values sigma_b and sigma_c.\nThe samples for hm, b and c of each can be retrieved directly from posterior. For example, for the first tree:\npar(mfrow = c(1,3))\nplot(density(posterior[,\"hm[1]\"]), main = \"\", xlab = \"hm\")\nplot(density(posterior[,\"b[1]\"]), main = \"\",  xlab = \"b\")\nplot(density(posterior[,\"c[1]\"]), main = \"\",  xlab = \"c\")\nWe can also print the summary information directly from othe original fitted object:\nprint(bayesian_fit, pars=c(\"hm\"), probs=c(.025,.5,.975))\nSimilarly, we can also extract the predicted values for each observation of height (I only show it for one observation, otherwise it becomes too long):\nprint(bayesian_fit, pars=c(\"h_mod[1]\"), probs=c(.025,.5,.975))\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd Bayesian version of mixed model exercise",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab10/no_solution.html#informative-prior-elicitation",
    "href": "Lab10/no_solution.html#informative-prior-elicitation",
    "title": "Lab 10",
    "section": "4.3 Informative prior elicitation",
    "text": "4.3 Informative prior elicitation\nLet’s define some reasonable priors. Let’s start with the priors for the means of the populations. The parameter hm is the maximum height of the trees. We are dealing with a pine tree that growth is humid warm areas of North America and typical has heights of 50 to 80 feet with record heights (https://edis.ifas.ufl.edu/publication/ST478). Since it is often between 50 and 80, we can specify our prior to have a mean of 65 feet and a standard deviation of 10:\nlibrary(truncnorm)\nset.seed(1234)\nprior_mu_hm = rtruncnorm(1000, mean = 65, sd = 10, a = 0)\nsum(prior_mu_hm &gt; 50 & prior_mu_hm &lt; 80)/1e3 # 87% of heights within this range\nhist(prior_mu_hm)\nWe know that parameter b is related to growth rates (it is 4 times the maximum growth rate normalized by maximum height, see Chapter 3 of the book). Searching the literature (actually using AI with sources…) tells use that average growth rate in this species is about 2 feet/year. How does b related to this? First let’s compute the derivative of the logistic (which would tell you the growth rate). I will let the computer compute the derivative for me…\nlibrary(Deriv) # Package to compute symbolic derivatives\nderiv_logistic = Deriv(logistic, \"t\")\ncurve(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = x), 0, 35, ylab = \"Growth rate\")\nWe can now calculate the average growth rate by averaging the first 30 years of growth (after that it does not seem to grow much):\navg_growth_rate = mean(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = 1:30))\nOk, that is exactly the value we got from literature (I guess our data is very average, but you will not always be so lucky), meaning that an average growth rate of 2 feet/year corresponds to b = 0.25. We don’t have information on uncertainty, so we can take a conservative estimate of say, quarter of the mean:\nprior_mu_b = rtruncnorm(1000, mean = 0.25, sd = 0.25/4, a = 0)\nhist(prior_mu_b)\nThis would lead to maximum growth rates of 0.5 - 7 feet/year. Finally the parameter c that tells us about the age at which half of maximum height is reached. Again, checking our AI-powered Google and some of the sources within, we see that most of the growth happens in the first 20 years and after that growth rate decrease significantly as the tree matures. Since the logistic curve is symmetric, this would imply a value of c = 10 and we can assume again a standard deviation of a quarter of that:\nprior_mu_c = rtruncnorm(1000, mean = 10, sd = 10/4, a = 0)\nhist(prior_mu_c)\nTo double check that our priors make sense, we can simulate 1000 growth curves based on the parameters we just obtained\n# Simulation prior population means\nt = tree1$age\nprior_mu = sapply(t, function(x)\n                  logistic(b = prior_mu_b, c = prior_mu_c, h_m = prior_mu_hm, x))\nhead(prior_mu) # Each column is a different age\nLet’s summarise all these trajectories:\nmean_prior_mu = colMeans(prior_mu)\nmed_prior_mu = apply(prior_mu, 2, quantile, prob = 0.5)\nlower_prior_mu = apply(prior_mu, 2, quantile, prob = 0.025)\nupper_prior_mu = apply(prior_mu, 2, quantile, prob = 0.975)\nplot(t, mean_prior_mu, ylim = range(prior_mu), t = \"l\")\nlines(t, med_prior_mu, col = 2)\nlines(t, lower_prior_mu, col = 3)\nlines(t, upper_prior_mu, col = 3)\nWe can see that this covers a wide range of trajectories, but it looks very reasonable. Of course later we can test what happens if our priors are wider but if you have information do use it and come up with sensible priors rather than covering an unreasonable range “just to be sure”.\nMore complicated is coming up with reasonable priors for the variances. The reason is that the variation across trees is going to depend on the context. Is this a wild population or a more artificial plantation? If it is a wild population, is it a mixed forest or a very homogeneous system? Unfortunately, most of the published research focuses on averages, so even if you are an expert, it can be hard to justify prior distributions for the variances. Given that we have 14 replicates we can allow to be rather vague and calculate these variances as a function of the better defined means above. For example, we could expect a standard deviation of 20% around the mean for each trait and half the value for our uncertainty of what the exact value should be:\nprior_sigma_hm = rtruncnorm(1000, mean = 65/5, sd = 65/10, a = 0)\nprior_sigma_b = rtruncnorm(1000, mean = 0.25/5, sd = 0.25/10, a = 0)\nprior_sigma_c = rtruncnorm(1000, mean = 10/5, sd = 10/10, a = 0)\nWe can now simulate individual trees by combining the priors for means and standard deviations.\nprior_hm = rtruncnorm(1000, mean = prior_mu_hm, sd = prior_sigma_hm, a = 0)\nprior_b  = rtruncnorm(1000, mean = prior_mu_b, sd = prior_sigma_b, a = 0)\nprior_c  = rtruncnorm(1000, mean = prior_mu_c, sd = prior_sigma_c, a = 0)\nThe resulting population should be more variable than the averages above:\nt = 1:25\nprior_h = sapply(t, function(x)\n                  logistic(b = prior_b, c = prior_c, h_m = prior_hm, x))\nmean_prior_h = colMeans(prior_h)\nmed_prior_h = apply(prior_h, 2, quantile, prob = 0.5)\nlower_prior_h = apply(prior_h, 2, quantile, prob = 0.025)\nupper_prior_h = apply(prior_h, 2, quantile, prob = 0.975)\nplot(t, mean_prior_h, ylim = range(prior_h), t = \"l\", ylab = \"Tree height\")\nlines(t, med_prior_h, col = 2)\nlines(t, lower_prior_h, col = 3)\nlines(t, upper_prior_h, col = 3)\nThe last variable which prior we need to specify is the measurement error. The measurement errors that we should expect depend on how the measurements were taken which of course we do not know. Assuming a visual estimation using trigonometry, the reported errors seem to be 1 - 4% of the tree height. This means a prior we believe an error of 0.2 - 3 m based on the prior predicted heights. We can achieve this as follows:\nprior_sigma = rtruncnorm(1000, mean = 1, sd = 1, a = 0)\nhist(prior_sigma)",
    "crumbs": [
      "Lab 10"
    ]
  },
  {
    "objectID": "Lab2/material.html",
    "href": "Lab2/material.html",
    "title": "1 Learning outcomes",
    "section": "",
    "text": "This lab will teach you\n\nTo read in data and reshape it so it matches your needs\nTo make different types of graphs that you need for data exploration and presentation purposes.\n\nIt does so by reproducing the figures shown in Chapter 2 and more. The exercises, which will be more difficult than those in Lab 1, will typically involve variations on the figures shown in the text. You will work through reading in the different data sets and constructing the figures shown, or variants of them. It would be even better to work through reading in and making exploratory plots of your own data."
  },
  {
    "objectID": "Lab2/material.html#reading-data",
    "href": "Lab2/material.html#reading-data",
    "title": "1 Learning outcomes",
    "section": "2.1 Reading data",
    "text": "2.1 Reading data\nFind the file called seedpred.dat. It is in the right format (plain text, long format), so you can just read it in with\ndata = read.table(\"seedpred.dat\", header = TRUE)\nAdd the variable available to the data frame by combining taken and remaining (using the $ symbol):\ndata$available = data$taken + data$remaining\nPitfall #1: finding your file If R responds to your read.table() or read.csv() command with an error like\nError in file(file, \"r\") : unable to open connection In addition: Warning message: cannot open file 'myfile.csv'\nit means it can’t find your file, probably because it isn’t looking in the right place. By default, R’s working directory is the directory in which the R program starts up, which is by default something like C:/Program Files/R/rw2010/bin. (R uses / as the [operating-system-independent] separator between directories in a file path.) If you are using Rstudio you have several options:\n\nGo to ‘Session’ and click ‘Set working directory’\nUse the setwd() command to set the working directory.\nWork within an RStudio project so that the working directory is automatically set to the folder.\n\ngetwd() tells you what the current working directory is. While you could just throw everything on your desktop, it’s good to get in the habit of setting up a separate working directory for different projects, so that your data files, metadata files, R script files, and so forth, are all in the same place. Depending on how you have gotten your data files onto your system (e.g. by downloading them from the web), Windows will sometimes hide or otherwise screw up the extension of your file (e.g. adding .txt to a file called mydata.dat). R needs to know the full name of the file, including the extension.\nFor example to set a working directory:\nsetwd(\"D:/Bolker/labs/\")\nPitfall #2: checking number of fields In some cases the number of fields is not the same for every line in your data file. In that case you may get an error like:\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\nor\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 1 did not have 5 elements\nIf you need to check on the number of fields that R thinks you have on each line, use\ncount.fields(\"myfile.dat\",sep=\",\")\n(you can omit the sep=\",\" argument if you have whitespace- rather than comma delimited data). If you are checking a long data file you can try\ncf = count.fields(\"myfile.dat\",sep=\",\")\nwhich(cf!=cf[1])\nto get the line numbers with numbers of fields different from the first line. By default R will try to fill in what it sees as missing fields with NA (“not available”) values; this can be useful but can also hide errors. You can try\nmydata &lt;- read.csv(\"myfile.dat\", fill = FALSE)\nto turn off this behavior; if you don’t have any missing fields at the end of lines in your data this should work.\nIf your file is a comma separated file, you can also use read.csv. This function has set some arguments to default, e.g. the seperator is a comma in this case (sep=\",\")\nPitfall #3: List separator It may happen when you save a file in excel as .csv that the decimals are not indicated by a dot . but by a comma , and that the list separator is a semi-colon. You have two options:\n\nUse read.csv2 that will automatically use ; as separator.\nChange the settings of MS Office to use . as decimal separator (this will force Excel to save csv with ,) and you will never encounter this problem in the future."
  },
  {
    "objectID": "Lab2/material.html#checking-data",
    "href": "Lab2/material.html#checking-data",
    "title": "1 Learning outcomes",
    "section": "2.2 Checking data",
    "text": "2.2 Checking data\nR will automatically recognize the type of data of the columns that are read in, but sometimes it goes wrong. To check that all your variables have been classified correctly:\nsapply(data, class)\nThis applies the class() command, which identifies the type of a variable, to each column in your data. Alternatively,\nsummary(data)\ncan be very helplful to get a glance of the characteristics of the data.\nNon-numeric missing-variable strings (such as a star, *) will also make R misclassify. Use na.strings in your read.table() command:\nmydata &lt;- read.table(\"mydata.dat\", na.strings = \"*\")\nYou can specify more than one value with (e.g.) na.strings=c(“”,”**”,“bad”,“-9999”).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTry out head(), summary() and str() on data; make sure you understand the results."
  },
  {
    "objectID": "Lab2/material.html#reshaping-data",
    "href": "Lab2/material.html#reshaping-data",
    "title": "1 Learning outcomes",
    "section": "2.3 Reshaping data",
    "text": "2.3 Reshaping data\nReshaping a dataframe is an important part of data exploration. For example, for making field recordings it is convenient to have the measurements of different plots or transects in different columns on your sheet. This is called a wide format. However, for data analysis in R (and other statistical programs) you need to have all measurements in one column, with an additional column indicating which plot a measurement belongs to (so called long format).\nBelow we create a dataframe in long format and reshape it. Here are the commands to generate the data frame I used as an example in the text (I use LETTERS, a built-in vector of the capitalized letters of the alphabet, and runif(), which picks a specified number of random numbers from a uniform distribution between 0 and 1. The command round(x,3) rounds x to 3 digits after the decimal place.):\nloc = factor(rep(LETTERS[1:3],2))\nday = factor(rep(1:2,each=3))\nval = round(runif(6),3)\nd = data.frame(loc,day,val)\nThis data set is stored in long format. To go to wide format, we first need to (install and) load the library tidyr. In Lab 1 you learned how to install packages. You can load a package by library() or require(). Thus to use an additional package it must be (i) installed on your machine (with install.packages()) or through the menu system and (ii) loaded in your current R session (with library()):\nlibrary(tidyr)\nd2 = pivot_wider(d, names_from = day, values_from = val)\nd2\nThis means that a column will be created for every value in day, such that every row corresponds to a value of loc and the values filled-in are taken from val\nTo go back to long format, we simply write:\npivot_longer(d2, cols = -loc, names_to = \"day\", values_to = \"val\")\ncols = -loc means all columns except loc are gathered into long format, names_to = \"day\" puts the column names into a new column called day and values_to = \"value\" puts the values into a new column called val.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake a new data.frame similar to the previous data.frame d, but with an extra column month consisting of two levels. The value of month should be 1 for the first records and 2 for the second. Next, reshape to wide format and back to long format. The long format is what we commonly use in statistics."
  },
  {
    "objectID": "Lab2/material.html#advanced-data-types-time-permitting",
    "href": "Lab2/material.html#advanced-data-types-time-permitting",
    "title": "1 Learning outcomes",
    "section": "2.4 Advanced data types (Time permitting)",
    "text": "2.4 Advanced data types (Time permitting)\nWhile you can usually get away by coding data in not quite the right way - for example, coding dates as numeric values or categorical variables as strings - R tries to “do the right thing” with your data, and it is more likely to do the right thing the more it knows about how your data are structured.\nFactors instead of strings\nSometimes you wan to keep strings as \"character\" and sometimes you want to convert them to \"factor\": if your strings are unique identifiers and you want to make them factors, the default read.table will not convert them automatically. You can specify the class to be assume with each column with the argument colClasses (see below).\nFactors instead of numeric values\nSometimes you have numeric labels for data that are really categorical values - for example if your sites or species have integer codes (often data sets will have redundant information in them, e.g. both a species name and a species code number). It’s best to specify appropriate data types, so use colClasses to force R to treat the data as a factor. For example, if we wanted to make tcum a factor instead of a numeric variable:\ndata2 = read.table(\"seedpred.dat\", header = TRUE, colClasses = c(rep(\"factor\", 2), rep(\"numeric\", 3)))\nsapply(data2, class)\nn.b.: by default, R sets the order of the factor levels alphabetically. You can find out the levels and their order in a factor f with levels(f). If you want your levels ordered in some other way (e.g. site names in order along some transect), you need to specify this explicitly. Most confusingly, R will sort strings in alphabetic order too, even if they represent numbers.\nThis is OK:\nf = factor(1:10)\nlevels(f)\nHowever, if we create a factor f through:\nf = factor(as.character(1:10))\nlevels(f)\nit will put the 10-th level as second. You can fix the levels by using the levels argument in factor() to tell R explicitly what you want it to do, e.g.:\nf = factor(as.character(1:10), levels = c(1:10))\nSo the levels=1:10 argument explicitly states that there are ten levels and that the order of these levels is 1,2,3,4,5,6,7,8,9,10. The levels argument needs a vector of unique numeric values or character strings (c(1:10)).\nAdditionally, if you create a factor with levels ‘north’, ‘middle’ and ‘south’ they will be sorted by alphabet\nx = c(\"north\", \"middle\", \"south\")\nfactor(x)\nIf you want to sort them geographically instead of alphabetically you again can use the levels argument. Additionally, you can add levels that were not included in the vector itself:\nf = factor(x, levels = c(\"far_north\", \"north\", \"middle\", \"south\"))\nLikewise, if your data contain a subset of integer values in a range, but you want to make sure the levels of the factor you construct include all of the values in the range, not just the ones in your data. Use levels again:\nf = factor(c(3, 3, 5, 6, 7, 8, 10), levels = 3:10)\nFinally, you may want to get rid of levels that were included in a previous factor but are no longer relevant:\nf = factor(c(\"a\", \"b\", \"c\", \"d\"))\nf2 = f[1:2]\nlevels(f2)\nNote that a character vector is returned displaying the different levels in the factor f.\nf2 = factor(as.character(f2))\nlevels(f2)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIllustrate the effects of the levels command by plotting the factor f=factor(c(3,3,5,6,7,8,10)) as created with and without intermediate levels, i.e. with and without levels c(1:10). For an extra challenge, draw them as two side-by-side subplots. (Use par(mfrow=c(1,1)) to restore a full plot window.)\n\n\n\nDates (time permitting)\nDates and times can be tricky in R, but you can handle your dates as type Date within R rather than using Julian days\nYou can use colClasses=\"Date\" within read.table() to read in dates directly from a file, but only if your dates are in four-digit-year/month/day (e.g. 2005/08/16 or 2005-08-16) format; otherwise R will either butcher your dates or complain\nError in fromchar(x) : character string is not in a standard unambiguous format\nIf your dates are in another format in a single column, read them in as character strings (colClasses=\"character\" or using as.is) and then use as.Date(), which uses a very flexible format argument to convert character formats to dates:\nas.Date(c(\"1jan1960\", \"2jan1960\", \"31mar1960\", \"30jul1960\"),\n        format = \"%d%b%Y\")\nas.Date(c(\"02/27/92\", \"02/27/92\", \"01/14/92\", \"02/28/92\", \"02/01/92\"),\n        format = \"%m/%d/%y\")\nThe most useful format codes are %m for month number, %d for day of month, %j% for Julian date (day of year), %y% for two-digit year (dangerous for dates before 1970!) and %Y% for four-digit year; see ?strftime for many more details. If you have your dates as separate (numeric) day, month, and year columns, you actually have to squash them together into a character format. This can be done with paste(), using sep=\"/\" to specify that the values should be separated by a slash and then convert them to dates:\nyear = c(2004,2004,2004,2005)\nmonth = c(10,11,12,1)\nday = c(20,18,28,17)\ndatestr = paste(year,month,day,sep=\"/\")\ndate = as.Date(datestr)\ndate\nWhen you want to split a date to month, year and day, you can use ‘strsplit’:\ndate.c = as.character(date)\ndate.char = strsplit(date.c, \"-\" )\nWhich you subsequently can turn in to multiple colums through matrix:\ndat.mat = matrix(unlist(date.char), ncol=3, byrow=TRUE)\nAlthough R prints the dates in date out so they look like a vector of character strings, they are really dates: class(date) will give you the answer \"Date\". Note that when using the dat.mat these are characters.\nPitfall #4 quotation marks in character variables If you have character strings in your data set with apostrophes or quotation marks embedded in them, you have to get R to ignore them. I used a data set recently that contained lines like this: Western Canyon|valley|Santa Cruz|313120N|1103145WO'Donnell Canyon\nI used\ndata3 = read.table(\"datafile\", sep = \"|\", quote = \"\")\nto tell R that | was the separator between fields and that it should ignore all apostrophes/single quotations/double quotations in the data set and just read them as part of a string."
  },
  {
    "objectID": "Lab2/material.html#accessing-data",
    "href": "Lab2/material.html#accessing-data",
    "title": "1 Learning outcomes",
    "section": "2.5 Accessing data",
    "text": "2.5 Accessing data\nTo access individual variables within your data set use mydata$varname or mydata[,n] or mydata[,\"varname\"] where n is the column number and varname is the variable name you want. You can also use attach(mydata) to set things up so that you can refer to the variable names alone (e.g. varname rather than mydata$varname). However, beware: if you then modify a variable, you can end up with two copies of it: one (modified) is a local variable called varname, the other (original) is a column in the data frame called varname: it’s probably better not to attach a data set, or only until after you’ve finished cleaning and modifying it. Furthermore, if you have already created a variable called varname, R will find it before it finds the version of varname that is part of your data set. Attaching multiple copies of a data set is a good way to get confused: try to remember to detach(mydata) when you’re done.\nHere some examples to get the column with name ‘species’\ndata[,\"species\"]\ndata[,1]\ndata$species # recommended! You explictly define the dataframe and name of the column\nTo access data that are built in to R or included in an R package (which you probably won’t need to do often), say\ndata(dataset)\n(data() by itself will list all available data sets.)"
  },
  {
    "objectID": "Lab2/material.html#scatter-plot",
    "href": "Lab2/material.html#scatter-plot",
    "title": "1 Learning outcomes",
    "section": "3.1 Scatter plot",
    "text": "3.1 Scatter plot\nFrom the previous lab you may remember that the base plot function in R is plot. The function plot takes a number of arguments but at least you need to specify the x and the y. If you refer to x and y by the column name of a data.frame you need to specify the name of the data.frame as well through data.\nplot(taken ~ available,data=data)\nThe graph above may not be very useful as it does not show how many datapoints are underlying every combination of seeds taken and seeds available. The function jitter adds small noise to a numeric vector which makes that observations that have the same combination of seeds available and taken are plotted at a slightly different location.\nplot(jitter(taken)~jitter(available),xlab=\"Seeds available\",\n     ylab=\"Seeds taken\",data=data)"
  },
  {
    "objectID": "Lab2/material.html#bubble-plot",
    "href": "Lab2/material.html#bubble-plot",
    "title": "1 Learning outcomes",
    "section": "3.2 Bubble plot",
    "text": "3.2 Bubble plot\nSometimes you have multiple variables in the dataset that you want to explore simultaneously. For example, you want to superimpose information on how often certain combinations of the number seeds available versus the number of seeds taken occur. For this a bubble or jitter plot may be useful. The bubble plot is a normal plot with the size of the points representing the number of observations for a given combination of seeds available and seeds taken.\nTo get this information we first need to make a summary table which we can get through:\nt1 = table(data$available, data$taken)\nThis table needs to changed to a long format in order to be useful for the plotting command. Change the table to a long format with seeds available and seeds taken as columns. I recommend that you print every intermediate variable and try to understand what is happening.\nt2 = as.data.frame(t1) # From table to data.frame (this pivots to longer format)\nt2 = rename(t2, `Seeds available` = \"Var1\",\n            `Seeds taken`         = \"Var2\",\n            val                   = \"Freq\")\nNow we can make a plot with the size of the bubbles proportional for the number of observations. Since the columnnames have a space in the string we should use the quotes.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=val)\nThe argument cex controls the size of the points. As you see the size of the bubbles are bit too large, so we need to adjust it. In addition, we need to adjust the scaling of the axis.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\nWe could add the number of observations to the plot (plot) using the command text. The function text needs at least three arguments, the x position and the y position of the text and the text to be printed at this position. text allows vectors for each of those arguments. Therefore we can write:\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ntext(t2[,1],t2[,2],t2[,3])\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the size of the printed numbers and remove the zeros from the graph.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the color of the points and the symbols. Consult Lab 1 how to do this"
  },
  {
    "objectID": "Lab2/material.html#bar-plot-with-error-bars",
    "href": "Lab2/material.html#bar-plot-with-error-bars",
    "title": "1 Learning outcomes",
    "section": "3.3 Bar plot (with error bars)",
    "text": "3.3 Bar plot (with error bars)\nThe command to produce the barplot (Figure 3) was:\nbarplot(t(log10(t1 + 1)), beside = TRUE, legend = TRUE, xlab = \"Available\",\n ylab = \"log10(1+# observations)\")\nop = par(xpd = TRUE)\ntext(34.5, 3.05, \"Number taken\")\npar(op)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRestricting your analysis to only the observations with 5 seeds available, create a barplot showing the distribution of number of seeds taken broken down by species. Choose whether you do this with ggplot2 or through the base plot function.\n\n\n\nTo add error bars to the barplot, one need to calculate the standard error of the means. We want to plot the standard error on top of the fraction seeds taken\nFirst, compute the fraction taken:\ndata$frac_taken = data$taken/data$available\nComputing the mean fraction taken for each number of seeds available, using the tapply() function: tapply() (“table apply”, pronounced “t apply”), is an extension of the table() function; it splits a specified vector into groups according to the factors provided, then applies a function (e.g. mean() or sd()) to each group. This idea of applying a function to a set of objects is a very general, very powerful idea in data manipulation with R; in due course we’ll learn about apply() (apply a function to rows and columns of matrices), lapply() (apply a function to lists), sapply() (apply a function to lists and simplify), and mapply() (apply a function to multiple lists). For the present, though,\nmean_frac_by_avail = tapply(data$frac_taken, data$available, mean)\ncomputes the mean of frac_taken for each group defined by a different value of available. R automatically converts available into a factor temporarily for this purpose. If you want to compute the mean by group for more than one variable in a data set, use aggregate(). We can also calculate the standard errors, \\(\\frac{\\sigma}{\\sqrt(n)}\\):\nn_by_avail = table(data$available)\nse_by_avail = tapply(data$frac_taken, data$available, sd)/\n              sqrt(n_by_avail)\nFirst we plot a barplot after which we add the error bars. The error bars can be drawn by using the function arrows that have an angle between the shaft of the angle and the edge of 90 degrees. To position the error bars at the middle of the bars we need to retrieve those positions from the barplot command. This can be done through assigning a name, e.g. bara to the barplot object and using those positions as x coordinates.\nbara = barplot(mean_frac_by_avail,ylim=c(0,0.09))\n# hack: we draw arrows but with very special \"arrowheads\"\narrows(bara[,1],mean_frac_by_avail-se_by_avail,bara[,1],\n       mean_frac_by_avail+se_by_avail, length=0.05, angle=90, code=3)"
  },
  {
    "objectID": "Lab2/material.html#histogram-by-species",
    "href": "Lab2/material.html#histogram-by-species",
    "title": "1 Learning outcomes",
    "section": "3.4 Histogram by species",
    "text": "3.4 Histogram by species\nTo make a histogram we can use the function hist:\nhist(data$frac_taken)\nTo draw a histogram per species, we need to split the data into a list with each element representing a species.\ndata.s = split(data$frac_taken,list(data$species))\nNext, we use sapply to plot the histograms.\npar(mfrow=c(4,2),oma=c(0,0,0,0),mar=c(4,4,0.1,0.1))\nsapply(data.s,hist,main=\"\")\n# or equivalently\nfor (i in 1:8){\n  hist(data.s[[i]],main=\"\")\n}"
  },
  {
    "objectID": "Lab2/material.html#multi-line-plots",
    "href": "Lab2/material.html#multi-line-plots",
    "title": "1 Learning outcomes",
    "section": "3.5 Multi-line plots",
    "text": "3.5 Multi-line plots\nTo illustrate how to make a multiline plot, we will read a different dataset\ndata.meas = read.table(\"ewcitmeas.dat\", header = TRUE, na.strings = \"*\")\nyear, mon, and day were read in as integers: I’ll create a date variable as described above. For convenience, I’m also defining a variable with the city names.\ndate = as.Date(paste(data.meas$year + 1900, data.meas$mon, data.meas$day, sep = \"/\"))\ncity_names = colnames(data.meas)[4:10]\nLater on it will be useful to have the data in long format. As in previous examples we will use the function pivot_longer().\ndata.meas= cbind(data.meas,date)\ndata_long = pivot_longer(select(data.meas, -day, -mon, -year), London:Sheffield,\n                         names_to = \"city\", values_to = \"incidence\")\nWe can make a plot with multiple lines as follows. We first setup the plotting region using plot followed by the function lines to add lines to the existing plot. Note that in the plotting command type=\"l\" is used to specify that lines are drawn instead of point (type=\"p\", the default). A legend can be added by adding the function legend\ndata_long.s = split(data_long,data_long$city)\nplot(incidence ~ date,col=1,type=\"l\",\n     data=data_long[data_long$city == \"London\",])\n\nunique.city = unique(data_long$city)\nfor (i in 2:length(unique.city)){\n  lines(incidence ~ date,type=\"l\",\n        data=data_long[data_long$city == unique.city[i],],col=i)\n}\nlegend(\"topright\",legend=unique.city,col=1:8,lty=1)"
  },
  {
    "objectID": "Lab2/material.html#histogram-and-density-plots",
    "href": "Lab2/material.html#histogram-and-density-plots",
    "title": "1 Learning outcomes",
    "section": "3.6 Histogram and density plots",
    "text": "3.6 Histogram and density plots\nI’ll start by just collapsing all the incidence data into a single, logged, non-NA vector (in this case I have to use c(as.matrix(x)) to collapse the data and remove all of the data frame information):\nallvals = na.omit(c(as.matrix(data.meas[, 4:10])))\nlogvals = log10(1 + allvals)\nThe histogram (hist() command is fairly easy: the only tricks are to leave room for the other lines that will go on the plot by setting the y limits with ylim, and to specify that we want the data plotted as relative frequencies, not numbers of counts (freq=FALSE or prob=TRUE). This option tells R to divide by total number of counts and then by the bin width, so that the area covered by all the bars adds up to 1. This scaling makes the vertical scale of the histogram compatible with a density plot, or among different histograms with different number of counts or bin widths.\nhist(logvals, col = \"gray\", main = \"\", xlab = \"Log weekly incidence\",\n ylab = \"Density\", freq = FALSE, ylim = c(0, 0.6))\nAdding lines for the density is straightforward, since R knows what to do with a density object - in general, the lines command just adds lines to a plot.\nlines(density(logvals), lwd = 2)\nlines(density(logvals, adjust = 0.5), lwd = 2, lty = 2)"
  },
  {
    "objectID": "Lab2/material.html#scaling-data",
    "href": "Lab2/material.html#scaling-data",
    "title": "1 Learning outcomes",
    "section": "3.7 Scaling data",
    "text": "3.7 Scaling data\nScaling the incidence in each city by the population size, or by the mean or maximum incidence in that city, begins to get us into some non-trivial data manipulation. This process may actually be easier in the wide format. Several useful commands: * rowMeans(), rowSums(), colMeans(), and colSums() will compute the means or sums of columns efficiently. In this case we would do something like colMeans(data[,4:10]) to get the mean incidence for each city.\n\napply() is the more general command for running some command on each of a set of rows or columns. When you look at the help for apply() you’ll see an argument called MARGIN, which specifies whether you want to operate on rows (1) or columns (2). For example, apply(data[,4:10],1,mean) is the equivalent of rowMeans(data[,4:10]), but we can also easily say (e.g.) apply(data[,4:10],1,max) to get the maxima instead. Later, when you’ve gotten practice defining your own functions, you can apply any function - not just R’s built-in functions.\nscale() is a function for subtracting and dividing specified amounts out of the columns of a matrix. It is fairly flexible: scale(x,center=TRUE,scale=TRUE) will center by subtracting the means and then scale by dividing by the standard errors of the columns. Fairly obviously, setting either to FALSE will turn off that part of the operation. You can also specify a vector for either center or scale, in which case scale() will subtract or divide the columns by those vectors instead.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFigure out how to use apply() and scale() to scale all columns so they have a minimum of 0 and a maximum of 1 (hint: subtract the minimum and divide by (max-min))."
  },
  {
    "objectID": "Lab2/material.html#box-and-whisker-and-violin-plots",
    "href": "Lab2/material.html#box-and-whisker-and-violin-plots",
    "title": "1 Learning outcomes",
    "section": "3.8 Box-and-whisker and violin plots",
    "text": "3.8 Box-and-whisker and violin plots\nBy this time, box-and-whisker and violin plots will (I hope) seem easy. Since the labels get a little crowded (R is not really sophisticated about dealing with axis labels-crowded labels), I’ll use the substr() (substring) command to abbreviate each city’s name to its first three letters.\ncity_abbr = substr(city_names, 1, 3)\nThe boxplot() command uses a formula - the variable before the ~ is the data and the variable after it is the factor to use to split the data up.\nboxplot(log10(1 + incidence) ~ city, data = data_long, ylab = \"Log(incidence+1)\",\n names = city_abbr)"
  },
  {
    "objectID": "Lab2/material.html#pair-plot",
    "href": "Lab2/material.html#pair-plot",
    "title": "1 Learning outcomes",
    "section": "3.9 Pair plot",
    "text": "3.9 Pair plot\nFirst let’s make sure the earthquake data are accessible:\ndata(quakes)\nLuckily, most of the plots I drew in this section are fairly automatic. To draw a scatterplot matrix, just use pairs() (base):\npairs(quakes, pch = \".\")\n(pch=\".\" marks the data with a single-pixel point, which is handy if you are fortunate enough to have a really big data set).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGenerate three new plots based on one of the data sets in this lab, or on your own data."
  },
  {
    "objectID": "Lab2/material.html#scatter-plot-1",
    "href": "Lab2/material.html#scatter-plot-1",
    "title": "1 Learning outcomes",
    "section": "4.1 Scatter plot",
    "text": "4.1 Scatter plot\nIn ggplot a scatterplot can be made as follows\nlibrary(ggplot2)\n    ggplot(data=t2)+\n        geom_point(aes(x = `Seeds available`, y = `Seeds taken`,\n                       size = log(val)/2))+\n      ylab(\"Seeds taken\")+\n      xlab(\"Seeds available\")\n    # or alternatively labs(x=\"Seeds taken\",y=\"Seeds available\")\nIn ggplot2 you need to specify the dataset the variables come from. You do this through data=.... Next you specify the type of plot you want. For example, a point plot can be specified through geom_point. Within geom_point, you need to specify the aesthetics (aes(x...,y...)) which determines what is plotted on the axes of the point plot. For example aes(x=x,y=y) put column x on the x-axis and column y on the y-axis.\nIf data is specified in the ggplot statement, it means that all plotting commands below ggplot(...)+ use that dataframe as reference. If the size command is put inside the aes then size is dependent on some variable, if put outside the aes it requires a single value (similarly for e.g. colour, linetype and shape). New commands can be added to the first statement (ggplot) by adding a + after each line. If a column name contains a space you can refer to it by putting it between backticks: ..."
  },
  {
    "objectID": "Lab2/material.html#bar-plot-with-error-bars-1",
    "href": "Lab2/material.html#bar-plot-with-error-bars-1",
    "title": "1 Learning outcomes",
    "section": "4.2 Bar plot (with error bars)",
    "text": "4.2 Bar plot (with error bars)\nA barplot can be created with ggplot2 as follows:\nggplot(data=t2)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),\n           stat=\"identity\",position=position_dodge())\nAgain through aes we specify what is on the x and y. Through fill we subdivide the bars by the values in taken. stat=identity expresses that the values assigned to y will be used (compare stat=\"count\"). Through specifying position_dodge() bars are printed side by side instead of stacked bars (position_fill()).\nMore impressively, the ggplot package can automatically plot a barplot of a three-way cross-tabulation (one barplot per species): try\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = as.data.frame(t1.species) %&gt;%\n                rename(`Seeds available` = \"Var1\",\n                       `Seeds taken`     = \"Var2\",\n                       species           = \"Var3\",\n                       val               = \"Freq\")\n\nggplot(data=t1.species)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),stat=\"identity\",\n           position=position_dodge())+\n  facet_wrap(~species)+\n  coord_flip()\nwith facet_wrap a sequence of panels is made a specified by the variable behind the ~. The coord_flip rotates the plot."
  },
  {
    "objectID": "Lab2/material.html#histogram-by-species-1",
    "href": "Lab2/material.html#histogram-by-species-1",
    "title": "1 Learning outcomes",
    "section": "4.3 Histogram by species",
    "text": "4.3 Histogram by species\nTo make a histogram with ggplot2 you can get the frequencies less easily, so will be plot the counts\nggplot(data=data)+\ngeom_bar(aes(x=frac_taken),stat=\"count\")+\n  facet_wrap(~ species)\nggplot(data=data,aes(x=frac_taken))+\n  geom_histogram(aes(y = ..density..))+\n  facet_wrap(~species)"
  },
  {
    "objectID": "Lab2/material.html#multiple-line-plots",
    "href": "Lab2/material.html#multiple-line-plots",
    "title": "1 Learning outcomes",
    "section": "4.4 Multiple-line plots",
    "text": "4.4 Multiple-line plots\nWith ggplot2 we specify:\nggplot() +\n  geom_line(aes(x=date,y=incidence, colour=city),data=data_long)"
  },
  {
    "objectID": "Lab2/material.html#histogram-and-density-plots-1",
    "href": "Lab2/material.html#histogram-and-density-plots-1",
    "title": "1 Learning outcomes",
    "section": "4.5 Histogram and density plots",
    "text": "4.5 Histogram and density plots\nWith ggplot2 we specify:\nggplot()+\n  geom_histogram(aes(x=logvals,y=..density..))+\n  geom_density(aes(x=logvals,y=..density..))"
  },
  {
    "objectID": "Lab2/material.html#box-and-whisker-and-violin-plots-1",
    "href": "Lab2/material.html#box-and-whisker-and-violin-plots-1",
    "title": "1 Learning outcomes",
    "section": "4.6 Box-and-whisker and violin plots",
    "text": "4.6 Box-and-whisker and violin plots\nWith ggplot2 we specify:\nggplot(data=data_long)+\n  geom_boxplot((aes(x=city,y=log10(incidence+1))))\nIf I want to make a violin plot, you can specify:\nggplot(data=data_long)+\n  geom_violin((aes(x=city,y=log10(incidence+1))))"
  },
  {
    "objectID": "Lab2/solution.html",
    "href": "Lab2/solution.html",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "",
    "text": "This lab will teach you\n\nTo read in data and reshape it so it matches your needs\nTo make different types of graphs that you need for data exploration and presentation purposes.\n\nIt does so by reproducing the figures shown in Chapter 2 and more. The exercises, which will be more difficult than those in Lab 1, will typically involve variations on the figures shown in the text. You will work through reading in the different data sets and constructing the figures shown, or variants of them. It would be even better to work through reading in and making exploratory plots of your own data.",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#reading-data",
    "href": "Lab2/solution.html#reading-data",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "2.1 Reading data",
    "text": "2.1 Reading data\nFind the file called seedpred.dat. It is in the right format (plain text, long format), so you can just read it in with\ndata = read.table(\"seedpred.dat\", header = TRUE)\nAdd the variable available to the data frame by combining taken and remaining (using the $ symbol):\ndata$available = data$taken + data$remaining\nPitfall #1: finding your file If R responds to your read.table() or read.csv() command with an error like\nError in file(file, \"r\") : unable to open connection In addition: Warning message: cannot open file 'myfile.csv'\nit means it can’t find your file, probably because it isn’t looking in the right place. By default, R’s working directory is the directory in which the R program starts up, which is by default something like C:/Program Files/R/rw2010/bin. (R uses / as the [operating-system-independent] separator between directories in a file path.) If you are using Rstudio you have several options:\n\nGo to ‘Session’ and click ‘Set working directory’\nUse the setwd() command to set the working directory.\nWork within an RStudio project so that the working directory is automatically set to the folder.\n\ngetwd() tells you what the current working directory is. While you could just throw everything on your desktop, it’s good to get in the habit of setting up a separate working directory for different projects, so that your data files, metadata files, R script files, and so forth, are all in the same place. Depending on how you have gotten your data files onto your system (e.g. by downloading them from the web), Windows will sometimes hide or otherwise screw up the extension of your file (e.g. adding .txt to a file called mydata.dat). R needs to know the full name of the file, including the extension.\nFor example to set a working directory:\nsetwd(\"D:/Bolker/labs/\")\nPitfall #2: checking number of fields In some cases the number of fields is not the same for every line in your data file. In that case you may get an error like:\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\nor\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 1 did not have 5 elements\nIf you need to check on the number of fields that R thinks you have on each line, use\ncount.fields(\"myfile.dat\",sep=\",\")\n(you can omit the sep=\",\" argument if you have whitespace- rather than comma delimited data). If you are checking a long data file you can try\ncf = count.fields(\"myfile.dat\",sep=\",\")\nwhich(cf!=cf[1])\nto get the line numbers with numbers of fields different from the first line. By default R will try to fill in what it sees as missing fields with NA (“not available”) values; this can be useful but can also hide errors. You can try\nmydata &lt;- read.csv(\"myfile.dat\", fill = FALSE)\nto turn off this behavior; if you don’t have any missing fields at the end of lines in your data this should work.\nIf your file is a comma separated file, you can also use read.csv. This function has set some arguments to default, e.g. the seperator is a comma in this case (sep=\",\")\nPitfall #3: List separator It may happen when you save a file in excel as .csv that the decimals are not indicated by a dot . but by a comma , and that the list separator is a semi-colon. You have two options:\n\nUse read.csv2 that will automatically use ; as separator.\nChange the settings of MS Office to use . as decimal separator (this will force Excel to save csv with ,) and you will never encounter this problem in the future.",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#checking-data",
    "href": "Lab2/solution.html#checking-data",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "2.2 Checking data",
    "text": "2.2 Checking data\nR will automatically recognize the type of data of the columns that are read in, but sometimes it goes wrong. To check that all your variables have been classified correctly:\nsapply(data, class)\nThis applies the class() command, which identifies the type of a variable, to each column in your data. Alternatively,\nsummary(data)\ncan be very helplful to get a glance of the characteristics of the data.\nNon-numeric missing-variable strings (such as a star, *) will also make R misclassify. Use na.strings in your read.table() command:\nmydata &lt;- read.table(\"mydata.dat\", na.strings = \"*\")\nYou can specify more than one value with (e.g.) na.strings=c(“”,”**”,“bad”,“-9999”).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTry out head(), summary() and str() on data; make sure you understand the results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nhead() gives the first five rows of your data and are used to inspect whether the read-in procedure went ok. For example, you can see whether the header is really a header or whether is was read-in as the first data row.\nsummary() gives you a summary view of your data.frame, reporting the mean and quantiles of your data, and numbers of NAs.\nstr() shows you the structure and data types that are in the data.frame or other R objects. str() can be convenient to check if the data types are as you want to have them, or to extract information from complex R objects such as a model object fitted with lm.",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#reshaping-data",
    "href": "Lab2/solution.html#reshaping-data",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "2.3 Reshaping data",
    "text": "2.3 Reshaping data\nReshaping a dataframe is an important part of data exploration. For example, for making field recordings it is convenient to have the measurements of different plots or transects in different columns on your sheet. This is called a wide format. However, for data analysis in R (and other statistical programs) you need to have all measurements in one column, with an additional column indicating which plot a measurement belongs to (so called long format).\nBelow we create a dataframe in long format and reshape it. Here are the commands to generate the data frame I used as an example in the text (I use LETTERS, a built-in vector of the capitalized letters of the alphabet, and runif(), which picks a specified number of random numbers from a uniform distribution between 0 and 1. The command round(x,3) rounds x to 3 digits after the decimal place.):\nloc = factor(rep(LETTERS[1:3],2))\nday = factor(rep(1:2,each=3))\nval = round(runif(6),3)\nd = data.frame(loc,day,val)\nThis data set is stored in long format. To go to wide format, we first need to (install and) load the library tidyr. In Lab 1 you learned how to install packages. You can load a package by library() or require(). Thus to use an additional package it must be (i) installed on your machine (with install.packages()) or through the menu system and (ii) loaded in your current R session (with library()):\nlibrary(tidyr)\nd2 = pivot_wider(d, names_from = day, values_from = val)\nd2\nThis means that a column will be created for every value in day, such that every row corresponds to a value of loc and the values filled-in are taken from val\nTo go back to long format, we simply write:\npivot_longer(d2, cols = -loc, names_to = \"day\", values_to = \"val\")\ncols = -loc means all columns except loc are gathered into long format, names_to = \"day\" puts the column names into a new column called day and values_to = \"value\" puts the values into a new column called val.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake a new data.frame similar to the previous data.frame d, but with an extra column month consisting of two levels. The value of month should be 1 for the first records and 2 for the second. Next, reshape to wide format and back to long format. The long format is what we commonly use in statistics.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe use pivot_wider and pivot_longer as before:\nloc = factor(rep(LETTERS[1:3], 4))\nmonth = factor(rep(1:2, each = 6))\nday = factor(rep(1:2, each = 3))\nval = round(runif(12), 3)\nd1 = data.frame(loc, month, day, val)\nd3 = pivot_wider(d1, names_from = c(month, day), values_from = val)\npivot_longer(d3, cols = -loc, names_to = \"month_day\", values_to = \"val\")\nNote that in the pivoting, we have ended up merging month and day. Indeed in a next step we would need to split them. You can check the package tidyr for all available commangs to transform data from and to wider and longer formats: https://tidyr.tidyverse.org/",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#advanced-data-types-time-permitting",
    "href": "Lab2/solution.html#advanced-data-types-time-permitting",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "2.4 Advanced data types (Time permitting)",
    "text": "2.4 Advanced data types (Time permitting)\nWhile you can usually get away by coding data in not quite the right way - for example, coding dates as numeric values or categorical variables as strings - R tries to “do the right thing” with your data, and it is more likely to do the right thing the more it knows about how your data are structured.\nFactors instead of strings\nSometimes you wan to keep strings as \"character\" and sometimes you want to convert them to \"factor\": if your strings are unique identifiers and you want to make them factors, the default read.table will not convert them automatically. You can specify the class to be assume with each column with the argument colClasses (see below).\nFactors instead of numeric values\nSometimes you have numeric labels for data that are really categorical values - for example if your sites or species have integer codes (often data sets will have redundant information in them, e.g. both a species name and a species code number). It’s best to specify appropriate data types, so use colClasses to force R to treat the data as a factor. For example, if we wanted to make tcum a factor instead of a numeric variable:\ndata2 = read.table(\"seedpred.dat\", header = TRUE, colClasses = c(rep(\"factor\", 2), rep(\"numeric\", 3)))\nsapply(data2, class)\nn.b.: by default, R sets the order of the factor levels alphabetically. You can find out the levels and their order in a factor f with levels(f). If you want your levels ordered in some other way (e.g. site names in order along some transect), you need to specify this explicitly. Most confusingly, R will sort strings in alphabetic order too, even if they represent numbers.\nThis is OK:\nf = factor(1:10)\nlevels(f)\nHowever, if we create a factor f through:\nf = factor(as.character(1:10))\nlevels(f)\nit will put the 10-th level as second. You can fix the levels by using the levels argument in factor() to tell R explicitly what you want it to do, e.g.:\nf = factor(as.character(1:10), levels = c(1:10))\nSo the levels=1:10 argument explicitly states that there are ten levels and that the order of these levels is 1,2,3,4,5,6,7,8,9,10. The levels argument needs a vector of unique numeric values or character strings (c(1:10)).\nAdditionally, if you create a factor with levels ‘north’, ‘middle’ and ‘south’ they will be sorted by alphabet\nx = c(\"north\", \"middle\", \"south\")\nfactor(x)\nIf you want to sort them geographically instead of alphabetically you again can use the levels argument. Additionally, you can add levels that were not included in the vector itself:\nf = factor(x, levels = c(\"far_north\", \"north\", \"middle\", \"south\"))\nLikewise, if your data contain a subset of integer values in a range, but you want to make sure the levels of the factor you construct include all of the values in the range, not just the ones in your data. Use levels again:\nf = factor(c(3, 3, 5, 6, 7, 8, 10), levels = 3:10)\nFinally, you may want to get rid of levels that were included in a previous factor but are no longer relevant:\nf = factor(c(\"a\", \"b\", \"c\", \"d\"))\nf2 = f[1:2]\nlevels(f2)\nNote that a character vector is returned displaying the different levels in the factor f.\nf2 = factor(as.character(f2))\nlevels(f2)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIllustrate the effects of the levels command by plotting the factor f=factor(c(3,3,5,6,7,8,10)) as created with and without intermediate levels, i.e. with and without levels c(1:10). For an extra challenge, draw them as two side-by-side subplots. (Use par(mfrow=c(1,1)) to restore a full plot window.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nf=factor(c(3,3,5,6,7,8,10))\nplot(f)\nf1=factor(c(3,3,5,6,7,8,10),levels=c(3:10))\nplot(f1)\n\n\n\n\n\n\nDates (time permitting)\nDates and times can be tricky in R, but you can handle your dates as type Date within R rather than using Julian days\nYou can use colClasses=\"Date\" within read.table() to read in dates directly from a file, but only if your dates are in four-digit-year/month/day (e.g. 2005/08/16 or 2005-08-16) format; otherwise R will either butcher your dates or complain\nError in fromchar(x) : character string is not in a standard unambiguous format\nIf your dates are in another format in a single column, read them in as character strings (colClasses=\"character\" or using as.is) and then use as.Date(), which uses a very flexible format argument to convert character formats to dates:\nas.Date(c(\"1jan1960\", \"2jan1960\", \"31mar1960\", \"30jul1960\"),\n        format = \"%d%b%Y\")\nas.Date(c(\"02/27/92\", \"02/27/92\", \"01/14/92\", \"02/28/92\", \"02/01/92\"),\n        format = \"%m/%d/%y\")\nThe most useful format codes are %m for month number, %d for day of month, %j% for Julian date (day of year), %y% for two-digit year (dangerous for dates before 1970!) and %Y% for four-digit year; see ?strftime for many more details. If you have your dates as separate (numeric) day, month, and year columns, you actually have to squash them together into a character format. This can be done with paste(), using sep=\"/\" to specify that the values should be separated by a slash and then convert them to dates:\nyear = c(2004,2004,2004,2005)\nmonth = c(10,11,12,1)\nday = c(20,18,28,17)\ndatestr = paste(year,month,day,sep=\"/\")\ndate = as.Date(datestr)\ndate\nWhen you want to split a date to month, year and day, you can use ‘strsplit’:\ndate.c = as.character(date)\ndate.char = strsplit(date.c, \"-\" )\nWhich you subsequently can turn in to multiple colums through matrix:\ndat.mat = matrix(unlist(date.char), ncol=3, byrow=TRUE)\nAlthough R prints the dates in date out so they look like a vector of character strings, they are really dates: class(date) will give you the answer \"Date\". Note that when using the dat.mat these are characters.\nPitfall #4 quotation marks in character variables If you have character strings in your data set with apostrophes or quotation marks embedded in them, you have to get R to ignore them. I used a data set recently that contained lines like this: Western Canyon|valley|Santa Cruz|313120N|1103145WO'Donnell Canyon\nI used\ndata3 = read.table(\"datafile\", sep = \"|\", quote = \"\")\nto tell R that | was the separator between fields and that it should ignore all apostrophes/single quotations/double quotations in the data set and just read them as part of a string.",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#accessing-data",
    "href": "Lab2/solution.html#accessing-data",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "2.5 Accessing data",
    "text": "2.5 Accessing data\nTo access individual variables within your data set use mydata$varname or mydata[,n] or mydata[,\"varname\"] where n is the column number and varname is the variable name you want. You can also use attach(mydata) to set things up so that you can refer to the variable names alone (e.g. varname rather than mydata$varname). However, beware: if you then modify a variable, you can end up with two copies of it: one (modified) is a local variable called varname, the other (original) is a column in the data frame called varname: it’s probably better not to attach a data set, or only until after you’ve finished cleaning and modifying it. Furthermore, if you have already created a variable called varname, R will find it before it finds the version of varname that is part of your data set. Attaching multiple copies of a data set is a good way to get confused: try to remember to detach(mydata) when you’re done.\nHere some examples to get the column with name ‘species’\ndata[,\"species\"]\ndata[,1]\ndata$species # recommended! You explictly define the dataframe and name of the column\nTo access data that are built in to R or included in an R package (which you probably won’t need to do often), say\ndata(dataset)\n(data() by itself will list all available data sets.)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#scatter-plot",
    "href": "Lab2/solution.html#scatter-plot",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.1 Scatter plot",
    "text": "3.1 Scatter plot\nFrom the previous lab you may remember that the base plot function in R is plot. The function plot takes a number of arguments but at least you need to specify the x and the y. If you refer to x and y by the column name of a data.frame you need to specify the name of the data.frame as well through data.\nplot(taken ~ available,data=data)\nThe graph above may not be very useful as it does not show how many datapoints are underlying every combination of seeds taken and seeds available. The function jitter adds small noise to a numeric vector which makes that observations that have the same combination of seeds available and taken are plotted at a slightly different location.\nplot(jitter(taken)~jitter(available),xlab=\"Seeds available\",\n     ylab=\"Seeds taken\",data=data)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#bubble-plot",
    "href": "Lab2/solution.html#bubble-plot",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.2 Bubble plot",
    "text": "3.2 Bubble plot\nSometimes you have multiple variables in the dataset that you want to explore simultaneously. For example, you want to superimpose information on how often certain combinations of the number seeds available versus the number of seeds taken occur. For this a bubble or jitter plot may be useful. The bubble plot is a normal plot with the size of the points representing the number of observations for a given combination of seeds available and seeds taken.\nTo get this information we first need to make a summary table which we can get through:\nt1 = table(data$available, data$taken)\nThis table needs to changed to a long format in order to be useful for the plotting command. Change the table to a long format with seeds available and seeds taken as columns. I recommend that you print every intermediate variable and try to understand what is happening.\nt2 = as.data.frame(t1) # From table to data.frame (this pivots to longer format)\nt2 = rename(t2, `Seeds available` = \"Var1\",\n            `Seeds taken`         = \"Var2\",\n            val                   = \"Freq\")\nNow we can make a plot with the size of the bubbles proportional for the number of observations. Since the columnnames have a space in the string we should use the quotes.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=val)\nThe argument cex controls the size of the points. As you see the size of the bubbles are bit too large, so we need to adjust it. In addition, we need to adjust the scaling of the axis.\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\nWe could add the number of observations to the plot (plot) using the command text. The function text needs at least three arguments, the x position and the y position of the text and the text to be printed at this position. text allows vectors for each of those arguments. Therefore we can write:\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ntext(t2[,1],t2[,2],t2[,3])\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the size of the printed numbers and remove the zeros from the graph.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis removes zeroes and puts it in a new dataframe:\nt3 &lt;- t2[t2$val&gt;0,]\nPlotting this new dataframe:\nplot(`Seeds taken` ~ `Seeds available`,data=t3,cex=log(val)*2,\n    xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ncex changes the text size:\ntext(t3[,1],t3[,2],t3[,3],cex=0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the color of the points and the symbols. Consult Lab 1 how to do this\n\n\n\n\n\n\nSolution\n\n\n\n\n\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5),col=\"blue\",pch=2)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#bar-plot-with-error-bars",
    "href": "Lab2/solution.html#bar-plot-with-error-bars",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.3 Bar plot (with error bars)",
    "text": "3.3 Bar plot (with error bars)\nThe command to produce the barplot (Figure 3) was:\nbarplot(t(log10(t1 + 1)), beside = TRUE, legend = TRUE, xlab = \"Available\",\n ylab = \"log10(1+# observations)\")\nop = par(xpd = TRUE)\ntext(34.5, 3.05, \"Number taken\")\npar(op)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRestricting your analysis to only the observations with 5 seeds available, create a barplot showing the distribution of number of seeds taken broken down by species. Choose whether you do this with ggplot2 or through the base plot function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = as.data.frame(t1.species) %&gt;%\n               rename(`Seeds available` = \"Var1\", `Seeds taken` = \"Var2\",\n                      species = \"Var3\", val = \"Freq\")\nggplot(data=t1.species[t1.species$`Seeds available` == 5,]) +\n  geom_bar(aes(x=`Seeds taken`,y=val,),stat=\"identity\",\n           position=position_dodge())+\n  facet_wrap(~species)+\n  coord_flip()\n\n\n\n\n\n\nTo add error bars to the barplot, one need to calculate the standard error of the means. We want to plot the standard error on top of the fraction seeds taken\nFirst, compute the fraction taken:\ndata$frac_taken = data$taken/data$available\nComputing the mean fraction taken for each number of seeds available, using the tapply() function: tapply() (“table apply”, pronounced “t apply”), is an extension of the table() function; it splits a specified vector into groups according to the factors provided, then applies a function (e.g. mean() or sd()) to each group. This idea of applying a function to a set of objects is a very general, very powerful idea in data manipulation with R; in due course we’ll learn about apply() (apply a function to rows and columns of matrices), lapply() (apply a function to lists), sapply() (apply a function to lists and simplify), and mapply() (apply a function to multiple lists). For the present, though,\nmean_frac_by_avail = tapply(data$frac_taken, data$available, mean)\ncomputes the mean of frac_taken for each group defined by a different value of available. R automatically converts available into a factor temporarily for this purpose. If you want to compute the mean by group for more than one variable in a data set, use aggregate(). We can also calculate the standard errors, \\(\\frac{\\sigma}{\\sqrt(n)}\\):\nn_by_avail = table(data$available)\nse_by_avail = tapply(data$frac_taken, data$available, sd)/\n              sqrt(n_by_avail)\nFirst we plot a barplot after which we add the error bars. The error bars can be drawn by using the function arrows that have an angle between the shaft of the angle and the edge of 90 degrees. To position the error bars at the middle of the bars we need to retrieve those positions from the barplot command. This can be done through assigning a name, e.g. bara to the barplot object and using those positions as x coordinates.\nbara = barplot(mean_frac_by_avail,ylim=c(0,0.09))\n# hack: we draw arrows but with very special \"arrowheads\"\narrows(bara[,1],mean_frac_by_avail-se_by_avail,bara[,1],\n       mean_frac_by_avail+se_by_avail, length=0.05, angle=90, code=3)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#histogram-by-species",
    "href": "Lab2/solution.html#histogram-by-species",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.4 Histogram by species",
    "text": "3.4 Histogram by species\nTo make a histogram we can use the function hist:\nhist(data$frac_taken)\nTo draw a histogram per species, we need to split the data into a list with each element representing a species.\ndata.s = split(data$frac_taken,list(data$species))\nNext, we use sapply to plot the histograms.\npar(mfrow=c(4,2),oma=c(0,0,0,0),mar=c(4,4,0.1,0.1))\nsapply(data.s,hist,main=\"\")\n# or equivalently\nfor (i in 1:8){\n  hist(data.s[[i]],main=\"\")\n}",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#multi-line-plots",
    "href": "Lab2/solution.html#multi-line-plots",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.5 Multi-line plots",
    "text": "3.5 Multi-line plots\nTo illustrate how to make a multiline plot, we will read a different dataset\ndata.meas = read.table(\"ewcitmeas.dat\", header = TRUE, na.strings = \"*\")\nyear, mon, and day were read in as integers: I’ll create a date variable as described above. For convenience, I’m also defining a variable with the city names.\ndate = as.Date(paste(data.meas$year + 1900, data.meas$mon, data.meas$day, sep = \"/\"))\ncity_names = colnames(data.meas)[4:10]\nLater on it will be useful to have the data in long format. As in previous examples we will use the function pivot_longer().\ndata.meas= cbind(data.meas,date)\ndata_long = pivot_longer(select(data.meas, -day, -mon, -year), London:Sheffield,\n                         names_to = \"city\", values_to = \"incidence\")\nWe can make a plot with multiple lines as follows. We first setup the plotting region using plot followed by the function lines to add lines to the existing plot. Note that in the plotting command type=\"l\" is used to specify that lines are drawn instead of point (type=\"p\", the default). A legend can be added by adding the function legend\ndata_long.s = split(data_long,data_long$city)\nplot(incidence ~ date,col=1,type=\"l\",\n     data=data_long[data_long$city == \"London\",])\n\nunique.city = unique(data_long$city)\nfor (i in 2:length(unique.city)){\n  lines(incidence ~ date,type=\"l\",\n        data=data_long[data_long$city == unique.city[i],],col=i)\n}\nlegend(\"topright\",legend=unique.city,col=1:8,lty=1)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#histogram-and-density-plots",
    "href": "Lab2/solution.html#histogram-and-density-plots",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.6 Histogram and density plots",
    "text": "3.6 Histogram and density plots\nI’ll start by just collapsing all the incidence data into a single, logged, non-NA vector (in this case I have to use c(as.matrix(x)) to collapse the data and remove all of the data frame information):\nallvals = na.omit(c(as.matrix(data.meas[, 4:10])))\nlogvals = log10(1 + allvals)\nThe histogram (hist() command is fairly easy: the only tricks are to leave room for the other lines that will go on the plot by setting the y limits with ylim, and to specify that we want the data plotted as relative frequencies, not numbers of counts (freq=FALSE or prob=TRUE). This option tells R to divide by total number of counts and then by the bin width, so that the area covered by all the bars adds up to 1. This scaling makes the vertical scale of the histogram compatible with a density plot, or among different histograms with different number of counts or bin widths.\nhist(logvals, col = \"gray\", main = \"\", xlab = \"Log weekly incidence\",\n ylab = \"Density\", freq = FALSE, ylim = c(0, 0.6))\nAdding lines for the density is straightforward, since R knows what to do with a density object - in general, the lines command just adds lines to a plot.\nlines(density(logvals), lwd = 2)\nlines(density(logvals, adjust = 0.5), lwd = 2, lty = 2)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#scaling-data",
    "href": "Lab2/solution.html#scaling-data",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.7 Scaling data",
    "text": "3.7 Scaling data\nScaling the incidence in each city by the population size, or by the mean or maximum incidence in that city, begins to get us into some non-trivial data manipulation. This process may actually be easier in the wide format. Several useful commands: * rowMeans(), rowSums(), colMeans(), and colSums() will compute the means or sums of columns efficiently. In this case we would do something like colMeans(data[,4:10]) to get the mean incidence for each city.\n\napply() is the more general command for running some command on each of a set of rows or columns. When you look at the help for apply() you’ll see an argument called MARGIN, which specifies whether you want to operate on rows (1) or columns (2). For example, apply(data[,4:10],1,mean) is the equivalent of rowMeans(data[,4:10]), but we can also easily say (e.g.) apply(data[,4:10],1,max) to get the maxima instead. Later, when you’ve gotten practice defining your own functions, you can apply any function - not just R’s built-in functions.\nscale() is a function for subtracting and dividing specified amounts out of the columns of a matrix. It is fairly flexible: scale(x,center=TRUE,scale=TRUE) will center by subtracting the means and then scale by dividing by the standard errors of the columns. Fairly obviously, setting either to FALSE will turn off that part of the operation. You can also specify a vector for either center or scale, in which case scale() will subtract or divide the columns by those vectors instead.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFigure out how to use apply() and scale() to scale all columns so they have a minimum of 0 and a maximum of 1 (hint: subtract the minimum and divide by (max-min)).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis solution can be made more elegant once we have learned to program our own functions\nincidence = data.meas[,4:10]\nincidence.min= apply(incidence,2,min,na.rm=T)\nincidence.max= apply(incidence,2,max,na.rm=T)\nrange = incidence.max-incidence.min\ndata.scaled = scale(incidence, center=incidence.min,scale=range)\nsummary(data.scaled)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#box-and-whisker-and-violin-plots",
    "href": "Lab2/solution.html#box-and-whisker-and-violin-plots",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.8 Box-and-whisker and violin plots",
    "text": "3.8 Box-and-whisker and violin plots\nBy this time, box-and-whisker and violin plots will (I hope) seem easy. Since the labels get a little crowded (R is not really sophisticated about dealing with axis labels-crowded labels), I’ll use the substr() (substring) command to abbreviate each city’s name to its first three letters.\ncity_abbr = substr(city_names, 1, 3)\nThe boxplot() command uses a formula - the variable before the ~ is the data and the variable after it is the factor to use to split the data up.\nboxplot(log10(1 + incidence) ~ city, data = data_long, ylab = \"Log(incidence+1)\",\n names = city_abbr)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#pair-plot",
    "href": "Lab2/solution.html#pair-plot",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "3.9 Pair plot",
    "text": "3.9 Pair plot\nFirst let’s make sure the earthquake data are accessible:\ndata(quakes)\nLuckily, most of the plots I drew in this section are fairly automatic. To draw a scatterplot matrix, just use pairs() (base):\npairs(quakes, pch = \".\")\n(pch=\".\" marks the data with a single-pixel point, which is handy if you are fortunate enough to have a really big data set).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGenerate three new plots based on one of the data sets in this lab, or on your own data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is up to your creativity :)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#scatter-plot-1",
    "href": "Lab2/solution.html#scatter-plot-1",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.1 Scatter plot",
    "text": "4.1 Scatter plot\nIn ggplot a scatterplot can be made as follows\nlibrary(ggplot2)\n    ggplot(data=t2)+\n        geom_point(aes(x = `Seeds available`, y = `Seeds taken`,\n                       size = log(val)/2))+\n      ylab(\"Seeds taken\")+\n      xlab(\"Seeds available\")\n    # or alternatively labs(x=\"Seeds taken\",y=\"Seeds available\")\nIn ggplot2 you need to specify the dataset the variables come from. You do this through data=.... Next you specify the type of plot you want. For example, a point plot can be specified through geom_point. Within geom_point, you need to specify the aesthetics (aes(x...,y...)) which determines what is plotted on the axes of the point plot. For example aes(x=x,y=y) put column x on the x-axis and column y on the y-axis.\nIf data is specified in the ggplot statement, it means that all plotting commands below ggplot(...)+ use that dataframe as reference. If the size command is put inside the aes then size is dependent on some variable, if put outside the aes it requires a single value (similarly for e.g. colour, linetype and shape). New commands can be added to the first statement (ggplot) by adding a + after each line. If a column name contains a space you can refer to it by putting it between backticks: ...",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#bar-plot-with-error-bars-1",
    "href": "Lab2/solution.html#bar-plot-with-error-bars-1",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.2 Bar plot (with error bars)",
    "text": "4.2 Bar plot (with error bars)\nA barplot can be created with ggplot2 as follows:\nggplot(data=t2)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),\n           stat=\"identity\",position=position_dodge())\nAgain through aes we specify what is on the x and y. Through fill we subdivide the bars by the values in taken. stat=identity expresses that the values assigned to y will be used (compare stat=\"count\"). Through specifying position_dodge() bars are printed side by side instead of stacked bars (position_fill()).\nMore impressively, the ggplot package can automatically plot a barplot of a three-way cross-tabulation (one barplot per species): try\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = as.data.frame(t1.species) %&gt;%\n                rename(`Seeds available` = \"Var1\",\n                       `Seeds taken`     = \"Var2\",\n                       species           = \"Var3\",\n                       val               = \"Freq\")\n\nggplot(data=t1.species)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),stat=\"identity\",\n           position=position_dodge())+\n  facet_wrap(~species)+\n  coord_flip()\nwith facet_wrap a sequence of panels is made a specified by the variable behind the ~. The coord_flip rotates the plot.",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#histogram-by-species-1",
    "href": "Lab2/solution.html#histogram-by-species-1",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.3 Histogram by species",
    "text": "4.3 Histogram by species\nTo make a histogram with ggplot2 you can get the frequencies less easily, so will be plot the counts\nggplot(data=data)+\ngeom_bar(aes(x=frac_taken),stat=\"count\")+\n  facet_wrap(~ species)\nggplot(data=data,aes(x=frac_taken))+\n  geom_histogram(aes(y = ..density..))+\n  facet_wrap(~species)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#multiple-line-plots",
    "href": "Lab2/solution.html#multiple-line-plots",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.4 Multiple-line plots",
    "text": "4.4 Multiple-line plots\nWith ggplot2 we specify:\nggplot() +\n  geom_line(aes(x=date,y=incidence, colour=city),data=data_long)",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#histogram-and-density-plots-1",
    "href": "Lab2/solution.html#histogram-and-density-plots-1",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.5 Histogram and density plots",
    "text": "4.5 Histogram and density plots\nWith ggplot2 we specify:\nggplot()+\n  geom_histogram(aes(x=logvals,y=..density..))+\n  geom_density(aes(x=logvals,y=..density..))",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab2/solution.html#box-and-whisker-and-violin-plots-1",
    "href": "Lab2/solution.html#box-and-whisker-and-violin-plots-1",
    "title": "Lab 2: Exploratory data analysis and visualization (solutions)",
    "section": "4.6 Box-and-whisker and violin plots",
    "text": "4.6 Box-and-whisker and violin plots\nWith ggplot2 we specify:\nggplot(data=data_long)+\n  geom_boxplot((aes(x=city,y=log10(incidence+1))))\nIf I want to make a violin plot, you can specify:\nggplot(data=data_long)+\n  geom_violin((aes(x=city,y=log10(incidence+1))))",
    "crumbs": [
      "Solutions",
      "Lab 2 (solutions)"
    ]
  },
  {
    "objectID": "Lab3/no_solution.html",
    "href": "Lab3/no_solution.html",
    "title": "Lab 3: Analyzing functions",
    "section": "",
    "text": "In this lab you will learn to analyse mathematical functions. This is an important step in ecological modelling. Next, we proceed with analysing and programming these functions in R. To do so, you will need more advanced programming skills such as for-loops, if-else statements and functions.",
    "crumbs": [
      "Lab 3"
    ]
  },
  {
    "objectID": "Lab3/no_solution.html#plotting-curves",
    "href": "Lab3/no_solution.html#plotting-curves",
    "title": "Lab 3: Analyzing functions",
    "section": "3.1 Plotting curves",
    "text": "3.1 Plotting curves\nHere are the R commands used to generate Figure 3.2 in the book (p 74). They just use curve(), with add=FALSE (the default, which draws a new plot) and add=TRUE (adds the curve to an existing plot), particular values of from and to, and various graphical parameters (ylim, ylab, lty).\ncurve(2*exp(-x/2),from=0,to=7,ylim=c(0,2),ylab=\"\")\ncurve(2*exp(-x),add=TRUE,lty=4)\ncurve(x*exp(-x/2),add=TRUE,lty=2)\ncurve(2*x*exp(-x/2),add=TRUE,lty=3)\ntext(0.4,1.9,expression(paste(\"exponential: \",2*e^(-x/2))),adj=0)\ntext(4,.5,expression(paste(\"Ricker: \",x*e^(-x/2))))\ntext(4,1,expression(paste(\"Ricker: \",2*x*e^(-x/2))),adj=0)\ntext(2.8,0,expression(paste(\"exponential: \",2*e^(-x))))\nThe only new thing in this figure is the use of expression() to add a mathematical formula to an R graphic. text(x,y,\"x^2\") puts x^2 on the graph at position \\((x,y)\\); text(x,y,expression(x^2)) (no quotation marks) puts \\(x^2\\) on the graph. See ?plotmath or ?demo(plotmath) for (much) more information.\nAn alternate way of plotting the exponential parts of this curve:\nxvec = seq(0,7,length=100)\nexp1_vec = 2*exp(-xvec/2)\nexp2_vec = 2*exp(-xvec)\nplot(xvec,exp1_vec,type=\"l\",ylim=c(0,2),ylab=\"\")\nlines(xvec,exp2_vec,lty=4)\nFinally, if you have a more complicated function you could use sapply() to call this function along with appropriate parameter values. you could say:\nexpfun = function(x,a=1,b=1) {\n   a*exp(-b*x)\n }\nexp1_vec = sapply(xvec,expfun,a=2,b=1/2)\nexp2_vec = sapply(xvec,expfun,a=2,b=1)\nThe advantage of curve() is that you don’t have to define any vectors: the advantage of doing things the other way arises when you want to keep the vectors around to do other calculations with them.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nConstruct a curve that has a maximum at (\\(x=5\\), \\(y=1\\)). Write the equation, draw the curve in R, and explain how you got there.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChoose appropriate parameter values through eyeballing so that the chosen curves more or less match the data. Eyeballing means that you knowledge on the effect of parameter values on the shape of the function. Later we will use likelihood methods to estimate the parameter values. Plot the curves on top of the data using curve.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTime permitting repeat the previous three questions for three of the shapes datasets.",
    "crumbs": [
      "Lab 3"
    ]
  },
  {
    "objectID": "Lab3/no_solution.html#for-loops",
    "href": "Lab3/no_solution.html#for-loops",
    "title": "Lab 3: Analyzing functions",
    "section": "4.1 For loops",
    "text": "4.1 For loops\nWhen programming your data analysis, you often need to iterate over multiple elements of a collection. These elements could be rows of a data.frame, datasets inside a list, numbers inside a vector, etc. The iteration usually means that you apply the same code over each element of the collection and you don’t want to “copy-paste” the code for each element. Iterating over a collection is called a “for loop” in programming. A for loop consists of three components:\n\nA collection over which you want to iterate.\nA variable that keeps track of where you are in the collection in each iteration.\nThe body of the loop where you apply some code.\n\nImagine that you want to calculate the factorial of 10. The factorial of a number is simply the product of all positive numbers smaller or equal than the number you specify (and it is denote with a “!” after the number). For example, the factorial of 3 is \\(3! = 3 \\times 2 \\times 1\\). A simple way of calculating the factorial of 10 by using a for loop is:\nresult = 1\nfor(i in 1:10) {\n  result = result*i\n}\nIn this for loop, the collection is 1:10, the variable to keep track of the number is i and the body of the loop is result = result*i. This for loop shows a very typical pattern: we want to summarise some collection of numbers into a single number, in this case, result.\nAnother typical pattern is when we want to calculate the elements of a collection. In this case, it is a good practice to “pre-allocate” your output collection before looping, so R knows how much memory to allocate for this vector. For example,\nx = 1:10\ny = numeric(10)\nfor(i in 1:10) {\n  y[i] = exp(x[i])\n}\nIn this case, the result of the for loop (y) is a collection of 10 elements where each element is the exponential transformation of the element in x with the same position. Note that we specify before the loop that y will have 10 elements. Although this is not strictly required in this case, it is a good practice both to avoid errors and to make your code run faster.\nFor loops are not that common in R as in other languages The reason is that many mathematical and statistical functions are already, implicitly, looping over your collections. For examples, when you take the exponential (exp()) of a collection of numbers, it will produce a new collection which is the result of looping over the original collection. That is:\nx = 1:10\ny = exp(x)\nis equivalent to the previous loop described before. As you can see, this second option requires less code and it is easier to read, which is one of the reasons why R is such a greate language for working with data. In addition, if you rely on this implicit looping your code will run much faster.\nHowever, there may be situations where you really cannot avoid a for loop. For example, if you have collected multiple datasets and need to perform the same analysis on each dataset, you could store your datasets in a list and use a for loop to iterate over the different datasets.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCalculate the logarithm of the sequence 1:10, using first a for loop and then without a for loop.",
    "crumbs": [
      "Lab 3"
    ]
  },
  {
    "objectID": "Lab3/no_solution.html#handling-missing-values-na",
    "href": "Lab3/no_solution.html#handling-missing-values-na",
    "title": "Lab 3: Analyzing functions",
    "section": "4.2 Handling missing values (NA)",
    "text": "4.2 Handling missing values (NA)\nMissing values in data collection and analysis deserve special attention. You can get missing values during you data collection for various reasons, e.g. you missed the opportunity to take a measurement, you lose one of the replicates due to contamination, you wrote down a wrong value, or you even lost some of your data.\nThe most important thing you need to understand, is that in almost all cases of a missing value, you should not represent a missing value with a zero (0). This will throw your analysis out of balance and give erroneous results. A common way of representing missing data when you are performing data input, would be with a character like an asterisk (*) or a hyphen (-).\nMany of the functions that read data in R have an argument that allows you to select how you have represented a missing-value in your data. As an example the function read.csv (which reads a comma-delimited data file) would be used like this to read from a file named “your-data-file”:\nmyData &lt;- read.csv(\"you-data-file\", na.strings=c(\"*\",\"-\") )\nIn this case we instructed R, with the argument na.strings=c(\"*\",\"_\") to read our file, and substitute any occurence of an asterisk (*) or a hyphen(-) with an NA symbol.\nThe R languages has a special way of representing missing values in a dataset. A missing value is denoted with the symbol NA which stands for “Not Available”. By default, missing values will “propagate” throughout the calculations. For example, given two vectors of data:\nx = c(1,2,3)\ny = c(2,4,NA)\nWhen you combine these vectors (e.g. add them or multiply them) you will see that the third component is always NA\nx + y\nx*y\nWhen you calculate some statistical property of your data (e.g. mean, standard deviation) it will, by default, report NA if there is at least one missing value in your data\nmean(x)\nmean(y)\nMost statistical functions in R allow you to specify how to deal with missing values. Most often, you are given the option to ignore any missing values from the data when calculating an statistical property through an argument often called na.rm. For example, in order to get the mean of the non-missing values of y we need:\nmean(y, na.rm = TRUE)\nwhich, of course, is the mean of 2 and 4. However, other functions not have an option to handle NA even though you still need to make a decision on how to deal with them. For example, when you calculate the length of a dataset (length()) do you want to consider the whole data or only the non-missing values? This is not a trivial question and the answer on the context where you will use the result. In any case, if you want to remove the NA when calculating the length you need to be more creative. Fortunately, R offers the function is.na which returns a vector of TRUE or FALSE values corresponding to the index of mssing or non-missing data values in the vector y:\nis.na(y)\nNext a vector without NA can be obtained through:\nlength(y[!is.na(y)])\nWhich only gives 2 as the third element is missing. Remember that ! is a negation operator, so !is.na actually means “is not NA”.\nBy the way, you should not confuse NA with NaN which stands for “Not a Number”. An NaN is the result of either an expression with indeterminate form (e.g. 0/0 or Inf/Inf) or when a function is evaluated outside of its valid domain (e.g. sqrt(-1) or log(-1)).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nGiven some data created from the following code c(25,1,10,89, NA, NA), calculate the mean value and the standard error of this mean (\\(s.e.m. = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation and \\(n\\) is the number of items) by ignoring missing values.",
    "crumbs": [
      "Lab 3"
    ]
  },
  {
    "objectID": "Lab3/no_solution.html#making-a-function",
    "href": "Lab3/no_solution.html#making-a-function",
    "title": "Lab 3: Analyzing functions",
    "section": "4.3 Making a function",
    "text": "4.3 Making a function\nWhen you want to repeat a calculation for different data, it is best to code your calculations inside a function. R consists of many built-in functions, but sometimes you need to do a calculation that is not available in R. A function is defined by 4 elements\n\nThe name of the function. For example, in R there is a function that calculates the arithmetic mean of a vector of data and its name is mean. You should make sure that the name of your function does not coincide with existing functions, that it is not too long and that it conveys its meaning. You can check if a function already exist in the base or any of the packages you loaded through ?nameoffunction.\nThe arguments of the function. These are the variables that you need to pass to the function (i.e., inputs). The arguments are defined by a position and a name. Also, some arguments may have default values which means that you do not need to specify them every time you call the function. For example, the function mean, contains three arguments (x, trim and na.rm) but the last two have default values.\nThe body of the function. This is the actual code that the function will execute. The real mean function in R has some crytpic body that requires advanced knowledge of the language to understand. However, a more “naive” implementation of mean could be sum(x)/length(x). Note that the body of a function can consist of multiple lines.\nThe return value of the function. This is the result of applying the function on the arguments. By default, the result of the last line code in the body of the function is the return value of the function. You can also return from any point in the body with the function return() with the variable you want to return inside.\n\nThe R language specifies a particular syntax on how to build a function. For example, a naive_mean could be defined as:\nnaive_mean = function(x, na.remove = FALSE) {\n  total = sum(x, na.rm = na.remove)\n  n = length(x[!is.na(x)])\n  result = total/n\n  return(result)\n}\nIn this case, the function naive_mean has two arguments (x and na.remove) where the second argument has a default value of FALSE and the body consists of several lines of code. These are respectively the sum of the elements of x with the na.rm depending on whether you specified TRUE or FALSE in the na.remove argument; n that calculates the length of the vector x without NAs, and the calculation of the mean. The last statement returns the result. Notice that arguments are separated by commas and the body of the function is enclosed in curly braces {}. The name of the function is simply the name of the variable to which you assigned the function (i.e., naive_mean). You can see below that you can use this function in a similar manner to the built-in mean\nx = 1:10\nnaive_mean(x)\nNotice that we did not specify the value of na.remove as the default is ok in this case. However, if we had missing values, the NA would propagate to the output:\nx = c(1,2,NA,4)\nnaive_mean(x)\nSpecifying na.remove=FALSE can be used as a double check that there are no NAs in your vector. If they are present it forces us to make a decision about what to do with the NAs. Let’s say that, for the moment, we want to just remove the values that are NA from the calculation. In this case, we just change the value of the default parameter.\nnaive_mean(x, na.remove = TRUE)\nFor convenience, default parameters are specified by name rather than position. However we could have also said naive_mean(x,TRUE) or even naive_mean(x = x, na.remove = TRUE). All these forms of calling functions are OK, whether you choose one style or another is a matter of taste.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nBuild a function to calculate the standard deviation (\\(\\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n\\left(x_i - \\bar x\\right)^2}{n - 1}}\\)). Test your function with some data that includes missing values, and compare to the built in function for the standard deviation sd.\n\n\n\nSuprisingly the base R does not have a built in function for the standard error of the mean (aka sem). The sem is defined as \\(\\frac{\\sigma}{\\sqrt(n)}\\).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nMake you own function for the sem and use your own home-made function of the standard deviation for that.\n\n\n\nAs you see you can call functions inside functions. It is recommended to divide the work you want to do into little functions that each carry out a specific task, and then combine those functions into a larger function that combines these tasks. This facilitates error checking.",
    "crumbs": [
      "Lab 3"
    ]
  },
  {
    "objectID": "Lab4/material.html",
    "href": "Lab4/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "This lab has two goals:\n\nPractice with probability distributions on different types of data.\nMake you familiar with the technicalities of stochastic distributions in R.\n\nIn particular, how to generate values from probability distributions and how to make your own probability distribution. Under time constraints, make sure that you make at least the exercises in the first three sections."
  },
  {
    "objectID": "Lab4/material.html#jensens-inequality",
    "href": "Lab4/material.html#jensens-inequality",
    "title": "1 Learning goals",
    "section": "4.1 Jensen’s inequality",
    "text": "4.1 Jensen’s inequality\nJensen’s inequality states the following: Suppose you have a number of values, \\(x\\), with a mean \\(\\bar{x}\\), and a non-linear function \\(f(x)\\). Then the mean of \\(f(x)\\) is not equal to \\(f(\\bar{x})\\).\nJensen’s inequality can be important in a number of cases. The first one is mentioned in Chapter 4 (page 104) on how variability can change the mean behaviour of a system (damselfish).\nAnother example where Jensen’s inequality kicks in is when transforming your data. Data transformations are commonly applied to get normally distributed errors. Because in statistical models you are often interested in the mean effect of a given treatment.\nNote that quantiles are not afffected by Jensen’s inequality as long as the transformation is monotonic. Thus, if you were to model the median effect of a given treatment you could back transform that.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind out what the effect of Jensen’s inequality is on a series of log-tranformed datapoints with respect to the estimated mean.\nUse the following pseudo-code:\n\nGenerate 10 random deviates from a uniform distribution (choose the range of 0 to 10).\nCalculate the mean of those 10 deviates.\nPlot the function \\(\\log(x)\\) with curve() on the range from 0-10, and plot random sample onto it.\nCalculate the mean of the log-transformed values and transform this mean back the normal scale, and compare to the mean calculated at 1.\nPlot the means with abline(h=...) if you want to draw a horizontal line or abline(v=...) to draw a vertical line.\nExplain differences between the two means.\n\n\n\n\nThis exercise shows that it is usually a good idea to leave variables untransformed when estimating the properties from this data."
  },
  {
    "objectID": "Lab4/material.html#zero-inflated-distributions",
    "href": "Lab4/material.html#zero-inflated-distributions",
    "title": "1 Learning goals",
    "section": "6.1 Zero-inflated distributions",
    "text": "6.1 Zero-inflated distributions\nThe general formula for the probability distribution of a zero-inflated distribution, with an underlying distribution \\(P(x)\\) and a zero-inflation probability of \\(p_z\\), is:\n\\[\n\\begin{eqnarray*}\n\\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\\\\n\\mbox{Prob}(x&gt;0) & = & (1-p_z) P(x)\n\\end{eqnarray*}\n\\]\nSo, for example, we could define a probability distribution for a zero-inflated negative binomial as follows:\ndzinbinom = function(x,mu,size,zprob) {\n  ifelse(x==0,\n         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),\n         (1-zprob)*dnbinom(x,mu=mu,size=size))\n}\nThe name, dzinbinom, follows the R convention for a probability distribution function: a d followed by the abbreviated name of the distribution, in this case zinbinom for “zero-inflated negative binomial”).\nThe ifelse() command checks every element of x to see whether it is zero or not and fills in the appropriate value depending on the answer.\nThe sampling function for our zero-inflated distribution would look like this:\nrzinbinom = function(n,mu,size,zprob) {\n  ifelse(runif(n)&lt;zprob,\n         0,\n         rnbinom(n,mu=mu,size=size))\n}\nThe command runif(n) picks n random values between 0 and 1; the ifelse command compares them with the value of zprob. If an individual value is less than zprob (which happens with probability zprob=\\(p_z\\)), then the corresponding random number is zero; otherwise it is a value picked out of the appropriate negative binomial distribution.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCheck graphically that these functions actually work. For instance, you could compare the results with a negative binomial function with the same mean and variance as the data."
  },
  {
    "objectID": "Lab4/solution.html",
    "href": "Lab4/solution.html",
    "title": "Lab 4: Probability distributions (solutions)",
    "section": "",
    "text": "This lab has two goals:\n\nPractice with probability distributions on different types of data.\nMake you familiar with the technicalities of stochastic distributions in R.\n\nIn particular, how to generate values from probability distributions and how to make your own probability distribution. Under time constraints, make sure that you make at least the exercises in the first three sections.",
    "crumbs": [
      "Solutions",
      "Lab 4 (solutions)"
    ]
  },
  {
    "objectID": "Lab4/solution.html#jensens-inequality",
    "href": "Lab4/solution.html#jensens-inequality",
    "title": "Lab 4: Probability distributions (solutions)",
    "section": "4.1 Jensen’s inequality",
    "text": "4.1 Jensen’s inequality\nJensen’s inequality states the following: Suppose you have a number of values, \\(x\\), with a mean \\(\\bar{x}\\), and a non-linear function \\(f(x)\\). Then the mean of \\(f(x)\\) is not equal to \\(f(\\bar{x})\\).\nJensen’s inequality can be important in a number of cases. The first one is mentioned in Chapter 4 (page 104) on how variability can change the mean behaviour of a system (damselfish).\nAnother example where Jensen’s inequality kicks in is when transforming your data. Data transformations are commonly applied to get normally distributed errors. Because in statistical models you are often interested in the mean effect of a given treatment.\nNote that quantiles are not afffected by Jensen’s inequality as long as the transformation is monotonic. Thus, if you were to model the median effect of a given treatment you could back transform that.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind out what the effect of Jensen’s inequality is on a series of log-tranformed datapoints with respect to the estimated mean.\nUse the following pseudo-code:\n\nGenerate 10 random deviates from a uniform distribution (choose the range of 0 to 10).\nCalculate the mean of those 10 deviates.\nPlot the function \\(\\log(x)\\) with curve() on the range from 0-10, and plot random sample onto it.\nCalculate the mean of the log-transformed values and transform this mean back the normal scale, and compare to the mean calculated at 1.\nPlot the means with abline(h=...) if you want to draw a horizontal line or abline(v=...) to draw a vertical line.\nExplain differences between the two means.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nrf = runif(10,min=0,max=10)\n\n\n\nmean(rf)\n\n\n\nplot(log(rf)~ rf)\ncurve(log(x),add=T)\n\n\n\nexp(mean(log(rf)))` versus `mean(rf)\n\n\n\nsegments(x0=0,y0=log(mean(rf)),x1=mean(rf), y1=log(mean(rf)),lty=1)\nsegments(x0=mean(rf),y0=0,x1=mean(rf), y1=log(mean(rf)),lty=1)\nsegments(x0=0,y0=mean(log(rf)),x1=exp(mean(log(rf))), y1=mean(log(rf)),lty=2)\nA dotted line for the mean of the log transformed values\nsegments(x0=exp(mean(log(rf))),y0=mean(log(rf)),\n            x1=exp(mean(log(rf))),\n            y1=min(log(rf)),lty=2)\nBy doing a log transformation first, the higher values are “compressed” and weigh less into the mean.\n\n\n\n\n\n\nThis exercise shows that it is usually a good idea to leave variables untransformed when estimating the properties from this data.",
    "crumbs": [
      "Solutions",
      "Lab 4 (solutions)"
    ]
  },
  {
    "objectID": "Lab4/solution.html#zero-inflated-distributions",
    "href": "Lab4/solution.html#zero-inflated-distributions",
    "title": "Lab 4: Probability distributions (solutions)",
    "section": "6.1 Zero-inflated distributions",
    "text": "6.1 Zero-inflated distributions\nThe general formula for the probability distribution of a zero-inflated distribution, with an underlying distribution \\(P(x)\\) and a zero-inflation probability of \\(p_z\\), is:\n\\[\n\\begin{eqnarray*}\n\\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\\\\n\\mbox{Prob}(x&gt;0) & = & (1-p_z) P(x)\n\\end{eqnarray*}\n\\]\nSo, for example, we could define a probability distribution for a zero-inflated negative binomial as follows:\ndzinbinom = function(x,mu,size,zprob) {\n  ifelse(x==0,\n         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),\n         (1-zprob)*dnbinom(x,mu=mu,size=size))\n}\nThe name, dzinbinom, follows the R convention for a probability distribution function: a d followed by the abbreviated name of the distribution, in this case zinbinom for “zero-inflated negative binomial”).\nThe ifelse() command checks every element of x to see whether it is zero or not and fills in the appropriate value depending on the answer.\nThe sampling function for our zero-inflated distribution would look like this:\nrzinbinom = function(n,mu,size,zprob) {\n  ifelse(runif(n)&lt;zprob,\n         0,\n         rnbinom(n,mu=mu,size=size))\n}\nThe command runif(n) picks n random values between 0 and 1; the ifelse command compares them with the value of zprob. If an individual value is less than zprob (which happens with probability zprob=\\(p_z\\)), then the corresponding random number is zero; otherwise it is a value picked out of the appropriate negative binomial distribution.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nCheck graphically that these functions actually work. For instance, you could compare the results with a negative binomial function with the same mean and variance as the data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nrzinbinom = function(n,mu,size,zprob) {\n    ifelse(runif(n)&lt;zprob, 0,rnbinom(n,mu=mu,size=size))\n}\na = rzinbinom(1000,mu=4,size=1,zprob=0.2)\nmean.a = mean(a)\nvar.a = var(a)\nsize = 1/(((var.a - mean.a))/mean.a^2)\na1 = rnbinom(1000,mu=mean.a,size=size)\nx = as.numeric(names(table(a)))\nplot(as.numeric(table(a))~ x,type=\"h\")\nx = as.numeric(names(table(a1)))\npoints(as.numeric(table(a1))~ x,type=\"p\")",
    "crumbs": [
      "Solutions",
      "Lab 4 (solutions)"
    ]
  },
  {
    "objectID": "Lab6/no_solution.html",
    "href": "Lab6/no_solution.html",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation",
    "section": "",
    "text": "You will learn how to:\n\nProgram the likelihood function of a model.\nEstimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.\nEstimate the confidence intervals of the model parameters through profiling and the quadratic approximation.\nPerform stochastic simulations from the fitted models\n\n5 (Optionally) Estimate parameters and uncertainty using Laplace’s approximation to Bayes rule",
    "crumbs": [
      "Lab 5 & 6"
    ]
  },
  {
    "objectID": "Lab6/no_solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "href": "Lab6/no_solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation",
    "section": "3.1 Finding the maximum likelihood estimate of the paramaters",
    "text": "3.1 Finding the maximum likelihood estimate of the paramaters\n\n\n\n\n\n\nExercise\n\n\n\n\n\nTake the steps below\n\nGenerate 50 values from a negative binomial (rnbinom) with \\(\\mu=1\\), \\(k=0.4\\). Save the values in variables in case we want to use them again later.\nPlot the numbers in a frequency diagram\nNext, define the negative log-likelihood function for a simple draw from a negative binomial distribution: first include the parameters of the model and then the variables in the data that are used in the model (see example above)\nCalculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Remember that combine these parameter values using a list() and to name them so you can recognize them later on.\nCalculate the NLL of parameter values that are far from the values that were used to generate the data (e.g. \\(\\mu=10\\), \\(k=10\\))\nCalculate the maximum likelihood estimate (MLE)? Use mle2 with the default options and use the method-of-moments estimates as the starting estimates (par): opt1 = mle2(minuslogl = NLLfun1, start = list(mu = mu.mom, k = k.mom))\nWhat is the difference in NLL between the MLE estimates and the NLL derived at 5? Does it make sense?\nPerform a likelihood ratio test at 95% confidence compare the values obtained in 5 and 7.",
    "crumbs": [
      "Lab 5 & 6"
    ]
  },
  {
    "objectID": "Lab6/no_solution.html#choosing-probability-distributions",
    "href": "Lab6/no_solution.html#choosing-probability-distributions",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation",
    "section": "6.1 Choosing probability distributions",
    "text": "6.1 Choosing probability distributions\nIn this exercise we revisit the first exercise from Lab 1. In that exercise we had six different datasets each describing a biological phenomenon. The first step was to choose a deterministic function that describes the mean effect of the predictor variable (x) on the response variable (y; Lab 3). The second step involved the choice of the stochastic distribution which describes how the data varies around the mean (Lab 4).\n\n\n\n\n\n\nExercise\n\n\n\n\n\nReload the first dataset, revisit the choice of your deterministic function, the eyeballed parameters, and the stochastic distribution. Next simulate data using these three components. Compare the simulated values with the observed values in a plot.",
    "crumbs": [
      "Lab 5 & 6"
    ]
  },
  {
    "objectID": "Lab6/no_solution.html#likelihood-surface",
    "href": "Lab6/no_solution.html#likelihood-surface",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation",
    "section": "7.1 Likelihood surface",
    "text": "7.1 Likelihood surface\nTo find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues from Lab 3 where you used the negative binomial to generate 50 numbers and fitted back the parameters.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFor the likelihood surface:\n\nSet up vectors of \\(\\mu\\) and \\(k\\) values. Let’s try \\(\\mu\\) from 0.4 to 3 in steps of 0.05 and \\(k\\) from 0.01 to 0.7 in steps of 0.01.\nSet up a matrix to hold the results, The matrix for the results will have rows corresponding to \\(\\mu\\) and columns corresponding to \\(k\\):\nRun for loops to calculate and store the values. Use a for nested in another one\nDrawing a contour using the function ‘contour’. Change the argument nlevels to 100 to get a better view of the likelihood surface\nAdd the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.",
    "crumbs": [
      "Lab 5 & 6"
    ]
  },
  {
    "objectID": "Lab6/no_solution.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "href": "Lab6/no_solution.html#optional-bayesian-parameter-estimation-laplaces-approximation",
    "title": "Lab 5 & 6 Fitting models to data + stochastic simulation",
    "section": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation",
    "text": "7.2 (OPTIONAL) Bayesian parameter estimation: Laplace’s approximation\nWe are going to analyze the shapes2 dataset again, but this time using a Bayesian approach where we approximate the posterior distribution. This approximation is sometimes called Laplace’s approximation, but that name is also reserved for a related technique that we will use in Chapter 10, so here we will call it quadratic approximation because it uses the same mathematical tools as the quadratic approximation in maximum likelihood (in fact, the two are equivalent if you assume uniform priors).\nWe will work with the second of the shapes datasets and we will construct a type of prior know as Weakly Informative Priors (WIPs). These are used when you do not have clear prior information (or don’t want to use it) except for knowing what the order of magnitude of the parameters should be. WIPs are very popular in modern Bayesian statistics because they have a small effect on the posterior distribution while addressing many of the complicated numerical issues that we encounter in maximum likelihood. Use quadratic approximation with WIPs is basically an enhanced version of maximum likelihood.\n\n7.2.1 Constructing priors\nLet’s assume that these data represent the predation rate as a function of prey density, so it makes to try the functional responses we learnt in Chapter 3. Let’s assume that we can build a prior that the prey density is in the order of 100 and that the predation rate is in the order of 50 With this information we can already come up with Weakly Informative Priors (WIPs for short) that only informs about the order of magnitude of a variable. A simple way to create WIPs might be:\n\nDefine the prior information with a Normal distribution for each parameter.\nSet the mean to the order of magnitude of the corresponding variable (or something related to it, depends on the role)\nSet the standard deviation equal to the mean.\nFor scale parameters (e.g. the standard deviation) the prior distribution is often a truncated normal with mean of 0 and standard deviation dependend on the scale of the response variable.\n\nLet’s first start with the functional response Type II that has the form a*x/(b + x). We will come up with reasonable prior distributions and generate a bunch of simulations (these are called prior predictions and the can be used to check your priors are reasonable). Because all parameters should be positive, I use truncated normals (that only keep positive part):\nlibrary(truncnorm) # to truncate parameters\n# Generate random samples of each parameter\nN = 1000\na = rtruncnorm(N, mean = 50, sd = 50, a  = 0) # a scales with predation rate\nb = rtruncnorm(N, mean = 100/2, sd = 100, a = 0) # b scales with half prey density\nsigma = rtruncnorm(N, mean = 0, sd = 10, a = 0) # sigma scales with predation rate\nFor every prior sample, we can then generate a mean prediction:\nxseq = 0:200\nmu_y_prior = sapply(1:N, function(i) a[i]*xseq/(b[i] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_prior = rowMeans(mu_y_prior)\nlower_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.025)\nupper_mu_y_prior = apply(mu_y_prior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_prior, type = \"l\", ylim = c(0, 100),\n    xlab = = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_prior, lty = 2)\nlines(xseq, upper_mu_y_prior, lty = 2)\npoints(shapes2)\nWith WIPS you want to make sure that the range of predictions is much wider than the observed data, while avoiding nonsensical predictions (e.g., negative values or values that are way too high).\n\n\n7.2.2 Laplace’s approximation: How Bayes rule was solved originally\nRemember that Bayes rule was\n\\[\nP(\\theta | D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)},\n\\] where \\(P(D|\\theta)\\) is equivalent to the likelihood function, \\(P(\\theta)\\) is the prior probability of parameter values, \\(P(\\theta | D)\\) is the posterior probability and \\(P(D)\\) is the probability of seeing thee particular data under the assumed mode.\nNote that \\(P(D|\\theta)\\) and \\(P(\\theta)\\) are built by you (they are functions of \\(\\theta\\)) and therefore you know them, but you do not know \\(P(D)\\) (this is an unknown value). If you knew it, then Bayesian statistics would be “straightforward” in the sense that you just need to multiple two functions and divide by a constant. In practice you would need to do some extra work as this only gives you a probability density function, you would then need to figure out the cumulative density function, how to generate random samples from the posterior, etc.\nThe first person to solve Bayes rule was called Laplace and he realized you could calculate \\(P(D)\\) as:\n\\[\nP(D) = \\int{P(D|\\theta)P(\\theta)d\\theta}\n\\]\nThis seemingly innocent integral can be quite hard (or in practice impossible) to solve beyond the simplest of models (the calculus of probability of functions can be too nasty). He developed a mathematical approximation that consists of:\n\nTake the log of \\(P(D|\\theta)P(\\theta\\) (let’s call it the log probability density or lpd).\nFind its maximum.\nApproximate the lpd with a 2nd Order Taylor approximation around the maximum.\n\nFrom that he derived what the integral should be if the approximation was exact, based on the Normal distribution. This procedure for approximating integrals will come back in Chapter 10.\nAfter Laplace published his method it was shown (not sure when or by whom) that his method of approximating \\(P(D)\\) was equivalent to the following:\n\nFind the value of \\(\\theta\\) that maximizes lpd (we call it Maximum a Posteriori, or MAP).\nApproximate the posterior distribution \\(P(\\theta | D)\\) by a normal distribution with mean equal to MAP and covariance matrix derive from the Hessian matrix of second order derivatives.\n\nThat is, Laplace’s approximation to \\(P(D)\\) is equivalent to approximating the posterior distribution by a Normal distribution centered around its mode.\nDoes this procedure sound familiar? If we assume uniform priors, Laplace’s approximation is exactly the same as maximum likelihood with quadratic approximation (section 6.5 in the book) which is the original maximum likelihood method published by Fisher in 1922. Indeed, Laplace invented this method 150 years before Fisher, but in a Bayesian rather than Frequentist context. The only difference is that in the Bayesian context you take into account prior distributions rather than just the likelihood.\nAs discussed in the book, this approximation becomes better as the amount of data increases (i.e., it is only exact asymptotically) and this remains true in the Bayesian context too. Some authors seem to think that the only correct way to do Bayesian statistics is to run complex algorithms such as Markov Chain Monte Carlo (we cover that in the next chapter) but using Laplace’s approximation is as valid as using the quadratic approximation for maximum likelihood. Thus, we will learn how to use this approach first and leave the more complex MCMC approaches for later.\n\n\n7.2.3 Implementing Laplace’s approximation\nWe can implement Laplace’s approximation in a similar way to how we implemented the method of maximum likelihood. First, we build a function to compute the log posterior density (for consistency, we will use the negative, as in the maximum likelihood theory):\n# First the negative log likelihood\nnll = function(a, b, sd, x, y) {\n  mu = (a*x)/(b+x)\n  nll = -sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Now we add the prior\nnlpd = function(a, b, sd, x, y){\n    nlpd = nll(a, b, sd, x, y) - \n           dnorm(a, mean = 50, sd = 50, log = TRUE) -\n           dnorm(b, mean = 50, sd = 100, log = TRUE) -\n           dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n    return(nlpd)\n}\nnlpd(a = 50, b = 50, sd = 10, x = shapes2$x, y = shapes2$y)\nWe can now run the optimizer as usual\nMAP = mle2(nlpd, start = list(a = 20, b = 10, sd = 1), \n           data = shapes2, method=\"L-BFGS-B\",\n           lower = c(a = 0, b = 0, sd = 0))\nLet’s look at the results:\nsummary(MAP)\nOf course these results assume we were doing maximum likelihood, so much of the information is not relevant. But the first two columns of the Coefficients sector are now giving us the mode of the posterior distribution and the standard deviation of each of the posterior marginal distributions.\nIn good Bayesian fashion we may want to generate a sample of values from the posterior distribution and use to make other calculations (e.g., predictions). We first need to extract the variance-covariance matrix\nV = vcov(MAP)\nThen we use a multivariate normal with mean equal to the MAP estimate and using the variance-covariance matrix we just extracted:\nlibrary(mvtnorm)\nposterior = rmvnorm(n = 1e4, mean = coef(MAP), sigma = V)\nLet’s visualize it (I only choose 1000 values because otherwise it is too slow):\npairs(posterior[1:1e3,], pch = \".\")\nWe can also look at the marginal distributions and compare the posterior and prior:\nprior = cbind(a = a, b = b, sd = sigma)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  plot(density(posterior[,i]), xlab = colnames(posterior)[i], main = \"\")\n  lines(density(prior[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nNotice that in the region where the posterior samples where generated (basically where most of the probability is located), the prior distributions are practically flat. This is what makes a prior weakly informative. It has practically no effect on the posterior but it can help the algorithm find the solution much faster. This will lead to Bayesian results that are very similar to Maximum Likelihood results.\nWe can also make predictions and compare it to the prior predictions we did before.\nN = nrow(posterior)\nxseq = 0:200\nmu_y_posterior = sapply(1:N, function(i) posterior[i,\"a\"]*xseq/(posterior[i,\"b\"] + xseq))\nThis matrix has 1000 columns (one for each sample of the priors) and 201 rows for the seq of x values. We can summarize all these predictions into an average and quantiles:\nmean_mu_y_posterior = rowMeans(mu_y_posterior)\nlower_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.025)\nupper_mu_y_posterior = apply(mu_y_posterior, 1, quantile, prob = 0.975)\nAnd now we can visualize it:\nplot(xseq, mean_mu_y_posterior, type = \"l\", ylim = c(0, 100),\n    xlab = \"Prey density\", ylab = \"Predation rate\")\nlines(xseq, lower_mu_y_posterior, lty = 2)\nlines(xseq, upper_mu_y_posterior, lty = 2)\nlines(xseq, mean_mu_y_prior, col = 2)\nlines(xseq, lower_mu_y_prior, lty = 2, col = 2)\nlines(xseq, upper_mu_y_prior, lty = 2, col = 2)\npoints(shapes2)\nIf you look hard you will see that the black lines are now plotted through the cloud of points and the uncertainty is quite small. Remember that this represents our uncertainty about the average predation rate and not individual values (that would require including the parameter sd too). The fact that our prior distribution was a bit off is not a problem as we made our prior uncertainty big enough to accommodate a wide range of possible responses.\n\n\n7.2.4 Computing DIC\nWe can compute DIC. Remember that this requires two calculations:\n\nTwice the negative log likelihood at the mean posterior estimate.\nThe same quantity but averaged over the posterior distribution.\n\nLet’s do the first one:\nmean_posterior = colMeans(posterior)\nterm1 = 2*nll(a = mean_posterior[1], b = mean_posterior[2], sd = mean_posterior[3], \n             shapes2$x, shapes2$y)\nThe second one is more involved but since we already have a sample of values from the posterior we can use them directly to estimate the mean deviance (this is essentially the Monte Carlo method to calculate averages):\nall_nlls = apply(posterior, 1, function(x) 2*nll(a = x[1], b = x[2], sd = x[3],\n                                                 x = shapes2$x, y = shapes2$y))\nterm2 = mean(all_nlls)                                                 \nThe effective number of parameters is the difference of term2 and term1:\npDIC = term2 - term1\nNotice that this is slightly larger than 3. The Deviance Information Criterion then becomes:\nDIC = term1 + 2*pDIC\nIn the next chapter (where we will use Markov Chain Monte Carlo) we will also learn more modern information criteria that are are meant to replace DIC.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b + x^2) to the same dataset as the model above using Laplace’s approximation.\nCompare this model and the previous model using DIC",
    "crumbs": [
      "Lab 5 & 6"
    ]
  },
  {
    "objectID": "Lab7/material.html",
    "href": "Lab7/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "You will learn how to:\n\nDeal with optimization problems and assess confidence limits\nOptionally, estimate parameters in a Bayesian way using Stan\n\n\n\nFitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\n\n\n\nIn this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC"
  },
  {
    "objectID": "Lab7/material.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "href": "Lab7/material.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "title": "1 Learning goals",
    "section": "",
    "text": "Fitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit."
  },
  {
    "objectID": "Lab7/material.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "href": "Lab7/material.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "title": "1 Learning goals",
    "section": "",
    "text": "In this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC"
  },
  {
    "objectID": "Lab7/solution.html",
    "href": "Lab7/solution.html",
    "title": "Lab 7: Optimisation and all that (solutions)",
    "section": "",
    "text": "You will learn how to:\n\nDeal with optimization problems and assess confidence limits\nOptionally, estimate parameters in a Bayesian way using Stan\n\n\n\nFitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution is given below in the big chunk of code\nshapes1= read.csv(\"shapes1.csv\")\nplot(shapes1)\nnll.mle = function(a,b,sd){\n    # this calculates the mean y for a given value of x: the deterministic function\n    mu = a*(1-exp(-b*shapes1$x))\n    # this calculates the likelihood of the function given the probability\n    # distribution, the data and mu and sd\n    nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))\n    return(nll)\n}\nlibrary(bbmle)\n# Try 4 different starting points\nmle2.1 = vector(\"list\", 4)\nstart_a = c(5,10,20,30)\nstart_b = c(0.001,0.005,0.01,0.1)\nfor(i in 1:4) {\n    mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method=\"Nelder-Mead\")\n}\n# Check the best fit (in this case it is 3rd starting point)\nfor(i in 1:4) {\n    print(logLik(mle2.1[[i]]))\n}\n# Extract the best fit for the rest of the analysis\nbest_mle2.1 = mle2.1[[3]]\nsummary(best_mle2.1)\nlogLik(best_mle2.1)\nconfint(best_mle2.1)\ncoef(best_mle2.1)\nplot(shapes1)\ncurve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)\ncurve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)\n# likelihood surface\na1 = seq(0,40,length.out = 100)\nb1.1 = seq(0.001,0.03,length.out=100)\nb1.2 = seq(0.1,10,length.out=100)\nnll.grid = expand.grid(a1,b1.1)\nnll.grid$NLL = NA\nno = 0\n# Construct first contour\nfor (i in 1:length(a1)){\n    for (j in 1:length(b1.1)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.1[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)\n    }\n}\nlibrary(reshape2)\nz1.1 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])\n# Construct second contour\nno = 0\nfor (i in 1:length(a1)){\n    for (j in 1:length(b1.2)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.2[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)\n    }\n}\nz1.2 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])\n# Plot the two contours\npar(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)\ncontour(a1,b1.2,z1.2,nlevels = 20, xaxt = \"n\", yaxt = \"n\", ylim = c(0,9))\naxis(2, seq(1,9,2))\npoints(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)\npoints(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)\npoints(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)\npoints(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)\ncontour(a1,b1.2,z1.2,levels=120,col=2,add=T)\npar(mar = c(3.5,4,0.5,1))\ncontour(a1,b1.1,z1.1,nlevels = 20)\npoints(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)\npoints(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)\npoints(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)\npoints(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)\ncontour(a1,b1.1,z1.1,levels=120,col=2,add=T)\n# profile\nnll.mle1 = function(a,sd){\n    # this calculates the mean y for a given value of x: the deterministic function\n    mu = a*(1-exp(-b*x))\n    # this calculates the likelihood of the function given the probability\n    # distribution, the data and mu and sd\n    nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))\n    return(nll)\n}\nnll = numeric(length(b1.1))\nfor (i in 1:length(b1.1)){\n    b = b1.1[i]\n    mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method=\"Nelder-Mead\")\n    nll[i] = -logLik(mle.21)\n}\npar(mfrow = c(1,1))\nplot(nll~ b1.1,type=\"l\",xlim=c(0.008,0.012), ylim = c(117,125))\nwhich.min(nll)\n# cutoff\n-logLik(best_mle2.1) + qchisq(0.95, 1)/2\nwhich(nll &lt; 119.852)\nb1.1[c(23,35)]\nplot(nll~ b1.1,type=\"l\",xlim=c(0.0070,0.012),ylim=c(116,125))\nabline(v=c(0.00744,0.01096),lty=2)\nabline(v=0.008968,lty=1,lwd=2)\nabline(v=c(0.00738,0.01103),lty=2,col=\"red\")\nse.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])\nb + c(-1,1)*qnorm(0.975) * se.mu\nconfint(best_mle2.1)\nabline(v=c(0.007177,0.0107589),col=\"blue\")\n\n\n\n\n\n\n\n\n\nIn this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLike in the previous lab, we just need to change the likelihood as we will keep the same priors (the models are similar enough):\n# First the log likelihood (NOT NEGATIVE)\nll2 = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x^2)/(b^2+x^2)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood2 = createLikelihood(likelihood = ll2, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood2$density(c(a = 50, b = 50, sd = 1))\nWe now create the Bayesian setup\nsetup2 = createBayesianSetup(likelihood = likelihood2, prior = priors)\nAnd run the DEzs algorithm to sample from the posterior:\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior2 = runMCMC(setup2, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior2)\nWe can extract the DIC values directly:\nc(Model1 = DIC(posterior)$DIC, Model2 = DIC(posterior2)$DIC)\nJust like before, it is clear that this second model does not perform better.",
    "crumbs": [
      "Solutions",
      "Lab 7 (solutions)"
    ]
  },
  {
    "objectID": "Lab7/solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "href": "Lab7/solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "title": "Lab 7: Optimisation and all that (solutions)",
    "section": "",
    "text": "Fitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question the previous lab and practise a bit more.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution is given below in the big chunk of code\nshapes1= read.csv(\"shapes1.csv\")\nplot(shapes1)\nnll.mle = function(a,b,sd){\n    # this calculates the mean y for a given value of x: the deterministic function\n    mu = a*(1-exp(-b*shapes1$x))\n    # this calculates the likelihood of the function given the probability\n    # distribution, the data and mu and sd\n    nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))\n    return(nll)\n}\nlibrary(bbmle)\n# Try 4 different starting points\nmle2.1 = vector(\"list\", 4)\nstart_a = c(5,10,20,30)\nstart_b = c(0.001,0.005,0.01,0.1)\nfor(i in 1:4) {\n    mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method=\"Nelder-Mead\")\n}\n# Check the best fit (in this case it is 3rd starting point)\nfor(i in 1:4) {\n    print(logLik(mle2.1[[i]]))\n}\n# Extract the best fit for the rest of the analysis\nbest_mle2.1 = mle2.1[[3]]\nsummary(best_mle2.1)\nlogLik(best_mle2.1)\nconfint(best_mle2.1)\ncoef(best_mle2.1)\nplot(shapes1)\ncurve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)\ncurve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)\n# likelihood surface\na1 = seq(0,40,length.out = 100)\nb1.1 = seq(0.001,0.03,length.out=100)\nb1.2 = seq(0.1,10,length.out=100)\nnll.grid = expand.grid(a1,b1.1)\nnll.grid$NLL = NA\nno = 0\n# Construct first contour\nfor (i in 1:length(a1)){\n    for (j in 1:length(b1.1)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.1[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)\n    }\n}\nlibrary(reshape2)\nz1.1 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])\n# Construct second contour\nno = 0\nfor (i in 1:length(a1)){\n    for (j in 1:length(b1.2)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.2[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)\n    }\n}\nz1.2 = as.matrix(pivot_wider(nll.grid, names_from = Var2, values_from = NLL)[,-1])\n# Plot the two contours\npar(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)\ncontour(a1,b1.2,z1.2,nlevels = 20, xaxt = \"n\", yaxt = \"n\", ylim = c(0,9))\naxis(2, seq(1,9,2))\npoints(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)\npoints(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)\npoints(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)\npoints(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)\ncontour(a1,b1.2,z1.2,levels=120,col=2,add=T)\npar(mar = c(3.5,4,0.5,1))\ncontour(a1,b1.1,z1.1,nlevels = 20)\npoints(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)\npoints(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)\npoints(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)\npoints(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)\ncontour(a1,b1.1,z1.1,levels=120,col=2,add=T)\n# profile\nnll.mle1 = function(a,sd){\n    # this calculates the mean y for a given value of x: the deterministic function\n    mu = a*(1-exp(-b*x))\n    # this calculates the likelihood of the function given the probability\n    # distribution, the data and mu and sd\n    nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))\n    return(nll)\n}\nnll = numeric(length(b1.1))\nfor (i in 1:length(b1.1)){\n    b = b1.1[i]\n    mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method=\"Nelder-Mead\")\n    nll[i] = -logLik(mle.21)\n}\npar(mfrow = c(1,1))\nplot(nll~ b1.1,type=\"l\",xlim=c(0.008,0.012), ylim = c(117,125))\nwhich.min(nll)\n# cutoff\n-logLik(best_mle2.1) + qchisq(0.95, 1)/2\nwhich(nll &lt; 119.852)\nb1.1[c(23,35)]\nplot(nll~ b1.1,type=\"l\",xlim=c(0.0070,0.012),ylim=c(116,125))\nabline(v=c(0.00744,0.01096),lty=2)\nabline(v=0.008968,lty=1,lwd=2)\nabline(v=c(0.00738,0.01103),lty=2,col=\"red\")\nse.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])\nb + c(-1,1)*qnorm(0.975) * se.mu\nconfint(best_mle2.1)\nabline(v=c(0.007177,0.0107589),col=\"blue\")",
    "crumbs": [
      "Solutions",
      "Lab 7 (solutions)"
    ]
  },
  {
    "objectID": "Lab7/solution.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "href": "Lab7/solution.html#optional-bayesian-parameter-estimation-markov-chain-monte-carlo",
    "title": "Lab 7: Optimisation and all that (solutions)",
    "section": "",
    "text": "In this section we will revisit the examples we did in Lab 6 but using a Markov Chain Monte Carlo approach to generate samples from the posterior distribution without having to make assumptions about its shape.\nThere are different libraries to perform Bayesian analysis in R. If we want to implement our own model from scratch then the options available are:\n\nBayesianTools: Write the log-likelihood and prior functions (as we have done before) and sample from the posterior using one of the algorithms available in the package BayesianTools. This is the most flexible option, the only caveat is that it might be slow because all the code is running within R (which is a slow language) and the little documentation availabe is really bad.\nNimble: Write your model using the BUGS language, compile it to C++ and sample from the posterior using Metropolis-Hastings, Gibbs or Hamiltonian Monte Carlo. This package is popular in ecology though it is lacking some documentation and support (and sometimes it crashes).\nStan: Write your model using Stan’s own language (an extension of BUGS) in C++ and then sample from the posterior using Hamiltonian Monte Carlo. This is considered the golden standard for Bayesian inference nowadays. It is fast and very well documented and supported.\n\nWe will use the first option in the course because it is easiest to transition from maximum likelihood to using BayesianTools the way course is setup. However, if you really want to use Bayesian inference in your own research I recommend you learn to use Stan or the (very flexible) package brms if your model can be specified as a formula. To help you with that I provide a supplement on Stan where I do the examples in the course in Stan. I also show examples of brms in the supplement on formula-based methods.\nWe will learn how to use BayesianTools using the same model we fitted in the previous lab (shapes2 with a Michaelis-Menten). BayesianTools require the (positive) log-likelihood and the priors to be specified as follows (notice that parameters are all included inside par and the data has to be accessed directly from within the function):\n# First the log likelihood (NOT NEGATIVE)\nll = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x)/(b+x)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood = createLikelihood(likelihood = ll, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood$density(c(a = 50, b = 50, sd = 1))\nThen the priors using BayesianTool’s built-in createPrior function, where we include (i) the log prior density, (ii) a function to sample from priors and (iii) lower and upper bounds on parameters (if relevant). As with the likelihood, parameters must be passed as a vector. Also the sampler should return a matrix of initial samples.\n# Prior density\nlpd = function(par){\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  pd = dnorm(a, mean = 50, sd = 50, log = TRUE) +\n       dnorm(b, mean = 50, sd = 100, log = TRUE) +\n       dnorm(sd, mean = 0,  sd = 10, log = TRUE)\n  return(pd)\n}\n# Generate samples from prior\nlibrary(truncnorm)\nsampler = function(n = 1) {\n  a  = rtruncnorm(n, mean = 50, sd = 50, a = 0)\n  b  = rtruncnorm(n, mean = 50, sd = 100, a = 0)\n  sd = rtruncnorm(n, mean = 0, sd = 10, a = 0)\n  cbind(a, b, sd)\n}\n# Wrap into special object\npriors = createPrior(density = lpd, sampler = sampler, lower = c(0, 0, 0),\n                     upper = rep(Inf, 3))\n# Test the functions\npar = priors$sampler()\npriors$density(par)\nWe have all the information to setup the Bayesian problem:\nsetup = createBayesianSetup(likelihood = likelihood, prior = priors)\nAnd now we can run the MCMC algorithm. BayesianTools contain many options for MCMC algorithms. There are a series of Metropolis algorithms that we could try but these algorithms are quite outdated and are quite difficult to tune properly. Nowadays other algorithms are being used by default:\n\nHamiltonian Monte Carlo: Considered to be the most efficient, it requires accurate calculations of gradients (similar to the derivative-based optimization algorithms). This is implemented in state-of-the-art Bayesian platforms such as Stan, PyMC or Turing. I provide a supplement for Stan (that uses C++) but here we will focus on native R implementations.\nDifferential Evolution Markov Chain: A modern version of MCMC that approximates the shape of the log posterior by keeping a number of internal chains that explore the surface. It is inspired by the Differential Evolution algorithm for optimization and it was partly developed at Wageningen University.\n\nBayesianTools implemented several flavours of DEMC and it we will use the version called DEzs (because it is the most robust and it was the one developed partly at WUR). See here for the publication.\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior = runMCMC(setup, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior)\nWe can see that 16 chains were ran because the DEzs algorithm runs internally 4 chains in order to learn the shape of the posterior distribution. Those internal chains are not independent of each other, so you should still this as 4 independent MCMC runs.\nThe effective sample size is equal to the total sample size (60,000 = 15,000 x 4) correct for autocorrelation.\nThe summary on parameters is the most important bit: it tells us information on the maginal posterior distributions of each parameter. The columns 2.5% and 97.5% can be used to construct credible intervals and the median can be used as a point estimate (similar to MLE or MAP estimates). Note that the MAP estimate is approximate since we did not run optimization.\nThe psf is the Gelman-Rubin diagnostic, which is calculated for each parameter individually and for the overall posterior (the multivariate version). Values below 1.2 are considered acceptable. Here we have values below 1.01 so we are doing quite well!\nFinally, the summary also reports DIC which we can use for model comparison and the correlations among parameters in the posterior distribution\nWe can visualize the trace plot as follows (note there is a bug in BayesianPlots that forces me to say start = 2):\nplot(posterior, start = 2)\nNote how the chains are well mixed and we are getting sensible shapes for the marginal distributions. They are unimodal and approximately symmetric, but they all have a slightly longer tail to the right. These longer tails are caused by the fact that these parameters are constrained to be positive (and is why a Normal approximation will never be exact).\nWe can plot the posterior marginal distributions against the priors\nNotice that this does not show the entire prior distribution but only the part. Let’s extract the posterior samples to further process them:\nposterior_sample = getSample(posterior, start = 2)\nhead(posterior_sample)\nWe can now do the same type of plot we did in the previous lab. Note that I am going to use a histogram to deal better with the noise in the MCMC sample (density plots can be too sensitive to that noise):\nprior_sample = priors$sampler(1e4)\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  histdata = hist(posterior_sample[,i], plot = FALSE, breaks = 30)\n  plot(histdata$mids, histdata$density, t = \"s\",\n       xlab = colnames(posterior_sample)[i], ylab = \"Density\")\n  lines(density(prior_sample[,i]), col = 2)\n}\npar(mfrow = c(1,1))\nWe can see, just like before, that the priors are practically flat in the area where the posterior probability concentrates, so the influence\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model a*x^2/(b^2 + x^2) to the same dataset as the model above using Hamiltonian Monte Carlo.\nCompare the two models using DIC\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLike in the previous lab, we just need to change the likelihood as we will keep the same priors (the models are similar enough):\n# First the log likelihood (NOT NEGATIVE)\nll2 = function(par) {\n  x = shapes2$x\n  y = shapes2$y\n  a = par[1]\n  b = par[2]\n  sd = par[3]\n  mu = (a*x^2)/(b^2+x^2)\n  ll = sum(dnorm(y, mean = mu,sd = sd, log = TRUE))\n}\n# Wrap into special object\nlikelihood2 = createLikelihood(likelihood = ll2, names = c(\"a\",\"b\",\"sd\"))\n# Test the function\nlikelihood2$density(c(a = 50, b = 50, sd = 1))\nWe now create the Bayesian setup\nsetup2 = createBayesianSetup(likelihood = likelihood2, prior = priors)\nAnd run the DEzs algorithm to sample from the posterior:\nset.seed(1234)\nstart = priors$sampler(4)\nsettings = list(iterations = 2e4, nrChains = 4, startValue = start, \n                burnin = 5e3, thin = 1)\nposterior2 = runMCMC(setup2, sampler = \"DEzs\", settings = settings)\nWe can get most of the information we need from the summary:\nsummary(posterior2)\nWe can extract the DIC values directly:\nc(Model1 = DIC(posterior)$DIC, Model2 = DIC(posterior2)$DIC)\nJust like before, it is clear that this second model does not perform better.",
    "crumbs": [
      "Solutions",
      "Lab 7 (solutions)"
    ]
  },
  {
    "objectID": "Lab9/no_solution.html",
    "href": "Lab9/no_solution.html",
    "title": "Lab 9: Modeling variance and dispersion",
    "section": "",
    "text": "1 Learning goals\nYou will learn how to:\n\nAllow for variance to vary across groups\nAllow for variance to vary as a continuous function\n\n\n\n2 Variance across groups\nIn previous labs we have been modelling the mean of the population or process under study. In this lab we are going to look into models of the variance or parameters related to variance (called scale parameters). For many distributions, the mean and variance are not independent and it might not be possible to specify them directly with the usual parameterizations. When it doubt, check Chapter 4 of the book (I will give you some hints on possible ways to decouple mean and scale parameters).\nIn this first section we will work with allometry data relating the length and biomass (as ash-free dry weight or AFD) of clams on a beach in Argentina (this data was used in the book by Zuur et al. (2009)). I have given you the data in the file Clams.txt:\nlibrary(ggplot2)\nclams = read.table(\"./Lab9/Clams.txt\", header = TRUE)\nggplot(clams, aes(x = LENGTH, y = AFD, color = as.factor(MONTH))) + geom_point()\nThis is an allometric relationship, so I will try a power law with a log-normal. I will also parameterize the log-normal distribution as a function of the actual mean \\(\\mu\\) and coefficient of variation \\(cv\\). We can calculate the meanlog and sdlog from mu and cv as\n# Derived from the moments of log-normal (check Wikipedia)\nmu = 0.2\ncv = 1\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\n# Test it\ntest = rlnorm(10000, meanlog = meanlog, sdlog = sdlog)\n(mean(test) - mu)/mu\n(mean(test)/sd(test) - cv)/cv\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a power law of the form \\(a x ^b\\) to the clam data with a fixed cv. Make sure to use proper lower and upper bounds on all parameters. Make sure that mu remains positive or the code will crash. Also, since a is very small you probably want to work with log(a) or something like that and undo the transformation inside the nll function.\nThe scatter seems to vary across months. Refit the model but vary cv per month. Interpret the result: is there evidence that cv would vary across months? Can we justify using this second model?\n\n\n\n\n\n\n3 Variance along continuous variables\nThis is count data so we should change the model to a discrete distribution!!\nIn the book, Bolker analyzes a dataset on cone production by fir trees as a function of their diameter:\nlibrary(emdbook)\nwith(cleanFIR, plot(TOTCONES~DBH))\nWe can see that the average cone production increases with the diameter but also the scatter in the data. Therefore, it might be a good idea to model how this scatter increases with diameter.\nTo helps us figure out a way to model this, we can try to visualize how the standard deviation varies across diameter but for that we need to bin the data. I show you below how to do this, and I also calculate the coefficient of variation because in many distributions (like Gamma and Log-Normal) the standard deviation is not constant but the coefficient of variation is.\n# Remove missing data\ncleanFIR = FirDBHFec[!is.na(FirDBHFec$TOTCONES) & !is.na(FirDBHFec$DBH),]\n# Create bins\nDBHpoints = c(6,8,10,12,14,18)\n# Allocate vetor to store cvs and sd and calculate for first bin\ncvs = numeric(length(DBHpoints))\nsds = numeric(length(DBHpoints))\ntemp = subset(cleanFIR, DBH &lt;= DBHpoints[1])$TOTCONES\ncvs[1] = sd(temp)/mean(temp)\nsds[1] = sd(temp)\n# Repeat for all bins\nfor(i in 2:length(DBHpoints)) {\n  temp = subset(cleanFIR, DBH &lt;= DBHpoints[i] & DBH &gt;= DBHpoints[i - 1])$TOTCONES\n  cvs[i] = sd(temp)/mean(temp)\n  sds[i] = sd(temp)\n}\nWe can now plot the results\npar(mfrow = c(1,2))\nplot(c(5,7,9,11,13,16), sds, xlab = \"DBH\", ylab = \"sd TOTCONES\")\nplot(c(5,7,9,11,13,16), cvs, xlab = \"DBH\", ylab = \"cv TOTCONES\")\npar(mfrow = c(1,1))\nWe can see that while the standard deviation increases with DBH, the coefficient of variation decreases. If I were to model these data with a normal distribution (which is not a good idea because it will predict a lot of negative cone production) I would need to add an increase in the standard deviation with diameter. If I were to model with a log-normal parameterized with a coefficient of variation (like in the example with clams), I would need to model the decrease in the coefficient of variation wit diameter.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit a non-linear model with a log-normal error distribution to the relationship between TOTCONES and DBH using the same parameterization as we used for the study on clams. Make sure to include a component to capture the decrease in the coefficient of variation with diameter.\n\nBe careful that TOTCONES contains zeros so you will need to adjust those (the trick that Bolker uses is to just add 1).",
    "crumbs": [
      "Lab 9"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/material.html",
    "href": "Supplements/bias_variance/material.html",
    "title": "1 Learning goals",
    "section": "",
    "text": "You will learn how to:\n\nReducing bias using restricted maximum likelihood (REML)\nReducing variance by using the mode of the posterior (MAP)"
  },
  {
    "objectID": "Supplements/bias_variance/material.html#bias-in-estimates-of-linear-models",
    "href": "Supplements/bias_variance/material.html#bias-in-estimates-of-linear-models",
    "title": "1 Learning goals",
    "section": "2.1 Bias in estimates of linear models",
    "text": "2.1 Bias in estimates of linear models\nIn a linear model the maximum likelihood estimates of intercepts and slopes are unbiased, but the maximum likelihood estimate of the variance (\\(\\sigma^2\\)) is biased. This motivated the development of REML to compute unbiased estimates of the parameters related to the variance in linear models. If you are fitting linear models, you probably want to use one of the canned methods.\n# Assume a very vanilla linear model but with std dev increasing exponentially\nx = seq(0,1, 0.1)\n# Generate many many samples\nN = 1e4\nY = t(sapply(x, function(x) rnorm(N, 2 + 10*x, exp(x))))\nWe will use the canned method gls from the package nlme to fit the model with normal maximum likelihood:\nlibrary(nlme)\ny = Y[,1]\nfit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\nfit\nWe can extract the intercept slope and exponential coefficient as follows:\nc(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\nWe can now do it for all the samples\nlibrary(future.apply)\nplan(multisession)\ncoefs = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nhead(coefs)\nWe can now calculate the means of each column and compare to the true values:\nbias = colMeans(coefs) - c(2, 10, 1)\nbias\nThis makes more relative to the true values:\nbias/c(2, 10, 1)\nNotice how the estimates of the coefficient relating \\(\\sigma\\) to \\(x\\) is being overestimated by 15% when we compare the average of the sampling distribution to the true value. We do not see this bias in the intercept and slope.\nIf we now redo the fits using REML we will get also unbiased estimates for the coefficient related to the variance:\ncoefs_REML = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"REML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nThe estimates are now unbiased\nbias_REML = colMeans(coefs_REML) - c(2, 10, 1)\nbias_REML/c(2, 10, 1)\nMany statisticians seem to be obsessed with the bias in estimates, but remember that any estimate that you make from a single sample will have an error. For example, we can quantify the Mean Squared Error (MSE) for the ML estimate:\nMSE = c(a = mean((coefs[,1] - 2)^2),\n        b = mean((coefs[,2] - 10)^2),\n        c = mean((coefs[,3] - 1)^2))\nMSE\nThe MSE is actually equal to the bias squared plus the variance of the estimator. This means that we can compute the contribution of bias and variance to the error:\nvars = c(a = var(coefs[,1]), b = var(coefs[,2]), c = var(coefs[,3]))\ncbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)\nWe can see that in the parameters a and b all the error is due to variance in the parameter (since they are unbiased) and in c only 11% of the error is due to the bias.\nThis means that using REML only reduces the error of the estimation by a small amount. Let’s compare the new MSE with REML:\nMSE_REML = c(a = mean((coefs_REML[,1] - 2)^2),\n             b = mean((coefs_REML[,2] - 10)^2),\n             c = mean((coefs_REML[,3] - 1)^2))\ncbind(MSE, MSE_REML, Reduction = (MSE - MSE_REML)/MSE)\nSo, using REML decreased the error in the estimation by c by 33% (incidentally the error in a and b also decreased, that will not always happen but can). This is a pretty good result, especially if you care about the variation in your data (i.e., if it is of ecological relevance and not just noise). So, if you can use REML in a linear model, it pays off to use it.\nNote that methods for model comparison that rely on (pure) maximum likelihood will not apply to models estimated via REML. This includes AIC, BIC and likelihood ratio tests. So, model comparison should always be done with models estimated via maximum likelihood, not REML (in practice this means you end up with two versions of your model)."
  },
  {
    "objectID": "Supplements/bias_variance/material.html#bias-in-estimates-of-generalized-linear-models",
    "href": "Supplements/bias_variance/material.html#bias-in-estimates-of-generalized-linear-models",
    "title": "1 Learning goals",
    "section": "2.2 Bias in estimates of generalized linear models",
    "text": "2.2 Bias in estimates of generalized linear models\nAdd example for glm"
  },
  {
    "objectID": "Supplements/bias_variance/material.html#bias-in-estimates-of-non-linear-models",
    "href": "Supplements/bias_variance/material.html#bias-in-estimates-of-non-linear-models",
    "title": "1 Learning goals",
    "section": "2.3 Bias in estimates of non-linear models",
    "text": "2.3 Bias in estimates of non-linear models\nLet’s do a similar analysis for the log-normal model with exponentially decaying cv that we fitted in Lab 9 I am going to assume some true parameter values (similar to what we got after fitting to the data):\n# True parameters\na = 0.61\nb = 2.1\nc = 4.0\nd = 0.13\n# Use the cleanFIR dataset for DBH\nmu = a*cleanFIR$DBH^b\ncv = c*exp(-d*cleanFIR$DBH)\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\nN = nrow(cleanFIR)\nY = rlnorm(N, meanlog, sdlog)\nplot(cleanFIR$DBH, Y)\nLet’s simulate 1000 datasets:\nlibrary(future.apply)\nplan(multisession)\nY = future_sapply(1:1e3, function(x) rlnorm(N, meanlog, sdlog), future.seed=TRUE)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model to each of the simulated datasets (check how you did it before). Make sure that the bounds are sufficiently wide so that none of the estimates lie on the bound.\nCheck the bias in the parameter estimates. How does it differ from the linear model?\nCompare the bias to the mean absolute error of the parameter estimates, how relevant is the bias?"
  },
  {
    "objectID": "Supplements/bias_variance/solution.html",
    "href": "Supplements/bias_variance/solution.html",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "",
    "text": "You will learn how to:\n\nReducing bias using restricted maximum likelihood (REML)\nReducing variance by using the mode of the posterior (MAP)",
    "crumbs": [
      "Solutions",
      "Variance and bias of estimates (solutions)"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/solution.html#bias-in-estimates-of-linear-models",
    "href": "Supplements/bias_variance/solution.html#bias-in-estimates-of-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.1 Bias in estimates of linear models",
    "text": "2.1 Bias in estimates of linear models\nIn a linear model the maximum likelihood estimates of intercepts and slopes are unbiased, but the maximum likelihood estimate of the variance (\\(\\sigma^2\\)) is biased. This motivated the development of REML to compute unbiased estimates of the parameters related to the variance in linear models. If you are fitting linear models, you probably want to use one of the canned methods.\n# Assume a very vanilla linear model but with std dev increasing exponentially\nx = seq(0,1, 0.1)\n# Generate many many samples\nN = 1e4\nY = t(sapply(x, function(x) rnorm(N, 2 + 10*x, exp(x))))\nWe will use the canned method gls from the package nlme to fit the model with normal maximum likelihood:\nlibrary(nlme)\ny = Y[,1]\nfit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\nfit\nWe can extract the intercept slope and exponential coefficient as follows:\nc(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\nWe can now do it for all the samples\nlibrary(future.apply)\nplan(multisession)\ncoefs = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"ML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nhead(coefs)\nWe can now calculate the means of each column and compare to the true values:\nbias = colMeans(coefs) - c(2, 10, 1)\nbias\nThis makes more relative to the true values:\nbias/c(2, 10, 1)\nNotice how the estimates of the coefficient relating \\(\\sigma\\) to \\(x\\) is being overestimated by 15% when we compare the average of the sampling distribution to the true value. We do not see this bias in the intercept and slope.\nIf we now redo the fits using REML we will get also unbiased estimates for the coefficient related to the variance:\ncoefs_REML = t(future_sapply(1:N, function(i) {\n            y = Y[,i]\n            fit = gls(y~x, weights = varExp(form = ~x), method = \"REML\")\n             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])\n            }))\nThe estimates are now unbiased\nbias_REML = colMeans(coefs_REML) - c(2, 10, 1)\nbias_REML/c(2, 10, 1)\nMany statisticians seem to be obsessed with the bias in estimates, but remember that any estimate that you make from a single sample will have an error. For example, we can quantify the Mean Squared Error (MSE) for the ML estimate:\nMSE = c(a = mean((coefs[,1] - 2)^2),\n        b = mean((coefs[,2] - 10)^2),\n        c = mean((coefs[,3] - 1)^2))\nMSE\nThe MSE is actually equal to the bias squared plus the variance of the estimator. This means that we can compute the contribution of bias and variance to the error:\nvars = c(a = var(coefs[,1]), b = var(coefs[,2]), c = var(coefs[,3]))\ncbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)\nWe can see that in the parameters a and b all the error is due to variance in the parameter (since they are unbiased) and in c only 11% of the error is due to the bias.\nThis means that using REML only reduces the error of the estimation by a small amount. Let’s compare the new MSE with REML:\nMSE_REML = c(a = mean((coefs_REML[,1] - 2)^2),\n             b = mean((coefs_REML[,2] - 10)^2),\n             c = mean((coefs_REML[,3] - 1)^2))\ncbind(MSE, MSE_REML, Reduction = (MSE - MSE_REML)/MSE)\nSo, using REML decreased the error in the estimation by c by 33% (incidentally the error in a and b also decreased, that will not always happen but can). This is a pretty good result, especially if you care about the variation in your data (i.e., if it is of ecological relevance and not just noise). So, if you can use REML in a linear model, it pays off to use it.\nNote that methods for model comparison that rely on (pure) maximum likelihood will not apply to models estimated via REML. This includes AIC, BIC and likelihood ratio tests. So, model comparison should always be done with models estimated via maximum likelihood, not REML (in practice this means you end up with two versions of your model).",
    "crumbs": [
      "Solutions",
      "Variance and bias of estimates (solutions)"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/solution.html#bias-in-estimates-of-generalized-linear-models",
    "href": "Supplements/bias_variance/solution.html#bias-in-estimates-of-generalized-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.2 Bias in estimates of generalized linear models",
    "text": "2.2 Bias in estimates of generalized linear models\nAdd example for glm",
    "crumbs": [
      "Solutions",
      "Variance and bias of estimates (solutions)"
    ]
  },
  {
    "objectID": "Supplements/bias_variance/solution.html#bias-in-estimates-of-non-linear-models",
    "href": "Supplements/bias_variance/solution.html#bias-in-estimates-of-non-linear-models",
    "title": "Supplement: Variance and bias of estimates (solutions)",
    "section": "2.3 Bias in estimates of non-linear models",
    "text": "2.3 Bias in estimates of non-linear models\nLet’s do a similar analysis for the log-normal model with exponentially decaying cv that we fitted in Lab 9 I am going to assume some true parameter values (similar to what we got after fitting to the data):\n# True parameters\na = 0.61\nb = 2.1\nc = 4.0\nd = 0.13\n# Use the cleanFIR dataset for DBH\nmu = a*cleanFIR$DBH^b\ncv = c*exp(-d*cleanFIR$DBH)\nmeanlog = log(mu/sqrt(1 + cv^2))\nsdlog   = sqrt(log(1 + cv^2))\nN = nrow(cleanFIR)\nY = rlnorm(N, meanlog, sdlog)\nplot(cleanFIR$DBH, Y)\nLet’s simulate 1000 datasets:\nlibrary(future.apply)\nplan(multisession)\nY = future_sapply(1:1e3, function(x) rlnorm(N, meanlog, sdlog), future.seed=TRUE)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nFit the model to each of the simulated datasets (check how you did it before). Make sure that the bounds are sufficiently wide so that none of the estimates lie on the bound.\nCheck the bias in the parameter estimates. How does it differ from the linear model?\nCompare the bias to the mean absolute error of the parameter estimates, how relevant is the bias?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nI first define a version of the NLL function that takes the simulated data as input.\nnll = function(a, b, c, d, Y) {\n  mu = a*cleanFIR$DBH^b\n  cv = c*exp(-d*cleanFIR$DBH)\n  meanlog = log(mu/sqrt(1 + cv^2))\n  sdlog   = sqrt(log(1 + cv^2))\n  NLL = -sum(dlnorm(Y, meanlog, sdlog, log = TRUE))\n  NLL\n}\nI test the function to make sure it all works\npar0 = c(a = 0.5, b = 2, c = 3, d = 0.15)\nfit = mle2(minuslogl = nll, start = as.list(par0), data = data.frame(Y = Y[,1]),\n                method = \"L-BFGS-B\",\n               lower = c(a = 0.01, b = 1, c = 1, d = 0),\n               upper = c(a = 4, b = 4, c = 20, d = 1),\n               control = list(parscale = abs(par0)))\nNow that we are certain this is going to work, let’s do it 1000 times and extract the coefficients\ncoefs = t(future_apply(Y, 2, function(Y) {\n    fit = mle2(minuslogl = nll, start = as.list(par0), data = data.frame(Y = Y),\n                method = \"L-BFGS-B\",\n               lower = c(a = 0.01, b = 1, c = 1, d = 0),\n               upper = c(a = 4, b = 4, c = 20, d = 1),\n               control = list(parscale = abs(par0)))\n    coef(fit)\n}))\nNow we can check the bias in the estimates (relative to the magnitude of each parameter)\nbias = colMeans(coefs) - c(0.61, 2.1, 4, 0.13)\nbias/c(0.61, 2.1, 4, 0.13)\nWe can see that the estimate of c and d, related to the coefficient of variation are biased, but so is a, which is related to the mean. This differs from the linear model where the parameters related to the mean were unbiased. In fact the bias in c and d are smaller than the bias in a.\nLet’s compute the MSE and how bias and variance contribute to it\nMSE = c(a = mean((coefs[,1] - 0.61)^2),\n        b = mean((coefs[,2] - 2.1)^2),\n        c = mean((coefs[,3] - 4)^2),\n        d = mean((coefs[,4] - 0.13)^2))\nvars = c(a = var(coefs[,1]), b = var(coefs[,2]), \n         c = var(coefs[,3]), d = var(coefs[,4]))\ncbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)\nWhat this tells us is that the variance in the estimates of all parameters is so large, that the bias becomes pretty much irrelevant to the overall error. SO, not only do we have biases in parameters related to mean (unlike in the linear model) but these biases matter very little given the high variance of the estimates. For that reason, even trying REML (which is much hard in a non-linear non-Normal model) its not worth it.",
    "crumbs": [
      "Solutions",
      "Variance and bias of estimates (solutions)"
    ]
  },
  {
    "objectID": "Supplements/optim.html",
    "href": "Supplements/optim.html",
    "title": "Supplement: mle2 vs optim",
    "section": "",
    "text": "Work in progress",
    "crumbs": [
      "Supplement: mle2 vs optim"
    ]
  }
]
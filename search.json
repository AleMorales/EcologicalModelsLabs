[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Labs CSA-34306 Ecological Modelling and Data Analysis in R",
    "section": "",
    "text": "Practical labs for the course CSA-34306 Ecological Modelling and Data Analysis in R at Wageningen University. Based on Ben Bolker’s book Ecological Models and Data in R modified by Bob Douma and Alejandro Morales. Use the sidebar to navigate through the labs."
  },
  {
    "objectID": "Lab1/no_solution.html",
    "href": "Lab1/no_solution.html",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "",
    "text": "The aim of this tutorial is to learn the basics of R. After completing the tutorial you will be able to:\n\nInstall R and associated packages and Rstudio\nDo interactive calculations in the R console\nConsult the built-in help in R\nCreate, modify and run R scripts\nSetup a typical workflow in R (loading libraries, reading data, doing statistics and saving results)\nWork with the most common data types in R (vectors, matrices, lists and data frames)"
  },
  {
    "objectID": "Lab1/no_solution.html#vectors",
    "href": "Lab1/no_solution.html#vectors",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.1 Vectors",
    "text": "11.1 Vectors\nAn important class of data types are vectors and matrices (1- and 2-dimensional rectangular arrays of numbers). Operations with vectors and matrices may seem a bit abstract now, but we need them to do useful things later. The only properties of vectors are their type (or class) and length, although they can also have an associated list of names.\nWe’ve already seen two ways to create vectors in R:\n\nA command in the console window or a script file listing the values, such as\n\n\ninitialsize=c(1,3,5,7,9,11)\n\n\nUsing read.table:\n\n::: {.cell hash=‘no_solution_cache/html/unnamed-chunk-37_fb8b8260320972ba333ee9d8ac5cc80a’}\ninitialsize=read.table(\"c:/temp/initialdata.txt\")\n:::\n(assuming of course that the file exists in the right place).\nYou can then use a vector in calculations as if it were a number (more or less)\n::: {.cell hash=‘no_solution_cache/html/unnamed-chunk-38_487b006dddc72d6278502ffc37122145’}\nfinalsize=initialsize+1\nfinalsize\n::: {.cell-output .cell-output-stdout} [1]  2  4  6  8 10 12 :::\nnewsize=sqrt(initialsize)\nnewsize\n::: {.cell-output .cell-output-stdout} [1] 1.000000 1.732051 2.236068 2.645751 3.000000 3.316625 ::: :::\nNotice that R applied each operation to every element in the vector. Similarly, commands like initialsize-5, 2*initialsize, initialsize/10 apply subtraction, multiplication, and division to each element of the vector. The same is true for\n\ninitialsize^2\n\n[1]   1   9  25  49  81 121\n\n\nIn R the default is to apply functions and operations to vectors in an element by element (or “vectorized”) manner. This is an extremely useful propery in R.\n\n11.1.1 Functions for creating vectors\nYou can use the seq function to create a set of regularly spaced values. seq’s syntax is x=seq(from,to,by) or x=seq(from,to) or x=seq(from,to,length.out). The first form generates a vector starting with from with the last entry not extending further than than to in steps of by. In the second form the value of by is assumed to be 1 or -1, depending on whether from or to is larger. The third form creates a vector with the desired endpoints and length. The syntax from:to is a shortcut for seq(from,to):\n\n1:8\n\n[1] 1 2 3 4 5 6 7 8\n\n\n\nExercise 1. 11.1Use seq to create the vector v=(1 5 9 13), and to create a vector going from 1 to 5 in increments of 0.2.\n\n\n\n\n\nYou can use rep to create a constant vector such as (1 1 1 1); the basic syntax is rep(values,lengths). For example,\n\nrep(3,5)\n\n[1] 3 3 3 3 3\n\n\ncreates a vector in which the value 3 is repeated 5 times. rep will repeat a whole vector multiple times\n\nrep(1:3,3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\nor will repeat each of the elements in a vector a given number of times:\n\nrep(1:3,each=3)\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nEven more flexibly, you can repeat each element in the vector a different number of times:\n\nrep( c(3,4),c(2,5) )\n\n[1] 3 3 4 4 4 4 4\n\n\nThe value 3 was repeated 2 times, followed by the value 4 repeated 5 times. rep can be a little bit mind-blowing as you get started, but it will turn out to be useful.\nTable 3 lists some of the main functions for creating and working with vectors.\n\n\n\n\n\n\n\n11.1.2 Vector indexing\nYou will often want to extract a specific entry or other part of a vector. This procedure is called vector indexing, and uses square brackets ([]):\n\nz=c(1,3,5,7,9,11)\nz[3]\n\n[1] 5\n\n\nz[3] extracts the third item, or element, in the vector z. You can also access a block of elements using the functions for vector construction, e.g.\n\nz[2:5]\n\n[1] 3 5 7 9\n\n\nextracts the second through fifth elements.\nWhat happens if you enter v=z[seq(1,5,2)] ? Try it and see, and make sure you understand what happened.\nYou can extracted irregularly spaced elements of a vector. For example\n\nz[c(1,2,5)]\n\n[1] 1 3 9\n\n\nYou can also use indexing to set specific values within a vector. For example,\n\nz[1]=12\n\nchanges the value of the first entry in z while leaving all the rest alone, and\n\nz[c(1,3,5)]=c(22,33,44)\n\nchanges the first, third, and fifth values (note that we had to use c to create the vector — can you interpret the error message you get if you try z[1,3,5] ?)\n\nExercise 1. 11.2Write a one-line command to extract a vector consisting of the second, first, and third elements of z in that order.\n\n\n\n\n\n\nExercise 1. 11.3Write a script file that computes values of \\(y=\\frac{(x-1)}{(x+1)}\\) for \\(x=1,2,\\cdots,10\\), and plots \\(y\\) versus \\(x\\) with the points plotted and connected by a line hint: in ?plot, search for type).\n\n\n\n\n\nThe sum of the geometric series \\(1 + r + r^2 + r^3 + ... + r^n\\) approaches the limit \\(1/(1-r)\\) for \\(r < 1\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 1. 11.4Set the values \\(r=0.5\\) and \\(n=10\\), and then write a one-line command that creates the vector \\(G = (r^0,r^1,r^2,...,r^n)\\). Compare the sum (using sum) of this vector to the limiting value \\(1/(1-r)\\).\nRepeat for \\(n=50\\). (Note that comparing very similar numeric values can be tricky: rounding can happen, and some numbers cannot be represented exactly in binary (computer) notation. By default R displays 7~significant digits (options(\"digits\")).\n\n\n\n\n\nFor example:\n\nx = 1.999999\nx\n\n[1] 1.999999\n\nx-2\n\n[1] -1e-06\n\nx=1.9999999999999\nx\n\n[1] 2\n\nx-2\n\n[1] -9.992007e-14\n\n\nAll the digits are still there, in the second case, but they are not shown. Also note that x-2 is not exactly \\(-1 \\times 10^{-13}\\); this is unavoidable.)\n\n\n11.1.3 Logical operators\nLogical operators return a value of TRUE or FALSE. For example, try:\n\na=1\nb=3\nd=a<b\ne=(a>b)\nd\n\n[1] TRUE\n\ne\n\n[1] FALSE\n\n\nThe parentheses around (a>b) are optional but make the code easier to read. One special case where you do need parentheses (or spaces) is when you make comparisons with negative values; a<-1 will surprise you by setting a=1, because <- (representing a left-pointing arrow) is equivalent to = in R. Use a< -1, or more safely a<(-1), to make this comparison.\n\n\n\n\n\nWhen we compare two vectors or matrices of the same size, or compare a number with a vector or matrix, comparisons are done element-by-element. For example,\n\nx=1:5\nb=(x<=3)\n\nSo if x and y are vectors, then (x==y) will return a vector of values giving the element-by-element comparisons. If you want to know whether x and y are identical vectors, use identical(x,y) which returns a single value: TRUE if each entry in x equals the corresponding entry in y, otherwise FALSE. You can use ?Logical to read more about logical operators. Note the difference between = and ==\n\nExercise 1. 11.5Can you figure out what happened in the following cautionary tale below?\n\n\n\na = 1:3\nb = 2:4\na==b\n\n[1] FALSE FALSE FALSE\n\na=b\na==b\n\n[1] TRUE TRUE TRUE\n\n\n\n\n\nExclamation points ! are used in R to mean “not”; != (not ==) means “not equal to”.\nR also does arithmetic on logical values, treating TRUE as 1 and FALSE as 0. So sum(b) returns the value 3, telling us that three entries of x satisfied the condition (x<=3). This is useful for (e.g.) seeing how many of the elements of a vector are larger than a cutoff value. Build more complicated conditions by using logical operators to combine comparisons:\n\n!: Negation\n&, &&: AND\n|, ||: OR\n\nOR is non-exclusive, meaning that x|y is true if either x or y or both are true (a ham-and-cheese sandwich would satisfy the condition “ham OR cheese”). For example, try\n\na=c(1,2,3,4)\nb=c(1,1,5,5)\n(a<b)& (a>3)\n\n[1] FALSE FALSE FALSE  TRUE\n\n(a<b) | (a>3)\n\n[1] FALSE FALSE  TRUE  TRUE\n\n\nand make sure you understand what happened. If it’s confusing, try breaking up the expression and looking at the results of a<b and a>3 separately first. The two forms of AND and OR differ in how they handle vectors. The shorter one does element-by-element comparisons; the longer one only looks at the first element in each vector.\nWe can also use logical vectors (lists of TRUE and FALSE values) to pick elements out of vectors. This is important, e.g., for subsetting data (getting rid of those pesky outliers!)\nAs a simple example, we might want to focus on just the low-light values of \\(r_{max}\\) in the Chlorella example:\n\nX=read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight=X[,1]\nrmax=X[,2]\nlowLight = Light[Light<50]\nlowLightrmax = rmax[Light<50]\nlowLight\n\n[1] 20 20 20 20 21 24 44\n\nlowLightrmax\n\n[1] 1.73 1.65 2.02 1.89 2.61 1.36 2.37\n\n\nWhat is really happening here (think about it for a minute) is that Light<50 generates a logical vector the same length as Light (TRUE TRUE TRUE ...) which is then used to select the appropriate values.\nIf you want the positions at which Light is lower than 50, you could say (1:length(Light))[Light<50], but you can also use a built-in function: which(Light<50). If you wanted the position at which the maximum value of Light occurs, you could say which(Light==max(Light)). (This normally results in a vector of length 1; when could it give a longer vector?) There is even a built-in command for this specific function, which.max (although which.max always returns just the first position at which the maximum occurs).\n\nExercise 1. 11.6What would happen if instead of setting lowLight you replaced Light by saying Light=Light[Light<50], and then rmax=rmax[Light<50]?\nWhy would that be wrong?\nTry it with some temporary variables — set Light2=Light and rmax2=rmax and then play with Light2 and rmax2 so you dont mess up your working variables — and work out what happened …\n\n\n\n\n\nWe can also combine logical operators (making sure to use the element-by-element & and | versions of AND and OR):\n\nLight[Light<50 & rmax <= 2.0]\n\n[1] 20 20 20 24\n\nrmax[Light<50 & rmax <= 2.0]\n\n[1] 1.73 1.65 1.89 1.36\n\n\n\nExercise 1. 11.7runif(n) is a function (more on it soon) that generates a vector of n random, uniformly distributed numbers between 0 and 1. Create a vector of 20 numbers, then select the subset of those numbers that is less than the mean. (If you want your answers to match mine exactly, use set.seed(273) to set the random-number generator to a particular starting point before you use runif. [273 is an arbitrary number I chose].)\n\n\n\n\n\n\nExercise 1. 11.8Find the positions of the elements that are less than the mean of the vector you just created (e.g. if your vector were (0.1 0.9. 0.7 0.3) the answer would be (1 4)).\n\n\n\n\n\nAs I mentioned in passing above, vectors can have names associated with their elements: if they do, you can also extract elements by name (use names to find out the names).\n\nx = c(first=7,second=5,third=2)\nnames(x)\n\n[1] \"first\"  \"second\" \"third\" \n\nx[\"first\"]\n\nfirst \n    7 \n\nx[c(\"third\",\"first\")]\n\nthird first \n    2     7 \n\n\nFinally, it is sometimes handy to be able to drop a particular set of elements, rather than taking a particular set: you can do this with negative indices. For example, x[-1] extracts all but the first element of a vector.\n\nExercise 1. 11.9Specify two ways to take only the elements in the odd positions (first, third, …) of a vector of arbitrary length."
  },
  {
    "objectID": "Lab1/no_solution.html#matrices",
    "href": "Lab1/no_solution.html#matrices",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.2 Matrices",
    "text": "11.2 Matrices\n\n11.2.1 Creating matrices\nA matrix is a two-dimensional array, and has the same kind of variables in every column. You can create matrices of numbers by creating a vector of the matrix entries, and then reshaping them to the desired number of rows and columns using the function matrix. For example\n\n(X=matrix(1:6,nrow=2,ncol=3))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\ntakes the values 1 to 6 and reshapes them into a 2 by 3 matrix.\nBy default R loads the values into the matrix column-wise (this is probably counter-intuitive since we’re used to reading tables row-wise). Use the optional parameter byrow to change this behavior. For example:\n\n(A=matrix(1:9,nrow=3,ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nR will re-cycle through entries in the data vector, if necessary to fill a matrix of the specified size. So for example\n\nmatrix(1,nrow=10,ncol=10)\n\ncreates a \\(10 \\times 10\\) matrix of ones.\n\nExercise 1. 11.10Use a command of the form X=matrix(v,nrow=2,ncol=4) where v is a data vector, to create the following matrix X:\n\n\n\n\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n\n\nIf you can, try to use R commands to construct the vector rather than typing out all of the individual values.\nR will also collapse a matrix to behave like a vector whenever it makes sense: for example sum(X) above is 12.\n\nExercise 1. 11.11Use rnorm (which is like runif, but generates Gaussian (normally distributed) numbers with a specified mean and standard deviation instead) and matrix to create a \\(5 \\times 7\\) matrix of Gaussian random numbers with mean 1 and standard deviation 2. (Use set.seed(273) again for consistency.)\n\n\n\n\n\nAnother useful function for creating matrices is diag. diag(v,n) creates an \\(n \\times n\\) matrix with data vector \\(v\\) on its diagonal. So for example diag(1,5) creates the \\(5 \\times 5\\) identity matrix, which has 1’s on the diagonal and 0 everywhere else. Try diag(1:5,5) and diag(1:2,5); why does the latter produce a warning?\nFinally, you can use the data.entry function. This function can only edit existing matrices, but for example\n\nA=matrix(0,nrow=3,ncol=4)\ndata.entry(A)\n\nwill create A as a \\(3 \\times 4\\) matrix, and then call up a spreadsheet-like interface in which you can change the values to whatever you need.\n\n\n\n\n\n\n\n11.2.2 cbind and rbind\nIf their sizes match, you can combine vectors to form matrices, and stick matrices together with vectors or other matrices. cbind (“column bind”) and rbind (“row bind”) are the functions to use.\ncbind binds together columns of two objects. One thing it can do is put vectors together to form a matrix:\n\n(C=cbind(1:3,4:6,5:7))\n\n     [,1] [,2] [,3]\n[1,]    1    4    5\n[2,]    2    5    6\n[3,]    3    6    7\n\n\nR interprets vectors as row or column vectors according to what you’re doing with them. Here it treats them as column vectors so that columns exist to be bound together. On the other hand,\n\n(D=rbind(1:3,4:6))\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\ntreats them as rows. Now we have two matrices that can be combined.\n\nExercise 1. 11.12Verify that rbind(C,D) works, cbind(C,C) works, but cbind(C,D) doesn’t. Why not?\n\n\n\n\n\n\n\n11.2.3 Matrix indexing\nMatrix indexing is like vector indexing except that you have to specify both the row and column, or range of rows and columns. For example z=A[2,3] sets z equal to 6, which is the (2nd row, 3rd column) entry of the matrix A that you recently created, and\n\nA[2,2:3]\n\n[1] 5 6\n\n(B=A[2:3,1:2])\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    7    8\n\n\nThere is an easy shortcut to extract entire rows or columns: leave out the limits, leaving a blank before or after the comma.\n\n(first.row=A[1,])\n\n[1] 1 2 3\n\n(second.column=A[,2])\n\n[1] 2 5 8\n\n\n(What does A[,] do?)\nAs with vectors, indexing also works in reverse for assigning values to matrix entries. For example,\n\n(A[1,1]=12)\n\n[1] 12\n\n\nYou can do the same with blocks, rows, or columns, for example\n\n(A[1,]=c(2,4,5))\n\n[1] 2 4 5\n\n\nIf you use which on a matrix, R will normally treat the matrix as a vector — so for example which(A==8) will give the answer 6 (figure out why). However, which does have an option that will treat its argument as a matrix:\n\nwhich(A==8,arr.ind=TRUE)\n\n     row col\n[1,]   3   2"
  },
  {
    "objectID": "Lab1/no_solution.html#lists",
    "href": "Lab1/no_solution.html#lists",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.3 Lists",
    "text": "11.3 Lists\nWhile vectors and matrices may seem familiar, lists are probably new to you. Vectors and matrices have to contain elements that are all the same type: lists in R can contain anything — vectors, matrices, other lists … Indexing lists is a little different too: use double square brackets [[ ]] (rather than single square brackets as for a vector) to extract an element of a list by number or name, or $ to extract an element by name (only). Given a list like this:\n\nL = list(A=x,B=y,C=c(\"a\",\"b\",\"c\"))\n\nThen L$A, L[[\"A\"]], and L[[1]] will all grab the first element of the list.\nYou won’t use lists too much at the beginning, but many of R’s own results are structured in the form of lists."
  },
  {
    "objectID": "Lab1/no_solution.html#data-frames",
    "href": "Lab1/no_solution.html#data-frames",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.4 Data frames",
    "text": "11.4 Data frames\nData frames are the solution to the problem that most data sets have several different kinds of variables observed for each sample (e.g. categorical site location and continuous rainfall), but matrices can only contain a single type of data. Data frames are a hybrid of lists and vectors; internally, they are a list of vectors that may be of different types but must all be the same length, but you can do most of the same things with them (e.g., extracting a subset of rows) that you can do with matrices. You can index them either the way you would index a list, using [[ ]] or $ — where each variable is a different item in the list — or the way you would index a matrix. Use as.matrix if you have a data frame (where all variables are the same type) that you really want to be a matrix, e.g. if you need to transpose it (use as.data.frame to go the other way)."
  },
  {
    "objectID": "Lab1/solution.html",
    "href": "Lab1/solution.html",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "",
    "text": "The aim of this tutorial is to learn the basics of R. After completing the tutorial you will be able to:\n\nInstall R and associated packages and Rstudio\nDo interactive calculations in the R console\nConsult the built-in help in R\nCreate, modify and run R scripts\nSetup a typical workflow in R (loading libraries, reading data, doing statistics and saving results)\nWork with the most common data types in R (vectors, matrices, lists and data frames)"
  },
  {
    "objectID": "Lab1/solution.html#vectors",
    "href": "Lab1/solution.html#vectors",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.1 Vectors",
    "text": "11.1 Vectors\nAn important class of data types are vectors and matrices (1- and 2-dimensional rectangular arrays of numbers). Operations with vectors and matrices may seem a bit abstract now, but we need them to do useful things later. The only properties of vectors are their type (or class) and length, although they can also have an associated list of names.\nWe’ve already seen two ways to create vectors in R:\n\nA command in the console window or a script file listing the values, such as ::: {.cell hash=‘solution_cache/html/unnamed-chunk-36_88e7ad89efcaaf140865787e562b6e38’}\n\ninitialsize=c(1,3,5,7,9,11)\n:::\n\nUsing read.table: ::: {.cell hash=‘solution_cache/html/unnamed-chunk-37_badf1b95ba3ccaa5b058792790e1a9be’}\n\ninitialsize=read.table(\"c:/temp/initialdata.txt\")\n::: (assuming of course that the file exists in the right place).\nYou can then use a vector in calculations as if it were a number (more or less) ::: {.cell hash=‘solution_cache/html/unnamed-chunk-38_c28c9f885b56f54969a64204a5b36a0d’}\nfinalsize=initialsize+1\nfinalsize\n::: {.cell-output .cell-output-stdout} [1]  2  4  6  8 10 12 :::\nnewsize=sqrt(initialsize)\nnewsize\n::: {.cell-output .cell-output-stdout} [1] 1.000000 1.732051 2.236068 2.645751 3.000000 3.316625 ::: :::\nNotice that R applied each operation to every element in the vector. Similarly, commands like initialsize-5, 2*initialsize, initialsize/10 apply subtraction, multiplication, and division to each element of the vector. The same is true for ::: {.cell hash=‘solution_cache/html/unnamed-chunk-39_7b5a8aa987a194f62996c9adf26a19fa’}\ninitialsize^2\n\n[1]   1   9  25  49  81 121\n\n:::\nIn R the default is to apply functions and operations to vectors in an element by element (or “vectorized”) manner. This is an extremely useful propery in R.\n\n11.1.1 Functions for creating vectors\nYou can use the seq function to create a set of regularly spaced values. seq‘s syntax is x=seq(from,to,by) or x=seq(from,to) or x=seq(from,to,length.out). The first form generates a vector starting with from with the last entry not extending further than than to in steps of by. In the second form the value of by is assumed to be 1 or -1, depending on whether from or to is larger. The third form creates a vector with the desired endpoints and length. The syntax from:to is a shortcut for seq(from,to): ::: {.cell hash=’solution_cache/html/unnamed-chunk-40_d6a02803966fc9744207b1ca68fbee4c’}\n1:8\n\n[1] 1 2 3 4 5 6 7 8\n\n:::\n\nExercise 1. 11.1Use seq to create the vector v=(1 5 9 13), and to create a vector going from 1 to 5 in increments of 0.2.\n\n\n\n\nSolution\n    seq(1,13,4)\n    seq(1,5,0.2)\n\n\n\nYou can use rep to create a constant vector such as (1 1 1 1); the basic syntax is rep(values,lengths). For example,\n\nrep(3,5)\n\n[1] 3 3 3 3 3\n\n\ncreates a vector in which the value 3 is repeated 5 times. rep will repeat a whole vector multiple times\n\nrep(1:3,3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\nor will repeat each of the elements in a vector a given number of times:\n\nrep(1:3,each=3)\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nEven more flexibly, you can repeat each element in the vector a different number of times:\n\nrep( c(3,4),c(2,5) )\n\n[1] 3 3 4 4 4 4 4\n\n\nThe value 3 was repeated 2 times, followed by the value 4 repeated 5 times. rep can be a little bit mind-blowing as you get started, but it will turn out to be useful.\nTable 3 lists some of the main functions for creating and working with vectors.\n\n\n\n\n\n\n\n11.1.2 Vector indexing\nYou will often want to extract a specific entry or other part of a vector. This procedure is called vector indexing, and uses square brackets ([]):\n\nz=c(1,3,5,7,9,11)\nz[3]\n\n[1] 5\n\n\nz[3] extracts the third item, or element, in the vector z. You can also access a block of elements using the functions for vector construction, e.g.\n\nz[2:5]\n\n[1] 3 5 7 9\n\n\nextracts the second through fifth elements.\nWhat happens if you enter v=z[seq(1,5,2)] ? Try it and see, and make sure you understand what happened.\nYou can extracted irregularly spaced elements of a vector. For example\n\nz[c(1,2,5)]\n\n[1] 1 3 9\n\n\nYou can also use indexing to set specific values within a vector. For example,\n\nz[1]=12\n\nchanges the value of the first entry in z while leaving all the rest alone, and\n\nz[c(1,3,5)]=c(22,33,44)\n\nchanges the first, third, and fifth values (note that we had to use c to create the vector — can you interpret the error message you get if you try z[1,3,5] ?)\n\nExercise 1. 11.2Write a one-line command to extract a vector consisting of the second, first, and third elements of z in that order.\n\n\n\n\nSolution\n    z=c(1,3,5,7,9,11)\n    z[c(2,1,3)]\n\n\n\n\nExercise 1. 11.3Write a script file that computes values of \\(y=\\frac{(x-1)}{(x+1)}\\) for \\(x=1,2,\\cdots,10\\), and plots \\(y\\) versus \\(x\\) with the points plotted and connected by a line hint: in ?plot, search for type).\n\n\n\n\nSolution\n    x = c(1:10)\n    y = (x-1)/(x+1)\n    plot(y~x,type=\"b\")\n\n\n\nThe sum of the geometric series \\(1 + r + r^2 + r^3 + ... + r^n\\) approaches the limit \\(1/(1-r)\\) for \\(r < 1\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 1. 11.4Set the values \\(r=0.5\\) and \\(n=10\\), and then write a one-line command that creates the vector \\(G = (r^0,r^1,r^2,...,r^n)\\). Compare the sum (using sum) of this vector to the limiting value \\(1/(1-r)\\).\nRepeat for \\(n=50\\). (Note that comparing very similar numeric values can be tricky: rounding can happen, and some numbers cannot be represented exactly in binary (computer) notation. By default R displays 7~significant digits (options(\"digits\")).\n\n\n\n\nSolution\n    r = 0.5\n    n =10\n    sum(r^c(1:n))\n    n =50\n    sum(r^c(1:n))\n\n\n\nFor example:\n\nx = 1.999999\nx\n\n[1] 1.999999\n\nx-2\n\n[1] -1e-06\n\nx=1.9999999999999\nx\n\n[1] 2\n\nx-2\n\n[1] -9.992007e-14\n\n\nAll the digits are still there, in the second case, but they are not shown. Also note that x-2 is not exactly \\(-1 \\times 10^{-13}\\); this is unavoidable.)\n\n\n11.1.3 Logical operators\nLogical operators return a value of TRUE or FALSE. For example, try:\n\na=1\nb=3\nd=a<b\ne=(a>b)\nd\n\n[1] TRUE\n\ne\n\n[1] FALSE\n\n\nThe parentheses around (a>b) are optional but make the code easier to read. One special case where you do need parentheses (or spaces) is when you make comparisons with negative values; a<-1 will surprise you by setting a=1, because <- (representing a left-pointing arrow) is equivalent to = in R. Use a< -1, or more safely a<(-1), to make this comparison.\n\n\n\n\n\nWhen we compare two vectors or matrices of the same size, or compare a number with a vector or matrix, comparisons are done element-by-element. For example,\n\nx=1:5\nb=(x<=3)\n\nSo if x and y are vectors, then (x==y) will return a vector of values giving the element-by-element comparisons. If you want to know whether x and y are identical vectors, use identical(x,y) which returns a single value: TRUE if each entry in x equals the corresponding entry in y, otherwise FALSE. You can use ?Logical to read more about logical operators. Note the difference between = and ==\n\nExercise 1. 11.5Can you figure out what happened in the following cautionary tale below?\n\n\n\na = 1:3\nb = 2:4\na==b\n\n[1] FALSE FALSE FALSE\n\na=b\na==b\n\n[1] TRUE TRUE TRUE\n\n\n\n\nSolutionYou overwrote a by b in the command a=b and so the last statement confirms that they are identical\n\n\n\nExclamation points ! are used in R to mean “not”; != (not ==) means “not equal to”.\nR also does arithmetic on logical values, treating TRUE as 1 and FALSE as 0. So sum(b) returns the value 3, telling us that three entries of x satisfied the condition (x<=3). This is useful for (e.g.) seeing how many of the elements of a vector are larger than a cutoff value. Build more complicated conditions by using logical operators to combine comparisons:\n\n!: Negation\n&, &&: AND\n|, ||: OR\n\nOR is non-exclusive, meaning that x|y is true if either x or y or both are true (a ham-and-cheese sandwich would satisfy the condition “ham OR cheese”). For example, try\n\na=c(1,2,3,4)\nb=c(1,1,5,5)\n(a<b)& (a>3)\n\n[1] FALSE FALSE FALSE  TRUE\n\n(a<b) | (a>3)\n\n[1] FALSE FALSE  TRUE  TRUE\n\n\nand make sure you understand what happened. If it’s confusing, try breaking up the expression and looking at the results of a<b and a>3 separately first. The two forms of AND and OR differ in how they handle vectors. The shorter one does element-by-element comparisons; the longer one only looks at the first element in each vector.\nWe can also use logical vectors (lists of TRUE and FALSE values) to pick elements out of vectors. This is important, e.g., for subsetting data (getting rid of those pesky outliers!)\nAs a simple example, we might want to focus on just the low-light values of \\(r_{max}\\) in the Chlorella example:\n\nX=read.table(\"ChlorellaGrowth.txt\",header=TRUE)\nLight=X[,1]\nrmax=X[,2]\nlowLight = Light[Light<50]\nlowLightrmax = rmax[Light<50]\nlowLight\n\n[1] 20 20 20 20 21 24 44\n\nlowLightrmax\n\n[1] 1.73 1.65 2.02 1.89 2.61 1.36 2.37\n\n\nWhat is really happening here (think about it for a minute) is that Light<50 generates a logical vector the same length as Light (TRUE TRUE TRUE ...) which is then used to select the appropriate values.\nIf you want the positions at which Light is lower than 50, you could say (1:length(Light))[Light<50], but you can also use a built-in function: which(Light<50). If you wanted the position at which the maximum value of Light occurs, you could say which(Light==max(Light)). (This normally results in a vector of length 1; when could it give a longer vector?) There is even a built-in command for this specific function, which.max (although which.max always returns just the first position at which the maximum occurs).\n\nExercise 1. 11.6What would happen if instead of setting lowLight you replaced Light by saying Light=Light[Light<50], and then rmax=rmax[Light<50]?\nWhy would that be wrong?\nTry it with some temporary variables — set Light2=Light and rmax2=rmax and then play with Light2 and rmax2 so you dont mess up your working variables — and work out what happened …\n\n\n\n\nSolution\n    Light[Light<50 & rmax <= 2.0]\n    rmax[Light<50 & rmax <= 2.0]\n\n\n\nWe can also combine logical operators (making sure to use the element-by-element & and | versions of AND and OR):\n\nLight[Light<50 & rmax <= 2.0]\n\n[1] 20 20 20 24\n\nrmax[Light<50 & rmax <= 2.0]\n\n[1] 1.73 1.65 1.89 1.36\n\n\n\nExercise 1. 11.7runif(n) is a function (more on it soon) that generates a vector of n random, uniformly distributed numbers between 0 and 1. Create a vector of 20 numbers, then select the subset of those numbers that is less than the mean. (If you want your answers to match mine exactly, use set.seed(273) to set the random-number generator to a particular starting point before you use runif. [273 is an arbitrary number I chose].)\n\n\n\n\nSolution\n    a = runif(20)\n    a[a < mean(a)]\n\n\n\n\nExercise 1. 11.8Find the positions of the elements that are less than the mean of the vector you just created (e.g. if your vector were (0.1 0.9. 0.7 0.3) the answer would be (1 4)).\n\n\n\n\nSolution\n    which(a < mean(a))\n\n\n\nAs I mentioned in passing above, vectors can have names associated with their elements: if they do, you can also extract elements by name (use names to find out the names).\n\nx = c(first=7,second=5,third=2)\nnames(x)\n\n[1] \"first\"  \"second\" \"third\" \n\nx[\"first\"]\n\nfirst \n    7 \n\nx[c(\"third\",\"first\")]\n\nthird first \n    2     7 \n\n\nFinally, it is sometimes handy to be able to drop a particular set of elements, rather than taking a particular set: you can do this with negative indices. For example, x[-1] extracts all but the first element of a vector.\n\nExercise 1. 11.9Specify two ways to take only the elements in the odd positions (first, third, …) of a vector of arbitrary length.\n\n\n\n\nSolution\n    n = 20\n    a = c(1:n)\n    a1 = a[seq(1,n,2)]\n    a2 = a[-seq(2,n,2)]"
  },
  {
    "objectID": "Lab1/solution.html#matrices",
    "href": "Lab1/solution.html#matrices",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.2 Matrices",
    "text": "11.2 Matrices\n\n11.2.1 Creating matrices\nA matrix is a two-dimensional array, and has the same kind of variables in every column. You can create matrices of numbers by creating a vector of the matrix entries, and then reshaping them to the desired number of rows and columns using the function matrix. For example\n\n(X=matrix(1:6,nrow=2,ncol=3))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\ntakes the values 1 to 6 and reshapes them into a 2 by 3 matrix.\nBy default R loads the values into the matrix column-wise (this is probably counter-intuitive since we’re used to reading tables row-wise). Use the optional parameter byrow to change this behavior. For example:\n\n(A=matrix(1:9,nrow=3,ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nR will re-cycle through entries in the data vector, if necessary to fill a matrix of the specified size. So for example\n\nmatrix(1,nrow=10,ncol=10)\n\ncreates a \\(10 \\times 10\\) matrix of ones.\n\nExercise 1. 11.10Use a command of the form X=matrix(v,nrow=2,ncol=4) where v is a data vector, to create the following matrix X:\n\n\n\n\nSolution\n    matrix(rep(1:2,4),nrow=2) \n    \n\n\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n\n\nIf you can, try to use R commands to construct the vector rather than typing out all of the individual values.\nR will also collapse a matrix to behave like a vector whenever it makes sense: for example sum(X) above is 12.\n\nExercise 1. 11.11Use rnorm (which is like runif, but generates Gaussian (normally distributed) numbers with a specified mean and standard deviation instead) and matrix to create a \\(5 \\times 7\\) matrix of Gaussian random numbers with mean 1 and standard deviation 2. (Use set.seed(273) again for consistency.)\n\n\n\n\nSolution\n    set.seed(273)\n    matrix(rnorm(35,0,2),5,7)\n\n\n\nAnother useful function for creating matrices is diag. diag(v,n) creates an \\(n \\times n\\) matrix with data vector \\(v\\) on its diagonal. So for example diag(1,5) creates the \\(5 \\times 5\\) identity matrix, which has 1’s on the diagonal and 0 everywhere else. Try diag(1:5,5) and diag(1:2,5); why does the latter produce a warning?\nFinally, you can use the data.entry function. This function can only edit existing matrices, but for example\n\nA=matrix(0,nrow=3,ncol=4)\ndata.entry(A)\n\nwill create A as a \\(3 \\times 4\\) matrix, and then call up a spreadsheet-like interface in which you can change the values to whatever you need.\n\n\n\n\n\n\n\n11.2.2 cbind and rbind\nIf their sizes match, you can combine vectors to form matrices, and stick matrices together with vectors or other matrices. cbind (“column bind”) and rbind (“row bind”) are the functions to use.\ncbind binds together columns of two objects. One thing it can do is put vectors together to form a matrix:\n\n(C=cbind(1:3,4:6,5:7))\n\n     [,1] [,2] [,3]\n[1,]    1    4    5\n[2,]    2    5    6\n[3,]    3    6    7\n\n\nR interprets vectors as row or column vectors according to what you’re doing with them. Here it treats them as column vectors so that columns exist to be bound together. On the other hand,\n\n(D=rbind(1:3,4:6))\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\ntreats them as rows. Now we have two matrices that can be combined.\n\nExercise 1. 11.12Verify that rbind(C,D) works, cbind(C,C) works, but cbind(C,D) doesn’t. Why not?\n\n\n\n\nSolution\n    C=cbind(1:3,4:6,5:7)\n    D=rbind(1:3,4:6)\n    rbind(C,D)\n    cbind(C,D) \n\n\n\n\n\n11.2.3 Matrix indexing\nMatrix indexing is like vector indexing except that you have to specify both the row and column, or range of rows and columns. For example z=A[2,3] sets z equal to 6, which is the (2nd row, 3rd column) entry of the matrix A that you recently created, and\n\nA[2,2:3]\n\n[1] 5 6\n\n(B=A[2:3,1:2])\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    7    8\n\n\nThere is an easy shortcut to extract entire rows or columns: leave out the limits, leaving a blank before or after the comma.\n\n(first.row=A[1,])\n\n[1] 1 2 3\n\n(second.column=A[,2])\n\n[1] 2 5 8\n\n\n(What does A[,] do?)\nAs with vectors, indexing also works in reverse for assigning values to matrix entries. For example,\n\n(A[1,1]=12)\n\n[1] 12\n\n\nYou can do the same with blocks, rows, or columns, for example\n\n(A[1,]=c(2,4,5))\n\n[1] 2 4 5\n\n\nIf you use which on a matrix, R will normally treat the matrix as a vector — so for example which(A==8) will give the answer 6 (figure out why). However, which does have an option that will treat its argument as a matrix:\n\nwhich(A==8,arr.ind=TRUE)\n\n     row col\n[1,]   3   2"
  },
  {
    "objectID": "Lab1/solution.html#lists",
    "href": "Lab1/solution.html#lists",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.3 Lists",
    "text": "11.3 Lists\nWhile vectors and matrices may seem familiar, lists are probably new to you. Vectors and matrices have to contain elements that are all the same type: lists in R can contain anything — vectors, matrices, other lists … Indexing lists is a little different too: use double square brackets [[ ]] (rather than single square brackets as for a vector) to extract an element of a list by number or name, or $ to extract an element by name (only). Given a list like this:\n\nL = list(A=x,B=y,C=c(\"a\",\"b\",\"c\"))\n\nThen L$A, L[[\"A\"]], and L[[1]] will all grab the first element of the list.\nYou won’t use lists too much at the beginning, but many of R’s own results are structured in the form of lists."
  },
  {
    "objectID": "Lab1/solution.html#data-frames",
    "href": "Lab1/solution.html#data-frames",
    "title": "An introduction to R for ecological modeling (lab 1)",
    "section": "11.4 Data frames",
    "text": "11.4 Data frames\nData frames are the solution to the problem that most data sets have several different kinds of variables observed for each sample (e.g. categorical site location and continuous rainfall), but matrices can only contain a single type of data. Data frames are a hybrid of lists and vectors; internally, they are a list of vectors that may be of different types but must all be the same length, but you can do most of the same things with them (e.g., extracting a subset of rows) that you can do with matrices. You can index them either the way you would index a list, using [[ ]] or $ — where each variable is a different item in the list — or the way you would index a matrix. Use as.matrix if you have a data frame (where all variables are the same type) that you really want to be a matrix, e.g. if you need to transpose it (use as.data.frame to go the other way)."
  },
  {
    "objectID": "Lab2/index.html#reading-data",
    "href": "Lab2/index.html#reading-data",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "2.1 Reading data",
    "text": "2.1 Reading data\nFind the file called seedpred.dat. It is in the right format (plain text, long format), so you can just read it in with\n\ndata = read.table(\"seedpred.dat\", header = TRUE)\n\nAdd the variable available to the data frame by combining taken and remaining (using the $ symbol):\n\ndata$available = data$taken + data$remaining\n\nPitfall #1: finding your file If R responds to your read.table() or read.csv() command with an error like\nError in file(file, \"r\") : unable to open connection In addition: Warning message: cannot open file 'myfile.csv'\nit means it can’t find your file, probably because it isn’t looking in the right place. By default, R’s working directory is the directory in which the R program starts up, which is by default something like C:/Program Files/R/rw2010/bin. (R uses / as the [operating-system-independent] separator between directories in a file path.) If you are using Rstudio, go to ‘Session’ and click ‘Set working directory’. You can also use the setwd() command to set the working directory. getwd() tells you what the current working directory is. While you could just throw everything on your desktop, it’s good to get in the habit of setting up a separate working directory for different projects, so that your data files, metadata files, R script files, and so forth, are all in the same place. Depending on how you have gotten your data files onto your system (e.g. by downloading them from the web), Windows will sometimes hide or otherwise screw up the extension of your file (e.g. adding .txt to a file called mydata.dat). R needs to know the full name of the file, including the extension.\nFor example to set a working directory:\n\nsetwd(\"D:/Bolker/labs/\")\n\nPitfall #2: checking number of fields In some cases the number of fields is not the same for every line in your data file. In that case you may get an error like:\nError in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names\nor\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 1 did not have 5 elements\nIf you need to check on the number of fields that R thinks you have on each line, use\n\ncount.fields(\"myfile.dat\",sep=\",\")\n\n(you can omit the sep=\",\" argument if you have whitespace- rather than comma delimited data). If you are checking a long data file you can try\n\ncf = count.fields(\"myfile.dat\",sep=\",\")\nwhich(cf!=cf[1])\n\nto get the line numbers with numbers of fields different from the first line. By default R will try to fill in what it sees as missing fields with NA (“not available”) values; this can be useful but can also hide errors. You can try\n\nmydata <- read.csv(\"myfile.dat\", fill = FALSE)\n\nto turn off this behavior; if you don’t have any missing fields at the end of lines in your data this should work.\nIf your file is a comma separated file, you can also use read.csv. This function has set some arguments to default, e.g. the seperator is a comma in this case (sep=\",\")\nPitfall #3: List separator It may happen when you save a file in excel as .csv that the decimals are not indicated by a dot . but by a comma , and that the list separator is a semi-colon. To avoid this happen change the regional settings in your computer (control panel) to American or English, or change the list separator and the decimal symbol manually."
  },
  {
    "objectID": "Lab2/index.html#checking-data",
    "href": "Lab2/index.html#checking-data",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "2.2 Checking data",
    "text": "2.2 Checking data\nR will automatically recognize the type of data of the columns that are read in, but sometimes it goes wrong. To check that all your variables have been classified correctly:\n\nsapply(data, class)\n\n    species        tcum        tint   remaining       taken   available \n\"character\"   \"integer\"   \"integer\"   \"integer\"   \"integer\"   \"integer\" \n\n\nThis applies the class() command, which identifies the type of a variable, to each column in your data. Alternatively,\n\nsummary(data)\n\ncan be very helplful to get a glance of the characteristics of the data.\nNon-numeric missing-variable strings (such as a star, *) will also make R misclassify. Use na.strings in your read.table() command:\n\nmydata <- read.table(\"mydata.dat\", na.strings = \"*\")\n\nYou can specify more than one value with (e.g.) na.strings=c(“”,”**”,“bad”,“-9999”).\n\n\n\nTry out head(), summary() and str() on data; make sure you understand the results.\n\n\n\n\n\nhead() gives the first five rows of your data and are used to inspect whether the read-in procedure went ok. For example, you can see whether the header is really a header or whether is was read-in as the first data row.\nsummary() gives you a summary view of your data.frame, reporting the mean and quantiles of your data, and numbers of NAs.\nstr() shows you the structure and data types that are in the data.frame or other R objects. str() can be convenient to check if the data types are as you want to have them, or to extract information from complex R objects such as a model object fitted with lm."
  },
  {
    "objectID": "Lab2/index.html#reshaping-data",
    "href": "Lab2/index.html#reshaping-data",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "2.3 Reshaping data",
    "text": "2.3 Reshaping data\nReshaping a dataframe is an important part of data exploration. For example, for making field recordings it is convenient to have the measurements of different plots or transects in different columns on your sheet. This is called a wide format. However, for data analysis in R (and other statistical programs) you need to have all measurements in one column, with an additional column indicating which plot a measurement belongs to (so called long format).\nBelow we create a dataframe in long format and reshape it. Here are the commands to generate the data frame I used as an example in the text (I use LETTERS, a built-in vector of the capitalized letters of the alphabet, and runif(), which picks a specified number of random numbers from a uniform distribution between 0 and 1. The command round(x,3) rounds x to 3 digits after the decimal place.):\n\nloc = factor(rep(LETTERS[1:3],2))\nday = factor(rep(1:2,each=3))\nval = round(runif(6),3)\nd = data.frame(loc,day,val)\n\nThis data set is stored in long format. To go to wide format, we first need to (install and) load the library reshape2. In Lab 1 you learned how to install packages. You can load a package by library() or require(). Thus to use an additional package it must be (i) installed on your machine (with install.packages()) or through the menu system and (ii) loaded in your current R session (with library()):\n\nlibrary(reshape2)  \n\nWarning: package 'reshape2' was built under R version 4.3.1\n\nd2 = dcast(d,loc~day )\n\nUsing val as value column: use value.var to override.\n\nd2\n\n  loc     1     2\n1   A 0.922 0.027\n2   B 0.065 0.511\n3   C 0.708 0.856\n\n\nloc~day specifies that loc will be used as rows and day will be put into columns.\nTo go back to long format, we simply write:\n\nmelt(d2, variable.name=\"day\")\n\nBy specifying the variable.name to day, we put the columns (1 and 2) into a new column called day. The column in which the values are stored can be set by using the argument value.name\n\nmelt(d2, variable.name=\"day\",value.name=\"val\")\n\nUsing loc as id variables\n\n\n  loc day   val\n1   A   1 0.922\n2   B   1 0.065\n3   C   1 0.708\n4   A   2 0.027\n5   B   2 0.511\n6   C   2 0.856\n\n\n\n\n\nMake a new data.frame similar to the previous data.frame d, but with an extra column month consisting of two levels. The value of month should be 1 for the first records and 2 for the second. Next, reshape to wide format and back to long format. The long format is what we commonly use in statistics.\n\n\n\n\n\nloc = factor(rep(LETTERS[1:3],4))\nmonth = factor(rep(1:2,each=6))\nday = factor(rep(1:2,each=3))\nval = round(runif(12),3)\nd1 = data.frame(loc,month,day,val)\nd3 = dcast(d1,loc~month+day)\nmelt(d3,variable.name=\"month_day\")"
  },
  {
    "objectID": "Lab2/index.html#advanced-data-types-time-permitting",
    "href": "Lab2/index.html#advanced-data-types-time-permitting",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "2.4 Advanced data types (Time permitting)",
    "text": "2.4 Advanced data types (Time permitting)\nWhile you can usually get away by coding data in not quite the right way - for example, coding dates as numeric values or categorical variables as strings - R tries to “do the right thing” with your data, and it is more likely to do the right thing the more it knows about how your data are structured.\nFactors instead of strings\nSometimes R’s default of assigning characters is not what you want: if your strings are unique identifiers and you want to make it a factor, the default read.table will convert this into characters. If all of your non-numeric variables should be treated as factor strings rather than characters, you can just specify as.is=FALSE.\nFactors instead of numeric values\nSometimes you have numeric labels for data that are really categorical values - for example if your sites or species have integer codes (often data sets will have redundant information in them, e.g. both a species name and a species code number). It’s best to specify appropriate data types, so use colClasses to force R to treat the data as a factor. For example, if we wanted to make tcum a factor instead of a numeric variable:\n\ndata2 = read.table(\"seedpred.dat\", header = TRUE, colClasses = c(rep(\"factor\",\n2), rep(\"numeric\", 3)))\nsapply(data2, class)\n\nn.b.: by default, R sets the order of the factor levels alphabetically. You can find out the levels and their order in a factor f with levels(f). If you want your levels ordered in some other way (e.g. site names in order along some transect), you need to specify this explicitly. Most confusingly, R will sort strings in alphabetic order too, even if they represent numbers.\nThis is OK:\n\nf = factor(1:10)\nlevels(f)\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\nHowever, if we create a factor f through:\n\nf = factor(as.character(1:10))\nlevels(f)\n\n [1] \"1\"  \"10\" \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\" \n\n\nit will put the 10-th level as second. You can fix the levels by using the levels argument in factor() to tell R explicitly what you want it to do, e.g.:\n\nf = factor(as.character(1:10), levels = c(1:10))\n\nSo the levels=1:10 argument explicitly states that there are ten levels and that the order of these levels is 1,2,3,4,5,6,7,8,9,10. The levels argument needs a vector of unique numeric values or character strings (c(1:10)).\nAdditionally, if you create a factor with levels ‘north’, ‘middle’ and ‘south’ they will be sorted by alphabet\n\nx = c(\"north\", \"middle\", \"south\")\nfactor(x)\n\n[1] north  middle south \nLevels: middle north south\n\n\nIf you want to sort them geographically instead of alphabetically you again can use the levels argument. Additionally, you can add levels that were not included in the vector itself:\n\nf = factor(x, levels = c(\"far_north\", \"north\", \"middle\", \"south\"))\n\nLikewise, if your data contain a subset of integer values in a range, but you want to make sure the levels of the factor you construct include all of the values in the range, not just the ones in your data. Use levels again:\n\nf = factor(c(3, 3, 5, 6, 7, 8, 10), levels = 3:10)\n\nFinally, you may want to get rid of levels that were included in a previous factor but are no longer relevant:\n\nf = factor(c(\"a\", \"b\", \"c\", \"d\"))\nf2 = f[1:2]\nlevels(f2)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\nNote that a character vector is returned displaying the different levels in the factor f.\n\nf2 = factor(as.character(f2))\nlevels(f2)\n\n[1] \"a\" \"b\"\n\n\n\n\n\nIllustrate the effects of the levels command by plotting the factor f=factor(c(3,3,5,6,7,8,10)) as created with and without intermediate levels, i.e. with and without levels c(1:10). For an extra challenge, draw them as two side-by-side subplots. (Use par(mfrow=c(1,1)) to restore a full plot window.)\n\n\n\n\n\nf=factor(c(3,3,5,6,7,8,10))\nplot(f)\nf1=factor(c(3,3,5,6,7,8,10),levels=c(3:10))\nplot(f1)\n\n\nDates (time permitting)\nDates and times can be tricky in R, but you can handle your dates as type Date within R rather than using Julian days\nYou can use colClasses=\"Date\" within read.table() to read in dates directly from a file, but only if your dates are in four-digit-year/month/day (e.g. 2005/08/16 or 2005-08-16) format; otherwise R will either butcher your dates or complain\nError in fromchar(x) : character string is not in a standard unambiguous format\nIf your dates are in another format in a single column, read them in as character strings (colClasses=\"character\" or using as.is) and then use as.Date(), which uses a very flexible format argument to convert character formats to dates:\n\nas.Date(c(\"1jan1960\", \"2jan1960\", \"31mar1960\", \"30jul1960\"),\n        format = \"%d%b%Y\")\n\n[1] \"1960-01-01\" \"1960-01-02\" \"1960-03-31\" \"1960-07-30\"\n\n\n\nas.Date(c(\"02/27/92\", \"02/27/92\", \"01/14/92\", \"02/28/92\", \"02/01/92\"), \n        format = \"%m/%d/%y\")\n\n[1] \"1992-02-27\" \"1992-02-27\" \"1992-01-14\" \"1992-02-28\" \"1992-02-01\"\n\n\nThe most useful format codes are %m for month number, %d for day of month, %j% for Julian date (day of year), %y% for two-digit year (dangerous for dates before 1970!) and %Y% for four-digit year; see ?strftime for many more details. If you have your dates as separate (numeric) day, month, and year columns, you actually have to squash them together into a character format. This can be done with paste(), using sep=\"/\" to specify that the values should be separated by a slash and then convert them to dates:\n\nyear = c(2004,2004,2004,2005)\nmonth = c(10,11,12,1)\nday = c(20,18,28,17)\ndatestr = paste(year,month,day,sep=\"/\")\ndate = as.Date(datestr)\ndate\n\n[1] \"2004-10-20\" \"2004-11-18\" \"2004-12-28\" \"2005-01-17\"\n\n\nWhen you want to split a date to month, year and day, you can use ‘strsplit’:\n\ndate.c = as.character(date)\ndate.char = strsplit(date.c, \"-\" )\n\nWhich you subsequently can turn in to multiple colums through matrix:\n\ndat.mat = matrix(unlist(date.char), ncol=3, byrow=TRUE)\n\nAlthough R prints the dates in date out so they look like a vector of character strings, they are really dates: class(date) will give you the answer \"Date\". Note that when using the dat.mat these are characters.\nPitfall #4 quotation marks in character variables If you have character strings in your data set with apostrophes or quotation marks embedded in them, you have to get R to ignore them. I used a data set recently that contained lines like this: Western Canyon|valley|Santa Cruz|313120N|1103145WO'Donnell Canyon\nI used\n\ndata3 = read.table(\"datafile\", sep = \"|\", quote = \"\")\n\nto tell R that | was the separator between fields and that it should ignore all apostrophes/single quotations/double quotations in the data set and just read them as part of a string."
  },
  {
    "objectID": "Lab2/index.html#accessing-data",
    "href": "Lab2/index.html#accessing-data",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "2.5 Accessing data",
    "text": "2.5 Accessing data\nTo access individual variables within your data set use mydata$varname or mydata[,n] or mydata[,\"varname\"] where n is the column number and varname is the variable name you want. You can also use attach(mydata) to set things up so that you can refer to the variable names alone (e.g. varname rather than mydata$varname). However, beware: if you then modify a variable, you can end up with two copies of it: one (modified) is a local variable called varname, the other (original) is a column in the data frame called varname: it’s probably better not to attach a data set, or only until after you’ve finished cleaning and modifying it. Furthermore, if you have already created a variable called varname, R will find it before it finds the version of varname that is part of your data set. Attaching multiple copies of a data set is a good way to get confused: try to remember to detach(mydata) when you’re done.\nHere some examples to get the column with name ‘species’\n\ndata[,\"species\"]\ndata[,1]\ndata$species # recommended! You explictly define the dataframe and name of the column \n\nTo access data that are built in to R or included in an R package (which you probably won’t need to do often), say\n\ndata(dataset)\n\n(data() by itself will list all available data sets.)"
  },
  {
    "objectID": "Lab2/index.html#scatter-plot",
    "href": "Lab2/index.html#scatter-plot",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.1 Scatter plot",
    "text": "3.1 Scatter plot\nFrom the previous lab you may remember that the base plot function in R is plot. The function plot takes a number of arguments but at least you need to specify the x and the y. If you refer to x and y by the column name of a data.frame you need to specify the name of the data.frame as well through data.\n\nplot(taken ~ available,data=data)\n\n\n\n\nThe graph above may not be very useful as it does not show how many datapoints are underlying every combination of seeds taken and seeds available. The function jitter adds small noise to a numeric vector which makes that observations that have the same combination of seeds available and taken are plotted at a slightly different location.\n\nplot(jitter(taken)~jitter(available),xlab=\"Seeds available\",\n     ylab=\"Seeds taken\",data=data)"
  },
  {
    "objectID": "Lab2/index.html#bubble-plot",
    "href": "Lab2/index.html#bubble-plot",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.2 Bubble plot",
    "text": "3.2 Bubble plot\nSometimes you have multiple variables in the dataset that you want to explore simultaneously. For example, you want to superimpose information on how often certain combinations of the number seeds available versus the number of seeds taken occur. For this a bubble or jitter plot may be useful. The bubble plot is a normal plot with the size of the points representing the number of observations for a given combination of seeds available and seeds taken.\nTo get this information we first need to make a summary table which we can get through:\n\nt1 = table(data$available, data$taken)\n\nThis table needs to changed to a long format in order to be useful for the plotting command. Change the table to a long format with seeds available and seeds taken as columns. Note that we are melting a 2-d array (see ?melt.array)\n\nt2 = melt(t1,varnames=c(\"Seeds available\",\"Seeds taken\"),\n          value.name=\"val\")\n\nNow we can make a plot with the size of the bubbles proportional for the number of observations. Since the columnnames have a space in the string we should use the quotes.\n\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=val)\n\n\n\n\nThe argument cex controls the size of the points. As you see the size of the bubbles are bit too large, so we need to adjust it. In addition, we need to adjust the scaling of the axis.\n\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\n\n\n\n\nIn ggplot this can be done as follows\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n    ggplot(data=t2)+\n        geom_point(aes(x = `Seeds available`, y = `Seeds taken`, \n                       size = log(val)/2))+\n      ylab(\"Seeds taken\")+\n      xlab(\"Seeds available\")\n\nWarning in sqrt(x): NaNs produced\n\n\nWarning: Removed 10 rows containing missing values (`geom_point()`).\n\n\n\n\n    # or alternatively labs(x=\"Seeds taken\",y=\"Seeds available\")\n\nIn ggplot2 you need to specify the dataset the variables come from. You do this through data=.... Next you specify the type of plot you want. For example, a point plot can be specified through geom_point. Within geom_point, you need to specify the aesthetics (aes(x...,y...)) which determines what is plotted on the axes of the point plot. For example aes(x=x,y=y) put column x on the x-axis and column y on the y-axis.\nIf data is specified in the ggplot statement, it means that all plotting commands below ggplot(...)+ use that dataframe as reference. If the size command is put inside the aes then size is dependent on some variable, if put outside the aes it requires a single value (similarly for e.g. colour, linetype and shape). New commands can be added to the first statement (ggplot) by adding a + after each line. If a column name contains a space you can refer to it by putting it between backticks: ...\nWe could add the number of observations to the plot (plot) using the command text. The function text needs at least three arguments, the x position and the y position of the text and the text to be printed at this position. text allows vectors for each of those arguments. Therefore we can write:\n\nplot(`Seeds taken` ~ `Seeds available`,data=t2,cex=log(val)*2,\n        xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ntext(t2[,1],t2[,2],t2[,3])\n\n\n\n\n\n\n\nChange the size of the printed numbers and remove the zeros from the graph.\n\n\n\n\n\nthis removes zeroes and puts it in a new dataframe:\nt3 <- t2[t2$val>0,]\nplotting this new dataframe:\nplot(Seeds taken~Seeds available,data=t3,cex=log(val)*2,      xlim = c(0.3,5.8),ylim=c(-0.5,5.5))\ncex changes the text size:\ntext(t3[,1],t3[,2],t3[,3],cex=0.8)\n\n\n\n\n\nChange the color of the points and the symbols. Consult Lab 1 how to do this\n\n\n\n\n\nplot(Seeds taken~Seeds available,data=t2,cex=log(val)*2,         xlim = c(0.3,5.8),ylim=c(-0.5,5.5),col=\"blue\",pch=2)"
  },
  {
    "objectID": "Lab2/index.html#bar-plot-with-error-bars",
    "href": "Lab2/index.html#bar-plot-with-error-bars",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.3 Bar plot (with error bars)",
    "text": "3.3 Bar plot (with error bars)\nThe command to produce the barplot (Figure 3) was:\n\nbarplot(t(log10(t1 + 1)), beside = TRUE, legend = TRUE, xlab = \"Available\",\n ylab = \"log10(1+# observations)\")\nop = par(xpd = TRUE)\ntext(34.5, 3.05, \"Number taken\")\n\n\n\npar(op)\n\nAlternatively through ggplot\n\nggplot(data=t2)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),\n           stat=\"identity\",position=position_dodge())\n\n\n\n\nAgain through aes we specify what is on the x and y. Through fill we subdivide the bars by the values in taken. stat=identity expresses that the values assigned to y will be used (compare stat=\"count\"). Through specifying position_dodge() bars are printed side by side instead of stacked bars (position_fill()).\nMore impressively, the ggplot package can automatically plot a barplot of a three-way cross-tabulation (one barplot per species): try\n\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = melt(t1.species,varnames=c(\"Seeds available\",\"Seeds taken\",\n                                        \"species\"),value.name=\"val\")\nggplot(data=t1.species)+\n  geom_bar(aes(x=`Seeds available`,y=log10(val+1),\n               fill=as.factor(`Seeds taken`)),stat=\"identity\",\n           position=position_dodge())+\n  facet_wrap(~species)+\n  coord_flip()\n\n\n\n\nwith facet_wrap a sequence of panels is made a specified by the variable behind the ~. The coord_flip rotates the plot.\n\n\n\nRestricting your analysis to only the observations with 5 seeds available, create a barplot showing the distribution of number of seeds taken broken down by species. Choose whether you do this with ggplot2 or through the base plot function.\n\n\n\n\n\nt1.species = table(data$available,data$remaining,data$species)\nt1.species = melt(t1.species,varnames=c(\"Seeds available\",\"Seeds taken\",\"species\"),value.name=\"val\")\nggplot(data=t1.species[t1.species$Seeds available== 5,])+   geom_bar(aes(x=Seeds taken,y=val,),stat=\"identity\",            position=position_dodge())+   facet_wrap(~species)+   coord_flip()\n\n\nTo add error bars to the barplot, one need to calculate the standard error of the means. We want to plot the standard error on top of the fraction seeds taken\nFirst, compute the fraction taken:\n\ndata$frac_taken = data$taken/data$available\n\nComputing the mean fraction taken for each number of seeds available, using the tapply() function: tapply() (“table apply”, pronounced “t apply”), is an extension of the table() function; it splits a specified vector into groups according to the factors provided, then applies a function (e.g. mean() or sd()) to each group. This idea of applying a function to a set of objects is a very general, very powerful idea in data manipulation with R; in due course we’ll learn about apply() (apply a function to rows and columns of matrices), lapply() (apply a function to lists), sapply() (apply a function to lists and simplify), and mapply() (apply a function to multiple lists). For the present, though,\n\nmean_frac_by_avail = tapply(data$frac_taken, data$available, mean)\n\ncomputes the mean of frac_taken for each group defined by a different value of available. R automatically converts available into a factor temporarily for this purpose. If you want to compute the mean by group for more than one variable in a data set, use aggregate(). We can also calculate the standard errors, \\(\\frac{\\sigma}{\\sqrt(n)}\\):\n\nn_by_avail = table(data$available)\nse_by_avail = tapply(data$frac_taken, data$available, sd)/\n              sqrt(n_by_avail)\n\nFirst we plot a barplot after which we add the error bars. The error bars can be drawn by using the function arrows that have an angle between the shaft of the angle and the edge of 90 degrees. To position the error bars at the middle of the bars we need to retrieve those positions from the barplot command. This can be done through assigning a name, e.g. bara to the barplot object and using those positions as x coordinates.\n\nbara = barplot(mean_frac_by_avail,ylim=c(0,0.09))\n# hack: we draw arrows but with very special \"arrowheads\"\narrows(bara[,1],mean_frac_by_avail-se_by_avail,bara[,1], \n       mean_frac_by_avail+se_by_avail, length=0.05, angle=90, code=3)\n\n\n\n\nWith ggplot2 this is possible through the following syntax. Note that there is a convenient function, called summarySE in the Rmisc package to compute the means and se:\n\nlibrary(Rmisc)\n\nWarning: package 'Rmisc' was built under R version 4.3.1\n\n\nLoading required package: lattice\n\n\nLoading required package: plyr\n\n\nWarning: package 'plyr' was built under R version 4.3.1\n\nsum.data = summarySE(data,measurevar= \"frac_taken\",groupvars=c(\"available\"))\nggplot(aes(x=available,y=frac_taken),data=sum.data)+ \n  geom_bar(,stat=\"identity\")+\n  geom_errorbar(aes(ymin=frac_taken-se, ymax=frac_taken+se),\n                stat=\"identity\")\n\n\n\n\nMake sure that the data reference is in the ggplot() function so all other functions such as geom_bar and geom_errobar make use of the same data.frame."
  },
  {
    "objectID": "Lab2/index.html#histogram-by-species",
    "href": "Lab2/index.html#histogram-by-species",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.4 Histogram by species",
    "text": "3.4 Histogram by species\nTo make a histogram we can use the function hist:\n\nhist(data$frac_taken)\n\n\n\n\nTo draw a histogram per species, we need to split the data into a list with each element representing a species.\n\ndata.s = split(data$frac_taken,list(data$species))\n\nNext, we use sapply to plot the histograms.\n\npar(mfrow=c(4,2),oma=c(0,0,0,0),mar=c(4,4,0.1,0.1))\nsapply(data.s,hist,main=\"\")\n# or equivalently\nfor (i in 1:8){\n  hist(data.s[[i]],main=\"\")\n}\n\nWith ggplot2 you can get the frequencies less easily, so will be plot the counts\n\nggplot(data=data)+\ngeom_bar(aes(x=frac_taken),stat=\"count\")+\n  facet_wrap(~ species)\n\n\n\n\n\nggplot(data=data,aes(x=frac_taken))+\n  geom_histogram(aes(y = ..density..))+\n  facet_wrap(~species)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nI’m going to clear the workspace with rm(list=ls()). ls() lists all the objects in the workspace with ls() and then uses rm() to remove them. It is recommend to have this code at the top of every script, so every time you start working on the script you are sure the memory is cleared. You can also clear workspace from the menu in Rstudio (the little brush in the topright panel) and read in the measles data, which are space separated and have a header:\n\nrm(list = ls())\ndata = read.table(\"ewcitmeas.dat\", header = TRUE, na.strings = \"*\")\n\nyear, mon, and day were read in as integers: I’ll create a date variable as described above. For convenience, I’m also defining a variable with the city names.\n\ndate = as.Date(paste(data$year + 1900, data$mon, data$day, sep = \"/\"))\ncity_names = colnames(data)[4:10]\n\nLater on it will be useful to have the data in long format. It’s easiest to do use melt for this purpose. Note that only need to select the appropriate columns to melt (i.e. 4-11).\n\nlibrary(reshape2)\ndata= cbind(data,date)\ndata_long = melt(data[,4:11],id.vars=8,variable.name = \"city\",\n                 value.name=\"incidence\")"
  },
  {
    "objectID": "Lab2/index.html#multiple-line-plots",
    "href": "Lab2/index.html#multiple-line-plots",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.5 Multiple-line plots",
    "text": "3.5 Multiple-line plots\nWe can make a plot with multiple lines as follows. We first setup the plotting region using plot followed by the function lines to add lines to the existing plot. Note that in the plotting command type=\"l\" is used to specify that lines are drawn instead of point (type=\"p\", the default). A legend can be added by adding the function legend\n\ndata_long.s = split(data_long,data_long$city)\nplot(incidence ~ date,col=1,type=\"l\",\n     data=data_long[data_long$city == \"London\",])\n\nunique.city = unique(data_long$city)\nfor (i in 2:length(unique.city)){\n  lines(incidence ~ date,type=\"l\",\n        data=data_long[data_long$city == unique.city[i],],col=i)\n}\nlegend(\"topright\",legend=unique.city,col=1:8,lty=1)\n\nWith ggplot2 we specify\n\nggplot() +\n  geom_line(aes(x=date,y=incidence, colour=city),data=data_long)"
  },
  {
    "objectID": "Lab2/index.html#histogram-and-density-plots",
    "href": "Lab2/index.html#histogram-and-density-plots",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.6 Histogram and density plots",
    "text": "3.6 Histogram and density plots\nI’ll start by just collapsing all the incidence data into a single, logged, non-NA vector (in this case I have to use c(as.matrix(x)) to collapse the data and remove all of the data frame information):\n\nallvals = na.omit(c(as.matrix(data[, 4:10])))\nlogvals = log10(1 + allvals)\n\nThe histogram (hist() command is fairly easy: the only tricks are to leave room for the other lines that will go on the plot by setting the y limits with ylim, and to specify that we want the data plotted as relative frequencies, not numbers of counts (freq=FALSE or prob=TRUE). This option tells R to divide by total number of counts and then by the bin width, so that the area covered by all the bars adds up to 1. This scaling makes the vertical scale of the histogram compatible with a density plot, or among different histograms with different number of counts or bin widths.\n\nhist(logvals, col = \"gray\", main = \"\", xlab = \"Log weekly incidence\",\n ylab = \"Density\", freq = FALSE, ylim = c(0, 0.6))\n\n\n\n\nAdding lines for the density is straightforward, since R knows what to do with a density object - in general, the lines command just adds lines to a plot.\n\nlines(density(logvals), lwd = 2)\nlines(density(logvals, adjust = 0.5), lwd = 2, lty = 2)\n\nWith ggplot2 we specify:\n\nggplot()+\n  geom_histogram(aes(x=logvals,y=..density..))+\n  geom_density(aes(x=logvals,y=..density..))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Lab2/index.html#scaling-data",
    "href": "Lab2/index.html#scaling-data",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.7 Scaling data",
    "text": "3.7 Scaling data\nScaling the incidence in each city by the population size, or by the mean or maximum incidence in that city, begins to get us into some non-trivial data manipulation. This process may actually be easier in the wide format. Several useful commands: * rowMeans(), rowSums(), colMeans(), and colSums() will compute the means or sums of columns efficiently. In this case we would do something like colMeans(data[,4:10]) to get the mean incidence for each city.\n\napply() is the more general command for running some command on each of a set of rows or columns. When you look at the help for apply() you’ll see an argument called MARGIN, which specifies whether you want to operate on rows (1) or columns (2). For example, apply(data[,4:10],1,mean) is the equivalent of rowMeans(data[,4:10]), but we can also easily say (e.g.) apply(data[,4:10],1,max) to get the maxima instead. Later, when you’ve gotten practice defining your own functions, you can apply any function - not just R’s built-in functions.\nscale() is a function for subtracting and dividing specified amounts out of the columns of a matrix. It is fairly flexible: scale(x,center=TRUE,scale=TRUE) will center by subtracting the means and then scale by dividing by the standard errors of the columns. Fairly obviously, setting either to FALSE will turn off that part of the operation. You can also specify a vector for either center or scale, in which case scale() will subtract or divide the columns by those vectors instead.\n\n\n\n\nFigure out how to use apply() and scale() to scale all columns so they have a minimum of 0 and a maximum of 1 (hint: subtract the minimum and divide by (max-min)).\n\n\n\n\n\nThis solution can be made more elegant once we have learned to program our own functions\nincidence = data[,4:10]\nincidence.min= apply(incidence,2,min,na.rm=T)\nincidence.max= apply(incidence,2,max,na.rm=T)\nrange = incidence.max-incidence.min\ndata.scaled = scale(incidence, center=incidence.min,scale=range)\nsummary(data.scaled)"
  },
  {
    "objectID": "Lab2/index.html#box-and-whisker-and-violin-plots",
    "href": "Lab2/index.html#box-and-whisker-and-violin-plots",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.8 Box-and-whisker and violin plots",
    "text": "3.8 Box-and-whisker and violin plots\nBy this time, box-and-whisker and violin plots will (I hope) seem easy. Since the labels get a little crowded (R is not really sophisticated about dealing with axis labels-crowded labels), I’ll use the substr() (substring) command to abbreviate each city’s name to its first three letters.\n\ncity_abbr = substr(city_names, 1, 3)\n\nThe boxplot() command uses a formula - the variable before the ~ is the data and the variable after it is the factor to use to split the data up.\n\nboxplot(log10(1 + incidence) ~ city, data = data_long, ylab = \"Log(incidence+1)\",\n names = city_abbr)\n\nOr through ggplot\n\nggplot(data=data_long)+\n  geom_boxplot((aes(x=city,y=log10(incidence+1))))\n\nIf I want to make a violin plot, you can specify:\n\nggplot(data=data_long)+\n  geom_violin((aes(x=city,y=log10(incidence+1))))"
  },
  {
    "objectID": "Lab2/index.html#pair-plot",
    "href": "Lab2/index.html#pair-plot",
    "title": "Exploratory data analysis and graphics (lab 2)",
    "section": "3.9 Pair plot",
    "text": "3.9 Pair plot\nFirst let’s make sure the earthquake data are accessible:\n\ndata(quakes)\n\nLuckily, most of the plots I drew in this section are fairly automatic. To draw a scatterplot matrix, just use pairs() (base):\n\npairs(quakes, pch = \".\")\n\n\n\n\n(pch=\".\" marks the data with a single-pixel point, which is handy if you are fortunate enough to have a really big data set).\n\n\n\nGenerate three new plots based on one of the data sets in this lab, or on your own data.\n\n\n\n\n\nThis is up to your creativity"
  },
  {
    "objectID": "Lab3/no_solution.html",
    "href": "Lab3/no_solution.html",
    "title": "Lab 3: Analyzing functions",
    "section": "",
    "text": "1 Learning goals\nIn this lab you will learn to analyse mathematical functions. This is an important step in ecological modelling. Next, we proceed with analysing and programming these functions in R. To do so, you will need more advanced programming skills such as for-loops, if-else statements and functions.\n\n\n2 Getting familiar with a bestiary of functions\nThe models that we will be fitting to data are composed of a deterministic component and a stochastic component. The deterministic component describes the expected pattern in absence of any randomness. You are not restricted to linear functions (as in linear regression) but you can choose among different functions.\nRemember that functions can be purely phenomological or mechanistic (see p.21 of Bolker). Bolker mentions the following non-linear functions in his chapter about a bestiary of functions: Hyperbolic, Michaelis-Menten (=Monod or Holling type II), Holling type III, Holling type IV, negative exponential, monomolecular (=limited exponential growth), Ricker, logistic, power law, von Bertalanffy, Sheppard, Hassell, non-rectangular hyperbola.\n\n\n\nThis exercise is the first in a series of connected and sequential exercises. The dataset ‘shapes.xlsx’ contains six different datasets, of which only the first two wil be used in this exercise, but if you want to practice more, you can do the exercise below for the remaining four as well.\nTo make yourself familiar with a number of deterministic functions, you are asked to take a number of steps: read in a dataset into R, make plots of the first two datasets and choose at least two appropriate functions for each dataset. Next, you will explore the properties of the selected functions after which you will choose appropriate parameter values through eyeballing so you get a reasonable fit between the data and the choosen function.\nA pseudocode that implements this idea:\n\nRead the first two datasets that are in shapes.xlsx. Save each sheet as .csv file and read it in.\nPlot the datasets in different graphs in a panel plot (hint: par(mfrow=c(..,..), and plot))\nChoose (at least) two appropriate functions based on the type of data or the shape of the data.\n\nhint 1: dataset 1 describes a light response curve of ten individual plants\nhint 2: dataset 2 describes the intake rate of a predator as a function of prey density\nhint 3: dataset 3 the data describe an allometric relationship\nhint 4: dataset 4 contains measurements of population size over time\nhint 5: This dataset you need to figure out for yourselves.\nhint 6: optional dataset 6 a species response curve (presence/absence). Fit a model that models the probability of presence (use google to find a good one).\n\nExplore the properties of the selected functions in the following steps:\n\nWhat is the value of \\(f(0)\\) and \\(f'(0)\\)?\nWhat are the limits for \\(x\\to\\infty\\) for \\(f(x)\\) and \\(f'(x)\\)\nWhat are the limits for \\(x\\to-\\infty\\) for \\(f(x)\\) and \\(f'(x)\\)\nIf \\(f(x)\\) saturates at, say the value \\(a\\), for \\(x\\to\\infty\\) then determine the \\(x\\)-value \\(x_1\\) at which \\(f(x_1)=\\frac{a}{2}\\). If \\(f(x)\\) obtains a maximum value, find the \\(x\\) and \\(y\\) coordinate of the maximum.\n\nChoose appropriate parameter values through eyeballing so that the chosen curves more or less match the data. Eyeballing means that you knowledge on the effect of parameter values on the shape of the function (see question 4). Later we will use likelihood methods to estimate the parameter values.\nTime permitting repeat subquestions 3-5 for the other three datasets.\n\n\n\n\n\n\n\n\n\n\n\n3 For loops\nWhen programming your data analysis, you often need to iterate over multiple elements of a collection. These elements could be rows of a data.frame, datasets inside a list, numbers inside a vector, etc. The iteration usually means that you apply the same code over each element of the collection and you don’t want to “copy-paste” the code for each element. Iterating over a collection is called a “for loop” in programming. A for loop consists of three components:\n\nA collection over which you want to iterate.\nA variable that keeps track of where you are in the collection in each iteration\nThe body of the loop where you apply some code.\n\nImagine that you want to calculate the factorial of 10. The factorial of a number is simply the product of all positive numbers smaller or equal than the number you specify (and it is denote with a “!” after the number). For example, the factorial of 3 is \\(3! = 3 \\times 2 \\times 1\\). A simple way of calculating the factorial of 10 by using a for loop is:\n\nresult = 1\nfor(i in 1:10) {\n  result = result*i\n}\n\nIn this for loop, the collection is 1:10, the variable to keep track of the number is i and the body of the loop is result = result*i. This for loop shows a very typical pattern: we want to summarise some collection of numbers into a single number, in this case, result.\nAnother typical pattern is when we want to calculate the elements of a collection. In this case, it is a good practice to “pre-allocate” your output collection before looping, so R knows how much memory to allocate for this vector. For example,\n\nx = 1:10\ny = numeric(10)\nfor(i in 1:10) {\n  y[i] = exp(x[i])\n}\n\nIn this case, the result of the for loop (y) is a collection of 10 elements where each element is the exponential transformation of the element in x with the same position. Note that we specify before the loop that y will have 10 elements. Although this is not strictly required in this case, it is a good practice both to avoid errors and to make your code run faster.\nFor loops are not that common in R as in other languages The reason is that many mathematical and statistical functions are already, implicitly, looping over your collections. For examples, when you take the exponential (exp()) of a collection of numbers, it will produce a new collection which is the result of looping over the original collection. That is:\n\nx = 1:10\ny = exp(x)\n\nis equivalent to the previous loop described before. As you can see, this second option requires less code and it is easier to read, which is one of the reasons why R is such a greate language for working with data. In addition, if you rely on this implicit looping your code will run much faster.\nHowever, there may be situations where you really cannot avoid a for loop. For example, if you have collected multiple datasets and need to perform the same analysis on each dataset, you could store your datasets in a list and use a for loop to iterate over the different datasets.\n\n\n\nCalculate the logarithm of the sequence 1:10, using first a for loop and then without a for loop.\n\n\n\n\n\n\n\n4 Missing values (NA)\nMissing values in data collection and analysis deserve special attention. You can get missing values during you data collection for various reasons, e.g. you missed the opportunity to take a measurement, you lose one of the replicates due to contamination, you wrote down a wrong value, or you even lost some of your data.\nThe most important thing you need to understand, is that in almost all cases of a missing value, you should not represent a missing value with a zero (0). This will throw your analysis out of balance and give erroneous results. A common way of representing missing data when you are performing data input, would be with a character like an asterisk (*) or a hyphen (-).\nMany of the functions that read data in R have an argument that allows you to select how you have represented a missing-value in your data. As an example the function read.csv (which reads a comma-delimited data file) would be used like this to read from a file named “your-data-file”:\n\nmyData <- read.csv(\"you-data-file\", na.strings=c(\"*\",\"-\") )\n\nIn this case we instructed R, with the argument na.strings=c(\"*\",\"_\") to read our file, and substitute any occurence of an asterisk (*) or a hyphen(-) with an NA symbol.\nThe R languages has a special way of representing missing values in a dataset. A missing value is denoted with the symbol NA which stands for “Not Available”. By default, missing values will “propagate” throughout the calculations. For example, given two vectors of data:\n\nx = c(1,2,3)\ny = c(2,4,NA)\n\nWhen you combine these vectors (e.g. add them or multiply them) you will see that the third component is always NA\n\nx + y\n\n[1]  3  6 NA\n\nx*y\n\n[1]  2  8 NA\n\n\nWhen you calculate some statistical property of your data (e.g. mean, standard deviation) it will, by default, report NA if there is at least one missing value in your data\n\nmean(x)\n\n[1] 2\n\nmean(y)\n\n[1] NA\n\n\nMost statistical functions in R allow you to specify how to deal with missing values. Most often, you are given the option to ignore any missing values from the data when calculating an statistical property through an argument often called na.rm. For example, in order to get the mean of the non-missing values of y we need:\n\nmean(y, na.rm = TRUE)\n\n[1] 3\n\n\nwhich, of course, is the mean of 2 and 4. However, other functions not have an option to handle NA even though you still need to make a decision on how to deal with them. For example, when you calculate the length of a dataset (length()) do you want to consider the whole data or only the non-missing values? This is not a trivial question and the answer on the context where you will use the result. In any case, if you want to remove the NA when calculating the length you need to be more creative. Fortunately, R offers the function is.na which returns a vector of TRUE or FALSE values corresponding to the index of mssing or non-missing data values in the vector y:\n\nis.na(y)\n\n[1] FALSE FALSE  TRUE\n\n\nNext a vector without NA can be obtained through:\n\nlength(y[!is.na(y)])\n\n[1] 2\n\n\nWhich only gives 2 as the third element is missing. Remember that ! is a negation operator, so !is.na actually means “is not NA”.\nBy the way, you should not confuse NA with NaN which stands for “Not a Number”. An NaN is the result of either an expression with indeterminate form (e.g. 0/0 or Inf/Inf) or when a function is evaluated outside of its valid domain (e.g. sqrt(-1) or log(-1)).\n\n\n\nGiven some data created from the following code c(25,1,10,89, NA, NA), calculate the mean value and the standard error of this mean (\\(s.e.m. = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation and \\(n\\) is the number of items) by ignoring missing values.\n\n\n\n\n\n\n\n5 Making a function\nWhen you want to repeat a calculation for different data, it is best to code your calculations inside a function. R consists of many built-in functions, but sometimes you need to do a calculation that is not available in R. A function is defined by 4 elements\n\nThe name of the function. For example, in R there is a function that calculates the arithmetic mean of a vector of data and its name is mean. You should make sure that the name of your function does not coincide with existing functions, that it is not too long and that it conveys its meaning. You can check if a function already exist in the base or any of the packages you loaded through ?nameoffunction.\nThe arguments of the function. These are the variables that you need to pass to the function (i.e., inputs). The arguments are defined by a position and a name. Also, some arguments may have default values which means that you do not need to specify them every time you call the function. For example, the function mean, contains three arguments (x, trim and na.rm) but the last two have default values.\nThe body of the function. This is the actual code that the function will execute. The real mean function in R has some crytpic body that requires advanced knowledge of the language to understand. However, a more “naive” implementation of mean could be sum(x)/length(x). Note that the body of a function can consist of multiple lines.\nThe return value of the function. This is the result of applying the function on the arguments. By default, the result of the last line code in the body of the function is the return value of the function. You can also return from any point in the body with the function return() with the variable you want to return inside.\n\nThe R language specifies a particular syntax on how to build a function. For example, a naive_mean could be defined as:\n\nnaive_mean = function(x, na.remove = FALSE) {\n  total = sum(x, na.rm = na.remove)\n  n = length(x[!is.na(x)])\n  result = total/n\n  return(result)\n}\n\nIn this case, the function naive_mean has two arguments (x and na.remove) where the second argument has a default value of FALSE and the body consists of several lines of code. These are respectively the sum of the elements of x with the na.rm depending on whether you specified TRUE or FALSE in the na.remove argument; n that calculates the length of the vector x without NAs, and the calculation of the mean. The last statement returns the result. Notice that arguments are separated by commas and the body of the function is enclosed in curly braces {}. The name of the function is simply the name of the variable to which you assigned the function (i.e., naive_mean). You can see below that you can use this function in a similar manner to the built-in mean\n\nx = 1:10\nnaive_mean(x)\n\n[1] 5.5\n\n\nNotice that we did not specify the value of na.remove as the default is ok in this case. However, if we had missing values, the NA would propagate to the output:\n\nx = c(1,2,NA,4)\nnaive_mean(x)\n\n[1] NA\n\n\nSpecifying na.remove=FALSE can be used as a double check that there are no NAs in your vector. If they are present it forces us to make a decision about what to do with the NAs. Let’s say that, for the moment, we want to just remove the values that are NA from the calculation. In this case, we just change the value of the default parameter.\n\nnaive_mean(x, na.remove = TRUE)\n\n[1] 2.333333\n\n\nFor convenience, default parameters are specified by name rather than position. However we could have also said naive_mean(x,TRUE) or even naive_mean(x = x, na.remove = TRUE). All these forms of calling functions are OK, whether you choose one style or another is a matter of taste.\n\n\n\nBuild a function to calculate the standard deviation (\\(\\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n\\left(x_i - \\bar x\\right)^2}{n - 1}}\\)). Test your function with some data that includes missing values, and compare to the built in function for the standard deviation sd.\n\n\n\n\n\nSuprisingly the base R does not have a built in function for the standard error of the mean (s.e.m.). The sem is defined as \\(\\frac{\\sigma}{\\sqrt(n)}\\).\n\n\n\nMake you own function for the sem and use your own home-made function of the standard deviation for that.\n\n\n\n\n\nAs you see you can call functions inside functions. It is recommended to divide the work you want to do into little functions that each carry out a specific task, and then combine those functions into a larger function that combines these tasks. This facilitates error checking.\n\n\n6 Numerical explorations: plotting curves\nHere are the R commands used to generate Figure 3.2 in the book (p 74). They just use curve(), with add=FALSE (the default, which draws a new plot) and add=TRUE (adds the curve to an existing plot), particular values of from and to, and various graphical parameters (ylim, ylab, lty).\n\ncurve(2*exp(-x/2),from=0,to=7,ylim=c(0,2),ylab=\"\")\ncurve(2*exp(-x),add=TRUE,lty=4)\ncurve(x*exp(-x/2),add=TRUE,lty=2)\ncurve(2*x*exp(-x/2),add=TRUE,lty=3)\ntext(0.4,1.9,expression(paste(\"exponential: \",2*e^(-x/2))),adj=0)\ntext(4,.5,expression(paste(\"Ricker: \",x*e^(-x/2))))\ntext(4,1,expression(paste(\"Ricker: \",2*x*e^(-x/2))),adj=0)\ntext(2.8,0,expression(paste(\"exponential: \",2*e^(-x))))\n\nThe only new thing in this figure is the use of expression() to add a mathematical formula to an R graphic. text(x,y,\"x^2\") puts x^2 on the graph at position \\((x,y)\\); text(x,y,expression(x^2)) (no quotation marks) puts \\(x^2\\) on the graph. See ?plotmath or ?demo(plotmath) for (much) more information.\nAn alternate way of plotting the exponential parts of this curve:\n\nxvec = seq(0,7,length=100)\nexp1_vec = 2*exp(-xvec/2)\nexp2_vec = 2*exp(-xvec)\n\n\nplot(xvec,exp1_vec,type=\"l\",ylim=c(0,2),ylab=\"\")\nlines(xvec,exp2_vec,lty=4)\n\nFinally, if you have a more complicated function you could use sapply() to call this function along with appropriate parameter values. you could say:\n\nexpfun = function(x,a=1,b=1) {\n   a*exp(-b*x)\n }\nexp1_vec = sapply(xvec,expfun,a=2,b=1/2)\nexp2_vec = sapply(xvec,expfun,a=2,b=1)\n\nThe advantage of curve() is that you don’t have to define any vectors: the advantage of doing things the other way arises when you want to keep the vectors around to do other calculations with them.\n\n\n\nConstruct a curve that has a maximum at (\\(x=5\\), \\(y=1\\)). Write the equation, draw the curve in R, and explain how you got there.\n\n\n\n\n\n\n\n7 A quick digression: ifelse() for piecewise functions\nThe ifelse() command in R is useful for constructing piecewise functions. Its basic syntax is ifelse(condition,value_if_true,value_if_false), where condition is a logical vector (e.g. x>0), value_if_true is a vector of alternatives to use if condition is TRUE, and value_if_false is a vector of alternatives to use if condition is FALSE. If you specify just one value, it will be expanded (recycled in R jargon) to be the right length. A simple example:\n\nx=c(-25,-16,-9,-4,-1,0,1,4,9,16,25)\nsqrt(ifelse(x<0,0,x))\n\n [1] 0 0 0 0 0 0 1 2 3 4 5\n\n\nif you said ifelse(x<0,0,sqrt(x))) you would get a warning: why)\nHere are some examples of using ifelse() to generate (1) a simple threshold; (2) a Holling type I or “hockey stick”; (3) a more complicated piecewise model that grows exponentially and then decreases linearly; (4) a double-threshold model. When plotting functions with abrubt changes with the function curve, beware that curve draws a line by evaluating a functions at several locations along the specified interval (from, to). You can increase these number of points by specifying n. The default value for n is 101.\n\nop=par(mfrow=c(2,2),mgp=c(2,1,0),mar=c(4.2,3,1,1))\ncurve(ifelse(x<2,1,3),from=0,to=5)\ncurve(ifelse(x<2,2*x,4),from=0,to=5)\ncurve(ifelse(x<2,exp(x),exp(2)-3*(x-2)),from=0,to=5)\ncurve(ifelse(x<2,1,ifelse(x<4,3,5)),from=0,to=5)\n\n\n\n\nThe double-threshold example (nested ifelse() commands) probably needs more explanation. In words, this command would go “if \\(x\\) is less than 2, set \\(y\\) to 1; otherwise (\\(x \\ge 2\\)), if \\(x\\) is less than 4 (i.e. \\(2 \\le x<4\\)), set \\(y\\) to 3; otherwise (\\(x \\ge 4\\)), set \\(y\\) to 5”.\n\n\n8 Evaluating derivatives in R\nR can evaluate derivatives, but it is not very good at simplifying them. In order for R to know that you really mean (e.g) x^2 to be a mathematical expression and not a calculation for R to try to do (and either fill in the current value of x or give an error that x is undefined), you have to specify it as expression(x^2); you also have to tell R (in quotation marks) what variable you want to differentiate with respect to:\n\nd1 = D(expression(x^2),\"x\"); d1\n\n2 * x\n\n\nUse eval() to fill in a list of particular values for which you want a numeric answer:\n\neval(d1,list(x=2))\n\n[1] 4\n\n\nTaking the second derivative:\n\nD(d1,\"x\")\n\n[1] 2\n\n\n(As of version 2.0.1,) R knows how to take the derivatives of expressions including all the basic arithmetic operators; exponentials and logarithms; trigonometric inverse trig, and hyperbolic trig functions; square roots; and normal (Gaussian) density and cumulative density functions; and gamma and log-gamma functions. You’re on your own for anything else (consider using a symbolic algebra package like Mathematica or Maple, at least to check your answers, if your problem is very complicated). deriv() is a slightly more complicated version of D() that is useful for incorporating the results of differentiation into functions: see the help page."
  },
  {
    "objectID": "Lab3/solution.html",
    "href": "Lab3/solution.html",
    "title": "Lab 3: Analyzing functions",
    "section": "",
    "text": "1 Learning goals\nIn this lab you will learn to analyse mathematical functions. This is an important step in ecological modelling. Next, we proceed with analysing and programming these functions in R. To do so, you will need more advanced programming skills such as for-loops, if-else statements and functions.\n\n\n2 Getting familiar with a bestiary of functions\nThe models that we will be fitting to data are composed of a deterministic component and a stochastic component. The deterministic component describes the expected pattern in absence of any randomness. You are not restricted to linear functions (as in linear regression) but you can choose among different functions.\nRemember that functions can be purely phenomological or mechanistic (see p.21 of Bolker). Bolker mentions the following non-linear functions in his chapter about a bestiary of functions: Hyperbolic, Michaelis-Menten (=Monod or Holling type II), Holling type III, Holling type IV, negative exponential, monomolecular (=limited exponential growth), Ricker, logistic, power law, von Bertalanffy, Sheppard, Hassell, non-rectangular hyperbola.\n\n\n\nThis exercise is the first in a series of connected and sequential exercises. The dataset ‘shapes.xlsx’ contains six different datasets, of which only the first two wil be used in this exercise, but if you want to practice more, you can do the exercise below for the remaining four as well.\nTo make yourself familiar with a number of deterministic functions, you are asked to take a number of steps: read in a dataset into R, make plots of the first two datasets and choose at least two appropriate functions for each dataset. Next, you will explore the properties of the selected functions after which you will choose appropriate parameter values through eyeballing so you get a reasonable fit between the data and the choosen function.\nA pseudocode that implements this idea:\n\nRead the first two datasets that are in shapes.xlsx. Save each sheet as .csv file and read it in.\nPlot the datasets in different graphs in a panel plot (hint: par(mfrow=c(..,..), and plot))\nChoose (at least) two appropriate functions based on the type of data or the shape of the data.\n\nhint 1: dataset 1 describes a light response curve of ten individual plants\nhint 2: dataset 2 describes the intake rate of a predator as a function of prey density\nhint 3: dataset 3 the data describe an allometric relationship\nhint 4: dataset 4 contains measurements of population size over time\nhint 5: This dataset you need to figure out for yourselves.\nhint 6: optional dataset 6 a species response curve (presence/absence). Fit a model that models the probability of presence (use google to find a good one).\n\nExplore the properties of the selected functions in the following steps:\n\nWhat is the value of \\(f(0)\\) and \\(f'(0)\\)?\nWhat are the limits for \\(x\\to\\infty\\) for \\(f(x)\\) and \\(f'(x)\\)\nWhat are the limits for \\(x\\to-\\infty\\) for \\(f(x)\\) and \\(f'(x)\\)\nIf \\(f(x)\\) saturates at, say the value \\(a\\), for \\(x\\to\\infty\\) then determine the \\(x\\)-value \\(x_1\\) at which \\(f(x_1)=\\frac{a}{2}\\). If \\(f(x)\\) obtains a maximum value, find the \\(x\\) and \\(y\\) coordinate of the maximum.\n\nChoose appropriate parameter values through eyeballing so that the chosen curves more or less match the data. Eyeballing means that you knowledge on the effect of parameter values on the shape of the function (see question 4). Later we will use likelihood methods to estimate the parameter values.\nTime permitting repeat subquestions 3-5 for the other three datasets.\n\n\n\n\n\n\n\n\n\n\nThe first step is to read in the data. The data is stored in different sheet of the file shapes.xlsx. Select a sheet, go to file and ‘save as’ and save the sheet as a comma-separated-file (.csv). Check if the file was saved correctly: Open the file in notepad, and check if the columns are separated by a comma and the decimal point is a ‘.’. Beware that if you are using a computer with dutch settings, you may run into problems for two reasons: First, the decimal character is a comma , in Dutch. Second the default list separator is a semicolon ; in Dutch instead of a ,. If this is not correct; go to control panel; to region and language settings; to advanced settings and change the decimal character into a . and the list separator (lijstschijdingsteken) in to a ,.\n\nsetwd(\"D://...//...//...) shapes1 = read.csv(shapes1.csv)\n\nAfter you have read in the data, you can make a plot, e.g. through plot(shapes1$y~ shapes1$x) or plot(y~x,data=shapes1) for dataset 1. Multiple plots can be made through specifying par(mfrow=c(3,2)). This will setup the grid, after using plot six times, the grid will be filled with plots. Note that all the datasets are different in their specification of the header (names). You can take account of the header name through header=TRUE inside the read.csv function. Check the datafiles to make sure whether or not there are column headings.\nChoosing appropriate deterministic functions\ndataset 1 light response curve. There are a number of options of functions to choose from, depending on the level of sophistication: \\(\\frac{ax}{(b+x)}\\), \\(a(1-e^{(-bx)})\\), \\(\\frac{1}{2\\theta}(\\alpha I+p_{max}-\\sqrt(\\alpha I+p_{max})^2-4\\theta I p_{max})\\) see page 98 of Bolker.\ndataset 2 The dataset describes a functional response. Bolker mentions four of those \\(min(ax,s)\\) \\(\\frac{ax}{(b+x)}\\), \\(\\frac{ax^2}{(b^2+x^2)}\\),\\(\\frac{ax^2}{(b+cx+x^2)}\\)\ndataset 3 Allometric relationships have the form \\(ax^b\\)\ndataset 4 This could be logistic growth \\(n(t)=\\frac{K}{1+(\\frac{K}{n_0})e^{-rt}}\\) or the gompertz function \\(f(x)=e^{-ae^{-bx}}\\)\ndataset 5 What about a negative exponential? \\(ae^{-bx}\\) or a power function \\(ax^b\\)\ndataset 6 Species reponse curves are curves that describe the probability of presence as a function of an environmental variable. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g. \\(a e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\)\nSee the word file on Brightspace “Bestiary of functions.docx”\nCurves can be added to the plots through curve: e.g. curve(2x+2x,from=0,to=20)\ndataset 1 Reasonable values for the first dataset assuming a michaelis menten relationship are a=25 and b=60. For the non-rectangular parabola one could choose values of theta = 0.7; a = 0.25; pmax = 25.\ndataset 2 curve(ifelse(x>27,18,(2/3)*x),add=T)\ncurve(20*x/(10+x),add=T)\ndataset 3 curve(0.6*x^2,add=T)\ndataset 4 K = 200; r = 0.2; N0=2; curve(K/(1+(K/N0)*exp(-r*x)),add=T)\ndataset 5 curve(8*(exp(-0.75*x)),add=T)\ndataset 6 mu = 5; b = 2; curve(exp(-(mu-x)^2/b)  ,add=T)\n\n\n\n\n\n3 For loops\nWhen programming your data analysis, you often need to iterate over multiple elements of a collection. These elements could be rows of a data.frame, datasets inside a list, numbers inside a vector, etc. The iteration usually means that you apply the same code over each element of the collection and you don’t want to “copy-paste” the code for each element. Iterating over a collection is called a “for loop” in programming. A for loop consists of three components:\n\nA collection over which you want to iterate.\nA variable that keeps track of where you are in the collection in each iteration\nThe body of the loop where you apply some code.\n\nImagine that you want to calculate the factorial of 10. The factorial of a number is simply the product of all positive numbers smaller or equal than the number you specify (and it is denote with a “!” after the number). For example, the factorial of 3 is \\(3! = 3 \\times 2 \\times 1\\). A simple way of calculating the factorial of 10 by using a for loop is:\n\nresult = 1\nfor(i in 1:10) {\n  result = result*i\n}\n\nIn this for loop, the collection is 1:10, the variable to keep track of the number is i and the body of the loop is result = result*i. This for loop shows a very typical pattern: we want to summarise some collection of numbers into a single number, in this case, result.\nAnother typical pattern is when we want to calculate the elements of a collection. In this case, it is a good practice to “pre-allocate” your output collection before looping, so R knows how much memory to allocate for this vector. For example,\n\nx = 1:10\ny = numeric(10)\nfor(i in 1:10) {\n  y[i] = exp(x[i])\n}\n\nIn this case, the result of the for loop (y) is a collection of 10 elements where each element is the exponential transformation of the element in x with the same position. Note that we specify before the loop that y will have 10 elements. Although this is not strictly required in this case, it is a good practice both to avoid errors and to make your code run faster.\nFor loops are not that common in R as in other languages The reason is that many mathematical and statistical functions are already, implicitly, looping over your collections. For examples, when you take the exponential (exp()) of a collection of numbers, it will produce a new collection which is the result of looping over the original collection. That is:\n\nx = 1:10\ny = exp(x)\n\nis equivalent to the previous loop described before. As you can see, this second option requires less code and it is easier to read, which is one of the reasons why R is such a greate language for working with data. In addition, if you rely on this implicit looping your code will run much faster.\nHowever, there may be situations where you really cannot avoid a for loop. For example, if you have collected multiple datasets and need to perform the same analysis on each dataset, you could store your datasets in a list and use a for loop to iterate over the different datasets.\n\n\n\nCalculate the logarithm of the sequence 1:10, using first a for loop and then without a for loop.\n\n\n\n\n\nfor (i in 1:10){\nlog(i)\n\nprint(log(i))\n}\nNote that the print statement was added to print to screen\nlog(c(1:10))\n\n\n\n\n4 Missing values (NA)\nMissing values in data collection and analysis deserve special attention. You can get missing values during you data collection for various reasons, e.g. you missed the opportunity to take a measurement, you lose one of the replicates due to contamination, you wrote down a wrong value, or you even lost some of your data.\nThe most important thing you need to understand, is that in almost all cases of a missing value, you should not represent a missing value with a zero (0). This will throw your analysis out of balance and give erroneous results. A common way of representing missing data when you are performing data input, would be with a character like an asterisk (*) or a hyphen (-).\nMany of the functions that read data in R have an argument that allows you to select how you have represented a missing-value in your data. As an example the function read.csv (which reads a comma-delimited data file) would be used like this to read from a file named “your-data-file”:\n\nmyData <- read.csv(\"you-data-file\", na.strings=c(\"*\",\"-\") )\n\nIn this case we instructed R, with the argument na.strings=c(\"*\",\"_\") to read our file, and substitute any occurence of an asterisk (*) or a hyphen(-) with an NA symbol.\nThe R languages has a special way of representing missing values in a dataset. A missing value is denoted with the symbol NA which stands for “Not Available”. By default, missing values will “propagate” throughout the calculations. For example, given two vectors of data:\n\nx = c(1,2,3)\ny = c(2,4,NA)\n\nWhen you combine these vectors (e.g. add them or multiply them) you will see that the third component is always NA\n\nx + y\n\n[1]  3  6 NA\n\nx*y\n\n[1]  2  8 NA\n\n\nWhen you calculate some statistical property of your data (e.g. mean, standard deviation) it will, by default, report NA if there is at least one missing value in your data\n\nmean(x)\n\n[1] 2\n\nmean(y)\n\n[1] NA\n\n\nMost statistical functions in R allow you to specify how to deal with missing values. Most often, you are given the option to ignore any missing values from the data when calculating an statistical property through an argument often called na.rm. For example, in order to get the mean of the non-missing values of y we need:\n\nmean(y, na.rm = TRUE)\n\n[1] 3\n\n\nwhich, of course, is the mean of 2 and 4. However, other functions not have an option to handle NA even though you still need to make a decision on how to deal with them. For example, when you calculate the length of a dataset (length()) do you want to consider the whole data or only the non-missing values? This is not a trivial question and the answer on the context where you will use the result. In any case, if you want to remove the NA when calculating the length you need to be more creative. Fortunately, R offers the function is.na which returns a vector of TRUE or FALSE values corresponding to the index of mssing or non-missing data values in the vector y:\n\nis.na(y)\n\n[1] FALSE FALSE  TRUE\n\n\nNext a vector without NA can be obtained through:\n\nlength(y[!is.na(y)])\n\n[1] 2\n\n\nWhich only gives 2 as the third element is missing. Remember that ! is a negation operator, so !is.na actually means “is not NA”.\nBy the way, you should not confuse NA with NaN which stands for “Not a Number”. An NaN is the result of either an expression with indeterminate form (e.g. 0/0 or Inf/Inf) or when a function is evaluated outside of its valid domain (e.g. sqrt(-1) or log(-1)).\n\n\n\nGiven some data created from the following code c(25,1,10,89, NA, NA), calculate the mean value and the standard error of this mean (\\(s.e.m. = \\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation and \\(n\\) is the number of items) by ignoring missing values.\n\n\n\n\n\ndata <- c(25,1,10,89, NA, NA)\nsd(data,na.rm=T)/sqrt(length(na.omit(data)))\n\n\n\n\n5 Making a function\nWhen you want to repeat a calculation for different data, it is best to code your calculations inside a function. R consists of many built-in functions, but sometimes you need to do a calculation that is not available in R. A function is defined by 4 elements\n\nThe name of the function. For example, in R there is a function that calculates the arithmetic mean of a vector of data and its name is mean. You should make sure that the name of your function does not coincide with existing functions, that it is not too long and that it conveys its meaning. You can check if a function already exist in the base or any of the packages you loaded through ?nameoffunction.\nThe arguments of the function. These are the variables that you need to pass to the function (i.e., inputs). The arguments are defined by a position and a name. Also, some arguments may have default values which means that you do not need to specify them every time you call the function. For example, the function mean, contains three arguments (x, trim and na.rm) but the last two have default values.\nThe body of the function. This is the actual code that the function will execute. The real mean function in R has some crytpic body that requires advanced knowledge of the language to understand. However, a more “naive” implementation of mean could be sum(x)/length(x). Note that the body of a function can consist of multiple lines.\nThe return value of the function. This is the result of applying the function on the arguments. By default, the result of the last line code in the body of the function is the return value of the function. You can also return from any point in the body with the function return() with the variable you want to return inside.\n\nThe R language specifies a particular syntax on how to build a function. For example, a naive_mean could be defined as:\n\nnaive_mean = function(x, na.remove = FALSE) {\n  total = sum(x, na.rm = na.remove)\n  n = length(x[!is.na(x)])\n  result = total/n\n  return(result)\n}\n\nIn this case, the function naive_mean has two arguments (x and na.remove) where the second argument has a default value of FALSE and the body consists of several lines of code. These are respectively the sum of the elements of x with the na.rm depending on whether you specified TRUE or FALSE in the na.remove argument; n that calculates the length of the vector x without NAs, and the calculation of the mean. The last statement returns the result. Notice that arguments are separated by commas and the body of the function is enclosed in curly braces {}. The name of the function is simply the name of the variable to which you assigned the function (i.e., naive_mean). You can see below that you can use this function in a similar manner to the built-in mean\n\nx = 1:10\nnaive_mean(x)\n\n[1] 5.5\n\n\nNotice that we did not specify the value of na.remove as the default is ok in this case. However, if we had missing values, the NA would propagate to the output:\n\nx = c(1,2,NA,4)\nnaive_mean(x)\n\n[1] NA\n\n\nSpecifying na.remove=FALSE can be used as a double check that there are no NAs in your vector. If they are present it forces us to make a decision about what to do with the NAs. Let’s say that, for the moment, we want to just remove the values that are NA from the calculation. In this case, we just change the value of the default parameter.\n\nnaive_mean(x, na.remove = TRUE)\n\n[1] 2.333333\n\n\nFor convenience, default parameters are specified by name rather than position. However we could have also said naive_mean(x,TRUE) or even naive_mean(x = x, na.remove = TRUE). All these forms of calling functions are OK, whether you choose one style or another is a matter of taste.\n\n\n\nBuild a function to calculate the standard deviation (\\(\\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n\\left(x_i - \\bar x\\right)^2}{n - 1}}\\)). Test your function with some data that includes missing values, and compare to the built in function for the standard deviation sd.\n\n\n\n\n\nsigma.self = function(x,na.rm=F){\nmean.x = mean(x,na.rm=na.rm)\n\nn = length(na.omit(x))\n\nsd = sqrt(sum((x-mean.x)^2,na.rm=na.rm)/(n-1))\n\nreturn(sd)\n}\n\n\nSuprisingly the base R does not have a built in function for the standard error of the mean (s.e.m.). The sem is defined as \\(\\frac{\\sigma}{\\sqrt(n)}\\).\n\n\n\nMake you own function for the sem and use your own home-made function of the standard deviation for that.\n\n\n\n\n\nsem.self <- function(x,na.rm=F){\nlength.x <- length(na.omit(x)) sigma.self(x,na.rm=na.rm)/sqrt(length(x)) }\n\n\nAs you see you can call functions inside functions. It is recommended to divide the work you want to do into little functions that each carry out a specific task, and then combine those functions into a larger function that combines these tasks. This facilitates error checking.\n\n\n6 Numerical explorations: plotting curves\nHere are the R commands used to generate Figure 3.2 in the book (p 74). They just use curve(), with add=FALSE (the default, which draws a new plot) and add=TRUE (adds the curve to an existing plot), particular values of from and to, and various graphical parameters (ylim, ylab, lty).\n\ncurve(2*exp(-x/2),from=0,to=7,ylim=c(0,2),ylab=\"\")\ncurve(2*exp(-x),add=TRUE,lty=4)\ncurve(x*exp(-x/2),add=TRUE,lty=2)\ncurve(2*x*exp(-x/2),add=TRUE,lty=3)\ntext(0.4,1.9,expression(paste(\"exponential: \",2*e^(-x/2))),adj=0)\ntext(4,.5,expression(paste(\"Ricker: \",x*e^(-x/2))))\ntext(4,1,expression(paste(\"Ricker: \",2*x*e^(-x/2))),adj=0)\ntext(2.8,0,expression(paste(\"exponential: \",2*e^(-x))))\n\nThe only new thing in this figure is the use of expression() to add a mathematical formula to an R graphic. text(x,y,\"x^2\") puts x^2 on the graph at position \\((x,y)\\); text(x,y,expression(x^2)) (no quotation marks) puts \\(x^2\\) on the graph. See ?plotmath or ?demo(plotmath) for (much) more information.\nAn alternate way of plotting the exponential parts of this curve:\n\nxvec = seq(0,7,length=100)\nexp1_vec = 2*exp(-xvec/2)\nexp2_vec = 2*exp(-xvec)\n\n\nplot(xvec,exp1_vec,type=\"l\",ylim=c(0,2),ylab=\"\")\nlines(xvec,exp2_vec,lty=4)\n\nFinally, if you have a more complicated function you could use sapply() to call this function along with appropriate parameter values. you could say:\n\nexpfun = function(x,a=1,b=1) {\n   a*exp(-b*x)\n }\nexp1_vec = sapply(xvec,expfun,a=2,b=1/2)\nexp2_vec = sapply(xvec,expfun,a=2,b=1)\n\nThe advantage of curve() is that you don’t have to define any vectors: the advantage of doing things the other way arises when you want to keep the vectors around to do other calculations with them.\n\n\n\nConstruct a curve that has a maximum at (\\(x=5\\), \\(y=1\\)). Write the equation, draw the curve in R, and explain how you got there.\n\n\n\n\n\nFor example: \\(-(x-5)^2+1\\)\ncurve(-(x-5)^2+1,from=-10,to=10)\nabline(v=5)\nabline(h=1)\n\n\n\n\n7 A quick digression: ifelse() for piecewise functions\nThe ifelse() command in R is useful for constructing piecewise functions. Its basic syntax is ifelse(condition,value_if_true,value_if_false), where condition is a logical vector (e.g. x>0), value_if_true is a vector of alternatives to use if condition is TRUE, and value_if_false is a vector of alternatives to use if condition is FALSE. If you specify just one value, it will be expanded (recycled in R jargon) to be the right length. A simple example:\n\nx=c(-25,-16,-9,-4,-1,0,1,4,9,16,25)\nsqrt(ifelse(x<0,0,x))\n\n [1] 0 0 0 0 0 0 1 2 3 4 5\n\n\nif you said ifelse(x<0,0,sqrt(x))) you would get a warning: why)\nHere are some examples of using ifelse() to generate (1) a simple threshold; (2) a Holling type I or “hockey stick”; (3) a more complicated piecewise model that grows exponentially and then decreases linearly; (4) a double-threshold model. When plotting functions with abrubt changes with the function curve, beware that curve draws a line by evaluating a functions at several locations along the specified interval (from, to). You can increase these number of points by specifying n. The default value for n is 101.\n\nop=par(mfrow=c(2,2),mgp=c(2,1,0),mar=c(4.2,3,1,1))\ncurve(ifelse(x<2,1,3),from=0,to=5)\ncurve(ifelse(x<2,2*x,4),from=0,to=5)\ncurve(ifelse(x<2,exp(x),exp(2)-3*(x-2)),from=0,to=5)\ncurve(ifelse(x<2,1,ifelse(x<4,3,5)),from=0,to=5)\n\n\n\n\nThe double-threshold example (nested ifelse() commands) probably needs more explanation. In words, this command would go “if \\(x\\) is less than 2, set \\(y\\) to 1; otherwise (\\(x \\ge 2\\)), if \\(x\\) is less than 4 (i.e. \\(2 \\le x<4\\)), set \\(y\\) to 3; otherwise (\\(x \\ge 4\\)), set \\(y\\) to 5”.\n\n\n8 Evaluating derivatives in R\nR can evaluate derivatives, but it is not very good at simplifying them. In order for R to know that you really mean (e.g) x^2 to be a mathematical expression and not a calculation for R to try to do (and either fill in the current value of x or give an error that x is undefined), you have to specify it as expression(x^2); you also have to tell R (in quotation marks) what variable you want to differentiate with respect to:\n\nd1 = D(expression(x^2),\"x\"); d1\n\n2 * x\n\n\nUse eval() to fill in a list of particular values for which you want a numeric answer:\n\neval(d1,list(x=2))\n\n[1] 4\n\n\nTaking the second derivative:\n\nD(d1,\"x\")\n\n[1] 2\n\n\n(As of version 2.0.1,) R knows how to take the derivatives of expressions including all the basic arithmetic operators; exponentials and logarithms; trigonometric inverse trig, and hyperbolic trig functions; square roots; and normal (Gaussian) density and cumulative density functions; and gamma and log-gamma functions. You’re on your own for anything else (consider using a symbolic algebra package like Mathematica or Maple, at least to check your answers, if your problem is very complicated). deriv() is a slightly more complicated version of D() that is useful for incorporating the results of differentiation into functions: see the help page."
  },
  {
    "objectID": "Lab4/no_solution.html#jensens-inequality",
    "href": "Lab4/no_solution.html#jensens-inequality",
    "title": "Lab 4: Probability distributions",
    "section": "4.1 Jensen’s inequality",
    "text": "4.1 Jensen’s inequality\nJensen’s inequality states the following: Suppose you have a number of values, \\(x\\), with a mean \\(\\bar{x}\\), and a non-linear function \\(f(x)\\). Then the mean of \\(f(x)\\) is not equal to \\(f(\\bar{x})\\).\nJensen’s inequality can be important in a number of cases. The first one is mentioned in Ch. 4 (page 104) how variability can change the mean behaviour of a system (damselfish). Another example where Jensen’s inequality kicks in is when transforming your data. Data-transformations are commonly applied to get normally distributed errors.\nIn statistical models, you often estimate the mean effect of a given treatment.\n\n\n\nFind out what the effect of Jensen’s inequality is on a series of log-tranformed datapoints with respect to the estimated mean.\nUse the following pseudo-code: 1. Generate 10 random deviates from a uniform distribution (choose the range of 0 to 10). 2. Calculate the mean of those 10 deviates. 3. Plot the function \\(\\log(x)\\) with curve on the range from 0-10, and plot your numbers onto it 4. Calculate the mean of the log-transformed values and transform this mean back the normal scale, and compare to the mean calculated at 1. 5. Plot the means with abline(h=...) if you want to draw a horizontal line or abline(v=...) to draw a vertical line. 6.Explain differences between the two means.\n\n\n\n\n\nThis exercise shows that it is usually a good idea to leave variables untransformed when estimating the properties from this data."
  },
  {
    "objectID": "Lab4/no_solution.html#zero-inflated-distributions",
    "href": "Lab4/no_solution.html#zero-inflated-distributions",
    "title": "Lab 4: Probability distributions",
    "section": "6.1 Zero-inflated distributions",
    "text": "6.1 Zero-inflated distributions\nThe general formula for the probability distribution of a zero-inflated distribution, with an underlying distribution \\(P(x)\\) and a zero-inflation probability of \\(p_z\\), is: \\[\\begin{eqnarray*}\n\\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\\\\n\\mbox{Prob}(x>0) & = & (1-p_z) P(x)\n\\end{eqnarray*}\\] So, for example, we could define a probability distribution for a zero-inflated negative binomial as follows:\n\ndzinbinom = function(x,mu,size,zprob) {\n  ifelse(x==0,\n         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),\n         (1-zprob)*dnbinom(x,mu=mu,size=size))\n}\n\n(the name, dzinbinom, follows the R convention for a probability distribution function: a d followed by the abbreviated name of the distribution, in this case zinbinom for “zero-inflated negative binomial”).\nThe ifelse() command checks every element of x to see whether it is zero or not and fills in the appropriate value depending on the answer.\nA random deviate generator would look like this:\n\nrzinbinom = function(n,mu,size,zprob) {\n  ifelse(runif(n)<zprob,\n         0,\n         rnbinom(n,mu=mu,size=size))\n}\n\nThe command runif(n) picks n random values between 0 and 1; the ifelse command compares them with the value of zprob. If an individual value is less than zprob (which happens with probability zprob=\\(p_z\\)), then the corresponding random number is zero; otherwise it is a value picked out of the appropriate negative binomial distribution.\n\n\n\nCheck graphically that these functions actually work. For instance, you could compare the results with a negative binomial function with the same mean and variance as the data."
  },
  {
    "objectID": "Lab4/solution.html#jensens-inequality",
    "href": "Lab4/solution.html#jensens-inequality",
    "title": "Lab 4: Probability distributions",
    "section": "4.1 Jensen’s inequality",
    "text": "4.1 Jensen’s inequality\nJensen’s inequality states the following: Suppose you have a number of values, \\(x\\), with a mean \\(\\bar{x}\\), and a non-linear function \\(f(x)\\). Then the mean of \\(f(x)\\) is not equal to \\(f(\\bar{x})\\).\nJensen’s inequality can be important in a number of cases. The first one is mentioned in Ch. 4 (page 104) how variability can change the mean behaviour of a system (damselfish). Another example where Jensen’s inequality kicks in is when transforming your data. Data-transformations are commonly applied to get normally distributed errors.\nIn statistical models, you often estimate the mean effect of a given treatment.\n\n\n\nFind out what the effect of Jensen’s inequality is on a series of log-tranformed datapoints with respect to the estimated mean.\nUse the following pseudo-code: 1. Generate 10 random deviates from a uniform distribution (choose the range of 0 to 10). 2. Calculate the mean of those 10 deviates. 3. Plot the function \\(\\log(x)\\) with curve on the range from 0-10, and plot your numbers onto it 4. Calculate the mean of the log-transformed values and transform this mean back the normal scale, and compare to the mean calculated at 1. 5. Plot the means with abline(h=...) if you want to draw a horizontal line or abline(v=...) to draw a vertical line. 6.Explain differences between the two means.\n\n\n\n\n\n\nrf = runif(10,min=0,max=10)\nmean(rf)\nplot(log(rf)~ rf) curve(log(x),add=T)\nexp(mean(log(rf))) versus mean(rf)\nsegments(x0=0,y0=log(mean(rf)),x1=mean(rf),              y1=log(mean(rf)),lty=1) segments(x0=mean(rf),y0=0,x1=mean(rf),             y1=log(mean(rf)),lty=1) segments(x0=0,y0=mean(log(rf)),x1=exp(mean(log(rf))),           y1=mean(log(rf)),lty=2)\n\nA dotted line for the mean of the log transformed values\nsegments(x0=exp(mean(log(rf))),y0=mean(log(rf)),           x1=exp(mean(log(rf))),           y1=min(log(rf)),lty=2)\nBy doing a log transformation first, the higher values are “compressed” and weigh less into the mean.\n\n\nThis exercise shows that it is usually a good idea to leave variables untransformed when estimating the properties from this data."
  },
  {
    "objectID": "Lab4/solution.html#zero-inflated-distributions",
    "href": "Lab4/solution.html#zero-inflated-distributions",
    "title": "Lab 4: Probability distributions",
    "section": "6.1 Zero-inflated distributions",
    "text": "6.1 Zero-inflated distributions\nThe general formula for the probability distribution of a zero-inflated distribution, with an underlying distribution \\(P(x)\\) and a zero-inflation probability of \\(p_z\\), is: \\[\\begin{eqnarray*}\n\\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\\\\n\\mbox{Prob}(x>0) & = & (1-p_z) P(x)\n\\end{eqnarray*}\\] So, for example, we could define a probability distribution for a zero-inflated negative binomial as follows: ::: {.cell hash=‘solution_cache/html/unnamed-chunk-34_b38068316938fabe66249775204df679’}\ndzinbinom = function(x,mu,size,zprob) {\n  ifelse(x==0,\n         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),\n         (1-zprob)*dnbinom(x,mu=mu,size=size))\n}\n::: (the name, dzinbinom, follows the R convention for a probability distribution function: a d followed by the abbreviated name of the distribution, in this case zinbinom for “zero-inflated negative binomial”).\nThe ifelse() command checks every element of x to see whether it is zero or not and fills in the appropriate value depending on the answer.\nA random deviate generator would look like this: ::: {.cell hash=‘solution_cache/html/unnamed-chunk-35_bb81d5e1361896d936e0d8b25021a780’}\nrzinbinom = function(n,mu,size,zprob) {\n  ifelse(runif(n)<zprob,\n         0,\n         rnbinom(n,mu=mu,size=size))\n}\n::: The command runif(n) picks n random values between 0 and 1; the ifelse command compares them with the value of zprob. If an individual value is less than zprob (which happens with probability zprob=\\(p_z\\)), then the corresponding random number is zero; otherwise it is a value picked out of the appropriate negative binomial distribution.\n\n\n\nCheck graphically that these functions actually work. For instance, you could compare the results with a negative binomial function with the same mean and variance as the data.\n\n\n\n\n\nrzinbinom = function(n,mu,size,zprob) {\nifelse(runif(n)<zprob,\n     `0,rnbinom(n,mu=mu,size=size))`\n}\na = rzinbinom(1000,mu=4,size=1,zprob=0.2)\nmean.a = mean(a)\nvar.a = var(a)\nsize = 1/(((var.a - mean.a))/mean.a^2)\na1 = rnbinom(1000,mu=mean.a,size=size)\nx = as.numeric(names(table(a)))\nplot(as.numeric(table(a))~ x,type=\"h\")\nx = as.numeric(names(table(a1)))\npoints(as.numeric(table(a1))~ x,type=\"p\")"
  },
  {
    "objectID": "Lab6/no_solution.html",
    "href": "Lab6/no_solution.html",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "",
    "text": "You will learn how to:\n\nProgram the likelihood function of a model.\nEstimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.\nEstimate the confidence intervals of the model parameters through profiling and the quadratic approximation.\nEstimate parameters in a Bayesian framework and how parameter uncertainty can be assessed\n\nIn case of time constraints, focus on sections 2-5. If you want something challenging do 6,7 and 8 as well."
  },
  {
    "objectID": "Lab6/no_solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "href": "Lab6/no_solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "3.1 Finding the maximum likelihood estimate of the paramaters",
    "text": "3.1 Finding the maximum likelihood estimate of the paramaters\n\n\n\nTake the steps below\n\nGenerate 50 values from a negative binomial (rnbinom) with \\(\\mu=1\\), \\(k=0.4\\). Save the values in variables in case we want to use them again later.\nPlot the numbers in a frequency diagram\nNext, define the negative log-likelihood function for a simple draw from a negative binomial distribution: the first parameter, par, will be the vector of parameters, and the second parameter, dat, will be the vector with simulated values.\nCalculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Combine these parameter values into the vector par with c() to pass them to the negative log-likelihood function. Naming the elements in the parameter vector is optional but can help avoid mistakes if the number o fparameters is large (e.g. par = c(mu = 1,k = 2)).\nCalculate the NLL of parameter values that are far from the values that were used to generate the data (\\(\\mu=10\\), \\(k=10\\))\nCalculate the maximum likelihood estimate (MLE)? Use optim with the default options (Nelder-Mead simplex method) and the method-of-moments estimates as the starting estimates (par): opt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)\nWhat is the difference in NLL between the MLE estimates and the NLL derived at 5?\n\nThe Likelihood Ratio Test would say, however, that the difference in likelihoods would have to be greater than \\(\\chi^2_2(0.95)/2\\) (two degrees of freedom because we are allowing both \\(\\mu\\) and \\(k\\) to change). This can be done through ldiff=nll.true-nll.mom and qchisq(0.95,df=2)/2. So — better, but not significantly better at \\(p=0.05\\). pchisq(2*ldiff,df=2,lower.tail=FALSE) would tell us the exact \\(p\\)-value if we wanted to know.)\n\n\n\n\n\n\nset.seed(1001)\nmu.true=1\nk.true=0.4\n\nx = rnbinom(50,mu=mu.true,size=k.true)\n\nplot(table(factor(x,levels=0:max(x))),\n   ylab=\"Frequency\",xlab=\"x\")\n\n\n\n# this function calculate the NLL of the data given the set of parameters defined in p\nNLLfun1 = function(p,dat=x) {\nmu=p[1]\nk=p[2]\n-sum(dnbinom(x,mu=mu,size=k,log=TRUE))\n}\n\n# the NLL of the data given the parameter values that were used to generate the data\nnll.true=NLLfun1(c(mu=mu.true,k=k.true))\n\nnll.true\n\n[1] 72.64764\n\nNLLfun1(c(mu=10,k=10))\n\n[1] 291.4351\n\nm = mean(x)\nv = var(x)\n# calculate parameters through method of moments\nmu.mom = m\nk.mom = m/(v/m-1)\n\n# find MLE estimate of the parameters given the data\nopt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)\ncoef(opt1)\n\nNULL\n\n# NLL at MLE\nopt1$value\n\n[1] 71.79646\n\n# compare with nll.true\n\n# significantly different?\nldiff=nll.true-opt1$value; ldiff\n\n[1] 0.8511813\n\n# no significant difference (which is what we would expect in 95% the generated datasets)\npchisq(2*ldiff,df=2,lower.tail=FALSE)\n\n[1] 0.4269103\n\n\nThe minimum negative log-likelihood (round(opt1$value,2)) is better than the NLL of the model with the true parameters (round(nll.true,2)), but all of these are within the LRT cutoff, i.e. the negative log likelihoods differ by less than 1.92. Remember that the cut-off is based on the Likelihood Ratio Test that states that twice the difference in the log-likelihood between the simpler and more complex model will follow a \\(\\chi^2\\) distribution with n degrees of freedom. \\(n\\) is the number of parameters that are fixed to a specific value. The cut-off value for a \\(\\chi^2\\) with 1 degree of freedom is 3.84. The value of 1.92 is derived from 3.84/2 because we evaluate the difference in log Likelihood and not twice the difference. In other words, we could also multiply all the logLikelihood surface by two and find the 3.84 cutoff."
  },
  {
    "objectID": "Lab6/no_solution.html#likelihood-surface",
    "href": "Lab6/no_solution.html#likelihood-surface",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.1 Likelihood surface",
    "text": "6.1 Likelihood surface\nTo find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues on exercise #3.1 (Lab 3) where you used the negative binomial to generate 50 numbers and fitted back the parameters.\n\n\n\nFor the likelihood surface:\n\nSet up vectors of \\(\\mu\\) and \\(k\\) values. Let’s try \\(\\mu\\) from 0.4 to 3 in steps of 0.05 and \\(k\\) from 0.01 to 0.7 in steps of 0.01.\nSet up a matrix to hold the results, The matrix for the results will have rows corresponding to \\(\\mu\\) and columns corresponding to \\(k\\):\nRun for loops to calculate and store the values. Use a for nested in another one\nDrawing a contour using the function ‘contour’. Change the argument nlevels to 100 to get a better view of the likelihood surface\nAdd the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments."
  },
  {
    "objectID": "Lab6/no_solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "href": "Lab6/no_solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.2 Optimisation problems and assessing the confidence limits of parameter estimates",
    "text": "6.2 Optimisation problems and assessing the confidence limits of parameter estimates\nFitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question 5.2 and practise a bit more.\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and optim method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\n\nshapes1= read.csv(\"shapes1.csv\")\nplot(shapes1)\n\nnll.mle = function(a,b,sd){\n  # this calculates the mean y for a given value of x: the deterministic function\n  mu = a*(1-exp(-b*shapes1$x))\n  # this calculates the likelihood of the function given the probability\n  # distribution, the data and mu and sd\n  nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))\n  return(nll)\n}\n\nlibrary(bbmle)\n\n# Try 4 different starting points\nmle2.1 = vector(\"list\", 4)\nstart_a = c(5,10,20,30)\nstart_b = c(0.001,0.005,0.01,0.1)\nfor(i in 1:4) {\n  mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method=\"Nelder-Mead\")\n}\n\n# Check the best fit (in this case it is 3rd starting point)\nfor(i in 1:4) {\n  print(logLik(mle2.1[[i]]))\n}\n\n# Extract the best fit for the rest of the analysis\nbest_mle2.1 = mle2.1[[3]]\nsummary(best_mle2.1)\nlogLik(best_mle2.1)\nconfint(best_mle2.1)\ncoef(best_mle2.1)\n\nplot(shapes1)\ncurve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)\ncurve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)\n\n# likelihood surface\na1 = seq(0,40,length.out = 100)\nb1.1 = seq(0.001,0.03,length.out=100)\nb1.2 = seq(0.1,10,length.out=100)\n\nnll.grid = expand.grid(a1,b1.1)\nnll.grid$NLL = NA\nno = 0\n# Construct first contour\nfor (i in 1:length(a1)){\n  for (j in 1:length(b1.1)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.1[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)\n  }\n}\nlibrary(reshape2)\nz1.1 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])\n\n# Construct second contour\nno = 0\nfor (i in 1:length(a1)){\n  for (j in 1:length(b1.2)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.2[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)\n  }\n}\nz1.2 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])\n\n# Plot the two contours\npar(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)\ncontour(a1,b1.2,z1.2,nlevels = 20, xaxt = \"n\", yaxt = \"n\", ylim = c(0,9))\naxis(2, seq(1,9,2))\npoints(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)\npoints(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)\npoints(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)\npoints(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)\ncontour(a1,b1.2,z1.2,levels=120,col=2,add=T)\n\npar(mar = c(3.5,4,0.5,1))\ncontour(a1,b1.1,z1.1,nlevels = 20)\npoints(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)\npoints(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)\npoints(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)\npoints(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)\ncontour(a1,b1.1,z1.1,levels=120,col=2,add=T)\n\n\n\n# profile\nnll.mle1 = function(a,sd){\n  # this calculates the mean y for a given value of x: the deterministic function\n  mu = a*(1-exp(-b*x))\n  # this calculates the likelihood of the function given the probability\n  # distribution, the data and mu and sd\n  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))\n  return(nll)\n}\n\nnll = numeric(length(b1.1))\nfor (i in 1:length(b1.1)){\n  b = b1.1[i]\n  mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method=\"Nelder-Mead\")\n  nll[i] = -logLik(mle.21)\n}\npar(mfrow = c(1,1))\nplot(nll~ b1.1,type=\"l\",xlim=c(0.008,0.012), ylim = c(117,125))\nwhich.min(nll)\n\n# cutoff\n-logLik(best_mle2.1) + qchisq(0.95, 1)/2\nwhich(nll < 119.852)\nb1.1[c(23,35)]\n\nplot(nll~ b1.1,type=\"l\",xlim=c(0.0070,0.012),ylim=c(116,125))\nabline(v=c(0.00744,0.01096),lty=2)\nabline(v=0.008968,lty=1,lwd=2)\nabline(v=c(0.00738,0.01103),lty=2,col=\"red\")\n\nse.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])\nb + c(-1,1)*qnorm(0.975) * se.mu\nconfint(best_mle2.1)\n\nabline(v=c(0.007177,0.0107589),col=\"blue\")"
  },
  {
    "objectID": "Lab6/no_solution.html#bayesian-parameter-estimation-negative-binomial",
    "href": "Lab6/no_solution.html#bayesian-parameter-estimation-negative-binomial",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.3 Bayesian parameter estimation: negative binomial",
    "text": "6.3 Bayesian parameter estimation: negative binomial\nIn this section we will practice parameter estimation using the Bayesian method on the same negative binomial example as before (Section 6.1). The purpose of this exercise is to gain intuition of how Markov Chain Monte Carlo (MCMC) algorithms work and better understand the differences (and similarities) between maximum likelihood and Bayesian parameter estimation. The MCMC algorithm implemented below can be useful for relatively simple models such as the ones covered in this course. For more complex data analysis we recommend to use dedicated R packages that implement more powerful (and automated) algorithms. A list of such packages can be found in the task view on Bayesian Inference (https://cran.r-project.org/web/views/Bayesian.html).\n\n6.3.1 From Bayes rule to log posterior\nThe aim of Bayesian analysis is to estimate the parameters of a model conditional on observed data (\\(P(\\theta | D)\\), known as posterior distribution) given the likelihood (\\(L(\\theta|D) = P(D|\\theta)\\)) and prior distributions of the parameters (\\(P(\\theta)\\)), according to Bayes rule:\n\\[\nP(\\theta | D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]\nDetails on Bayes rule are given in section 4.3 and 6.2.2 of the book. Note that the only unknown in the right hand side of Bayes rule is \\(P(D)\\). However, we know that \\(P(D) = \\int P(D|\\theta)P(\\theta)d\\theta\\). Therefore, in order to calculate the posterior distribution, we could calculate this integral. Any integration method would work, but integration will not be feasible for a large number of parameters. In practice, a more popular approach is to generate samples from the posterior distribution, while avoiding the integral. This is achieved by so-called Markov Chain Monte Carlo (MCMC) algorithms. These algorithms will provide a random sample from the posterior distribution given a formulation of the problem as:\n\\[\n\\log(P(\\theta | D)) \\propto \\log(P(D|\\theta)) + \\log(P(\\theta)) = \\mathcal{L} + \\log(P(\\theta))\n\\]\nWhere \\(\\mathcal{L}\\) is the positive log-likelihood and \\(\\propto\\) means “proportional to”. These algorithms work with logarithms for the same reason as in maximum likelihood estimation (i.e., to avoid numerical instability due to very large or very small numbers that would result from multiplication).\nThe first step of Bayesian parameter estimation is to build a function that calculates the log-posterior density for every parameter value. We will use the example of the negative binomial from section 6.1. This example fits a negative binomial distribution parameterized by its mean (mu) and size (k) both of which have to be positive.\nIn a Bayesian approach, we need to assign prior probabilities to each of the parameters, which means choosing a distribution and its parameters, based on prior knowledge. Of course, without a context, it is not possible to specify meaningful prior distributions (and this is arguably the hardest step in any Bayesian analysis), but for the sake of this exercise let’s assume that we can represent our prior beliefs with Normal distributions centered around 0 and with a standard deviation of 2 (in practice only half of these prior distributions are being used as mu and k are positive, but that is fine). This essentially means that were are 99% certain that mu and k will be lower than 4.6, prior to seeing any data.\nWe have to construct a function that can return the sum of the log-likelihood and log-prior densities for a given combination of mu and k in order to use MCMC (remember, this is not the exact log-posterior because of the unknown normalizing constant):\n\nLPfun1 = function(p, dat = x) {\n  # Mean and size of the negative binomial (use exp to force them to be positive)\n  mu = exp(p[1])\n  k  = exp(p[2])\n  # Logarithm of the prior distributions on mu and k\n  # (0 and 2 are parameters chosen by the user, they represent prior beliefs)\n  lp_mu = dnorm(mu, 0, 2, log = TRUE)\n  lp_k = dnorm(k, 0, 2, log = TRUE)\n  log_prior = lp_mu + lp_k\n  # Log-likelihood of the data under the model\n  LL = sum(dnbinom(dat,mu=mu,size=k,log=TRUE))\n  # Sum of the log-likelihood and the log-prior\n  LL + log_prior\n}\n\nThe main difference between LPfun1 and NLLfun1 created in section 6.1 is that the new function includes the log-prior densities of mu and k (lp_mu and lp_k, respectively) and that it returns the sum of log-likelihood + log-prior densities.\n\n\n6.3.2 Sampling from posterior: Metropolis-Hastings algorithm\nBelow is a simple version of the Metropolis-Hastings algorithm (section 7.3.1 of the book), with a multivariate Normal proposal distribution (you need to install package mvtnorm first!). Note that this function is written in a generic fashion, that is, it will work with any user-defined function that is assigned to the first argument (model) and any data required by said function is passed through the ... argument (this is the strategy is used in many R functions, including optim).\nThe inputs of the MH function (see below for code) are:\n\nmodel: Function that calculates the non-normalized log-posterior (i.e. LPfun1).\ninit: Initial values for the parameters. The closer to the “true” values the faster the MCMC algorithm will converge to the posterior distribution.\nsigma: Variance-covariance matrix of the proposal distribution used to calculate jumps in parameter space.\nniter: Number of iterations the algorithm will run for.\nburn: Fraction of iterations that will be used as burn-in (check section 7.3.2). These iterations will not be used for analysis but are required for convergence of the MCMC algorithm.\nseed: Seed for pseudo-random number generator that allows reproducing results.\n\nThe algorithm keeps track of all the parameter values it visits and stores them in the variable chain. Each iteration, it proposes new values for each parameter (proposal) sampled from a multivariate Normal distribution centered at the current values. The probability of accepting the proposal is equal to the exponent of the difference in log posterior densities (paccept, see Equation 7.3.2 in the book, taking into account that the proposal distribution is symmetric). If the proposal is accepted, then it is added to the chain and becomes the new current values (i.e., the algorithm “moves” to that location). After the run is finished, the values in chain are split between the burn-in samples and after burn-in. The variable acceptance calculates the fraction of jumps that were accepted (do not confuse with the probability of accepting an individual jump!). The higher this number is, the more efficient the algorithm is in exploring the posterior distribution.\n\nlibrary(mvtnorm)\n\nWarning: package 'mvtnorm' was built under R version 4.3.1\n\nMH = function(model, init, Sigma = diag(init/10), niter = 3e4, burn = 0.5,\n                      seed = 1134, ...) {\n  # To make results reproducible you should set a seed (change among chains!!!)\n  set.seed(seed)\n  # Pre-allocate chain of values\n  chain = matrix(NA, ncol = length(init), nrow = niter)\n  # Chain starts at init\n  current = init\n  lp_current = model(current, ...)\n  # Iterate niter times and update chain\n  for(iter in 1:niter) {\n    # Generate proposal values from multivariate Normal distribution\n    proposal = rmvnorm(1, mean = current, sigma = Sigma)\n    # Calculate probability of acceptance (proposal distribution is symmetric)\n    lp_proposal = model(proposal, ...)\n    paccept = min(1, exp(lp_proposal - lp_current))\n    # Accept the proposal... or not!\n    # If accept, update the current and lp_current values\n    accept = runif(1) < paccept\n    if(accept) {\n      chain[iter,] = proposal\n      lp_current = lp_proposal\n      current = chain[iter,]\n    } else {\n      chain[iter,] = current\n    }\n  }\n  # Calculate the length of burn-in\n  nburn = floor(niter*burn)\n  # Calculate final acceptance probability after burn-in (fraction of proposals accepted)\n  acceptance = 1 - mean(duplicated(chain[-(1:nburn),]))\n  # Package the results\n  list(burnin = chain[1:nburn,], sample = chain[-(1:nburn),],\n       acceptance = acceptance, nburn = nburn)\n}\n\nSo let’s tackle the negative binomial problem with the algorithm above. First, let’s regenerate the data that was used in the previous section:\n\nset.seed(1001)\nmu.true=1\nk.true=0.4\nx = rnbinom(50,mu=mu.true,size=k.true)\n\nNow we can run MH with some values. I want to make the point that choosing a good proposal distribution matters for an efficient MCMC algorithm. So let’s start with a variance-covariance matrix that is not reasonable (because it is too wide):\n\nSigma = diag(c(10,10))\n\nNow we can run MH combined with the LPfun1 function and some initial values:\n\ninit = log(c(1,1))\nbay1 = MH(LPfun1, init, Sigma, burn = 0.3, dat = x)\n\nThe first result you want to check is the acceptance probability to see how succesful proposals were:\n\nbay1$acceptance\n\n[1] 0.01957143\n\n\nThis is terrible! 98% of the proposed values were rejected so it would take really long to get a representative sample from the posterior distribution. The next step is usually to take a look at the traces of the values sampled by the MCMC (noticed that the sampling was done on the log transformation of the parameters as they are positive):\n\npar(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)\nplot(bay1$sample[,1], t = \"l\", ylab = \"Trace of log(mu)\")\nplot(bay1$sample[,2], t = \"l\", ylab = \"Trace of log(k)\")\n\n\n\n\n\n\n\n\nThe low probability of acceptance means that the traces look like “squiggly lines”, getting stuck at different values for hundreds of iterations (i.e., horizontal sections in the traces). This slows down the effective sampling and can introduce biases in the estimates (unless the algorithm runs for very long).\nNote that more modern MCMC algorithms (that R packages specialized on Bayesian statistics will use internally) will automatically tune the proposal distribution or even use alternatives methods to propose values that are more robust. However a poor man’s tuned MCMC may suffice for this introduction (and for simple models with few parameters) and it works as follows:\n\nCalculate the value that maximizes the posterior distribution using optim (a.k.a Maximum A Posteriori estimate or MAP for short).\nEstimate the variance-covariance matrix of the posterior distribution using the Hessian matrix returned by optim (analogous to what we do for maximum likelihood estimation).\nRun MH using the above results as the values for init and sigma, respectively.\n\nThe reason why this works better is because points 1 and 2 will often give a good first approximation of the posterior distribution, especially for large data (in which case the posterior distribution approaches a Normal distribution). This means that MH will be sampling from a distribution similar to the target distribution and hence a higher proportion of proposals will be accepted (intuitively, fewer values that are far in the tails of the posterior distribution will be proposed). This approach can be implemented as:\n\nmapfit = optim(fn = LPfun1, par = log(c(1,1)),\n               hessian = TRUE, method = \"BFGS\",\n               control = list(fnscale = -1), dat = x)\nSigma = solve(-mapfit$hessian)\ninit = mapfit$par\nbay2 = MH(LPfun1, init, Sigma, burn = 0.3)\n\nNotice that we should use control = list(fnscale = -1) because we want to maximize the posterior probability, not minimize it. That is also the reason why I add a negative sign in front of the Hessian as in solve(-mapfit$hessian) (in previous examples we were minimizing the negative log likelihood and we did not include the negative sign in front of the Hessian).\nWe can see that the matrix Sigma obtained from the Hessian around the MAP estimate is different from the one assumed in the first MH run (specifically, the variances are much smaller):\n\nSigma\n\n            [,1]        [,2]\n[1,] 0.078897091 0.002920995\n[2,] 0.002920995 0.122861528\n\n\nThese lower variances mean that the Markov chain does not wonder far into the tails of the posterior distribution but rather remains in the area of high probability. Thus, the new run has a higher acceptance probability:\n\nbay2$acceptance\n\n[1] 0.5599524\n\n\nNow this is the good. 56% of the time the candidates will be accepted, ensuring that the chain samples efficiently from the posterior. The traces will approach white noise (these are often called “fuzzy caterpillars” in the community):\n\npar(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)\nplot(bay2$sample[,1], t = \"l\", ylab = \"Trace of log(mu)\")\nplot(bay2$sample[,2], t = \"l\", ylab = \"Trace of log(k)\")\n\n\n\n\n\n\n\n\nAt this point we would normally calculate more diagnostics to build up more confidence on the results of the MCMC chains, but we will keep it simple in this introduction. The object bay2$sample contains a random sample from the posterior from which we can calculate several properties. First, remember that we took the logarithm of the parameters to avoid negative values, so we need to undo this transformation in the result:\n\nbay2sample = exp(bay2$sample)\n\nWe can visualize the estimates for each parameter using density (more common) or hist (easier to interpret):\n\npar(mfrow = c(1,2), mar = c(4,4,1.5,1))\nhist(bay2sample[,1], main = \"Density of mu\", freq = F, xlim = c(0,4))\nhist(bay2sample[,2], main = \"Density of k\", freq = F, xlim = c(0,1))\n\n\n\n\nOne striking feature is that the distributions are not symmetric, they have a longer tail to the right. This is typical of positive parameters that are close to 0. A consequence of this is that the mean, median and mode of the distributions will differ (though in in this case not so much). Let’s compare all the estimates we have so far for the negative binomial fitted to these data:\n\nmap = exp(mapfit$par)\nmeanp = colMeans(bay2sample)\nmedianp = c(median(bay2sample[,1]), median(bay2sample[,2]))\ncbind(map, meanp, medianp,\n      mom = c(mu.mom, k.mom),\n      mle = opt1$par,\n      true = c(mu.true, k.true))\n\n         map     meanp  medianp       mom       mle true\nmu 1.2209093 1.2897290 1.222248 1.2600000 1.2602356  1.0\nk  0.2876172 0.2926038 0.274538 0.3778531 0.2884793  0.4\n\n\nFor this model, data and priors, all estimates are quite similar to each other across different methods of estimation. The reason is because there is sufficient data (50 points for 2 parameters is quite some data…) such that the priors have a negligible effect.\nThe 95% credible intervals (analogous to 95% confidence intervals) can be calculated with the quantile function applied directly to the sample from the posterior:\n\nt(apply(bay2sample, 2, quantile, probs = c(0.025, 0.975)))\n\n          2.5%     97.5%\n[1,] 0.7078096 2.2138543\n[2,] 0.1348619 0.5519341"
  },
  {
    "objectID": "Lab6/solution.html",
    "href": "Lab6/solution.html",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "",
    "text": "You will learn how to:\n\nProgram the likelihood function of a model.\nEstimate the parameters of a model through maximum likelihood, including models with continuous and categorical covariates.\nEstimate the confidence intervals of the model parameters through profiling and the quadratic approximation.\nEstimate parameters in a Bayesian framework and how parameter uncertainty can be assessed\n\nIn case of time constraints, focus on sections 2-5. If you want something challenging do 6,7 and 8 as well."
  },
  {
    "objectID": "Lab6/solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "href": "Lab6/solution.html#finding-the-maximum-likelihood-estimate-of-the-paramaters",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "3.1 Finding the maximum likelihood estimate of the paramaters",
    "text": "3.1 Finding the maximum likelihood estimate of the paramaters\n\n\n\nTake the steps below\n\nGenerate 50 values from a negative binomial (rnbinom) with \\(\\mu=1\\), \\(k=0.4\\). Save the values in variables in case we want to use them again later.\nPlot the numbers in a frequency diagram\nNext, define the negative log-likelihood function for a simple draw from a negative binomial distribution: the first parameter, par, will be the vector of parameters, and the second parameter, dat, will be the vector with simulated values.\nCalculate the negative log-likelihood of the data for the parameter values with which you generated the numbers. Combine these parameter values into the vector par with c() to pass them to the negative log-likelihood function. Naming the elements in the parameter vector is optional but can help avoid mistakes if the number o fparameters is large (e.g. par = c(mu = 1,k = 2)).\nCalculate the NLL of parameter values that are far from the values that were used to generate the data (\\(\\mu=10\\), \\(k=10\\))\nCalculate the maximum likelihood estimate (MLE)? Use optim with the default options (Nelder-Mead simplex method) and the method-of-moments estimates as the starting estimates (par): opt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)\nWhat is the difference in NLL between the MLE estimates and the NLL derived at 5?\n\nThe Likelihood Ratio Test would say, however, that the difference in likelihoods would have to be greater than \\(\\chi^2_2(0.95)/2\\) (two degrees of freedom because we are allowing both \\(\\mu\\) and \\(k\\) to change). This can be done through ldiff=nll.true-nll.mom and qchisq(0.95,df=2)/2. So — better, but not significantly better at \\(p=0.05\\). pchisq(2*ldiff,df=2,lower.tail=FALSE) would tell us the exact \\(p\\)-value if we wanted to know.)\n\n\n\n\n\nThe solution is shown below in a big R chunk\n\n\n\nset.seed(1001)\nmu.true=1\nk.true=0.4\n\nx = rnbinom(50,mu=mu.true,size=k.true)\n\nplot(table(factor(x,levels=0:max(x))),\n   ylab=\"Frequency\",xlab=\"x\")\n\n\n\n# this function calculate the NLL of the data given the set of parameters defined in p\nNLLfun1 = function(p,dat=x) {\nmu=p[1]\nk=p[2]\n-sum(dnbinom(x,mu=mu,size=k,log=TRUE))\n}\n\n# the NLL of the data given the parameter values that were used to generate the data\nnll.true=NLLfun1(c(mu=mu.true,k=k.true))\n\nnll.true\n\n[1] 72.64764\n\nNLLfun1(c(mu=10,k=10))\n\n[1] 291.4351\n\nm = mean(x)\nv = var(x)\n# calculate parameters through method of moments\nmu.mom = m\nk.mom = m/(v/m-1)\n\n# find MLE estimate of the parameters given the data\nopt1 = optim(fn=NLLfun1,par=c(mu=mu.mom,k=k.mom),hessian=TRUE)\ncoef(opt1)\n\nNULL\n\n# NLL at MLE\nopt1$value\n\n[1] 71.79646\n\n# compare with nll.true\n\n# significantly different?\nldiff=nll.true-opt1$value; ldiff\n\n[1] 0.8511813\n\n# no significant difference (which is what we would expect in 95% the generated datasets)\npchisq(2*ldiff,df=2,lower.tail=FALSE)\n\n[1] 0.4269103\n\n\nThe minimum negative log-likelihood (round(opt1$value,2)) is better than the NLL of the model with the true parameters (round(nll.true,2)), but all of these are within the LRT cutoff, i.e. the negative log likelihoods differ by less than 1.92. Remember that the cut-off is based on the Likelihood Ratio Test that states that twice the difference in the log-likelihood between the simpler and more complex model will follow a \\(\\chi^2\\) distribution with n degrees of freedom. \\(n\\) is the number of parameters that are fixed to a specific value. The cut-off value for a \\(\\chi^2\\) with 1 degree of freedom is 3.84. The value of 1.92 is derived from 3.84/2 because we evaluate the difference in log Likelihood and not twice the difference. In other words, we could also multiply all the logLikelihood surface by two and find the 3.84 cutoff."
  },
  {
    "objectID": "Lab6/solution.html#likelihood-surface",
    "href": "Lab6/solution.html#likelihood-surface",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.1 Likelihood surface",
    "text": "6.1 Likelihood surface\nTo find the likelihood surface follow the steps below (background information can be found in Bolker Ch. 6). This exercise continues on exercise #3.1 (Lab 3) where you used the negative binomial to generate 50 numbers and fitted back the parameters.\n\n\n\nFor the likelihood surface:\n\nSet up vectors of \\(\\mu\\) and \\(k\\) values. Let’s try \\(\\mu\\) from 0.4 to 3 in steps of 0.05 and \\(k\\) from 0.01 to 0.7 in steps of 0.01.\nSet up a matrix to hold the results, The matrix for the results will have rows corresponding to \\(\\mu\\) and columns corresponding to \\(k\\):\nRun for loops to calculate and store the values. Use a for nested in another one\nDrawing a contour using the function ‘contour’. Change the argument nlevels to 100 to get a better view of the likelihood surface\nAdd the MLE estimates in the contour plot (use ‘points’). Additionally, add the parameter values that were used to generate the data, and the parameter values that were obtained with the method of moments.\n\n\n\n\n\n\nmuvec = seq(0.4,3,by=0.05)\nkvec = seq(0.01,0.7,by=0.01)\nresmat = matrix(nrow=length(muvec),ncol=length(kvec))\nfor (i in 1:length(muvec)) {\nfor (j in 1:length(kvec)) {\nresmat[i,j] = NLLfun1(c(muvec[i],kvec[j]))\n}\n}\ncontour(muvec,kvec,resmat,xlab=expression(mu),ylab=\"k\")\ncontour(muvec,kvec,resmat,nlevels=100,lty=2,add=TRUE)"
  },
  {
    "objectID": "Lab6/solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "href": "Lab6/solution.html#optimisation-problems-and-assessing-the-confidence-limits-of-parameter-estimates",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.2 Optimisation problems and assessing the confidence limits of parameter estimates",
    "text": "6.2 Optimisation problems and assessing the confidence limits of parameter estimates\nFitting a model to data requires you to specify a relationship between variables. After specifying this relationship we need to fit parameters of this model that best fits the data. This fitting is done through computer algorithms (optimizers). However, sometimes it may be hard to fit a model to data. After having found the best fitting model, you want to assess how certain you are about the parameter estimates. For assessing the uncertainty of model parameters several methods exist that have pros and cons.\nIf you feel comfortable with fitting models to data you are ready for a more challenging exercise. If you do not feel comfortable yet, go back to question 5.2 and practise a bit more.\nThis exercise has two purposes. First you will learn that an innocent looking function can be challenging to fit. Second, you will learn to assess the uncertainty in the parameter values. For assessing the uncertainty in the parameter estimates there are two methods: the profiling method and the quadratic approximation. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the approxation based on the Hessian matrix.\n\nTake the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function \\(a(1-e^{(-bx)}\\). Fit this model with normally distributed errors through this data with mle2 and optim method Nelder-Mead. Choose four different starting points of the optimisation: start_a = c(5,10,20,30), start_b = c(0.001,0.005,0.01,0.1) and compare the NLL of those four optimisations. Plot the curves into the plot with data and try to understand what happened. You can set the \\(\\sigma\\) to 3.\nTo understand the behaviour of the optimisation routine we will plot the likelihood surface over a range of values of \\(a\\) and \\(b\\). For \\(a\\) choose a number of parameter values in the range of 0-40 and for \\(b\\) choose a number of values in the range 0.1-10. Calculate for each combination the NLL and plot the NLL surface using contour plot. For more insight into the functioning of what the optimisation method did, you can add the starting points that you gave to mle2 and the best fitting points, use points() for this. Do you have a clue why the optimisation did not find the minimum point in the landscape? Now zoom in and choose values for \\(b\\) in the range of 0.001-0.03 and check again the NLL surface.\nhint: See Bolker Lab 6 for inspiration on coding.\nhint: You can use a for a double for-loop to run over all parameters\nhint: Store the NLL results in a matrix (you can make a 100x100 matrix by matrix(NA,nrow=100,ncol=100)).\nCalculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile. Use the following pseudocode to achieve this:\n\nAdapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter \\(a\\).\nVary \\(a\\) of a range and optimise the other parameteters.\nPlot the NLL as a function of parameter \\(a\\).\nFind the values of \\(a\\) that enclose \\(-L + \\chi^2(1-\\alpha)/2\\). In R this can be done through qchisq(0.95,1)/2.\nCompare your results with the results from the R function confint(). confint() uses the profiling method along with interpolation methods.\n\n(time permitting) Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:\n\nGet the standard error of the parameter estimates through vcov. Note that vcov return the variance/covariance matrix\nCalculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.\n\n(time permitting) Plot the confidence limits of the both method and compare the results. Is there a big difference between the methods?\nTo assess the uncertainty in the predictions from the model you can construct population prediction intervals (PPIs, see 7.5.3 Bolker). Population prediction intervals shows the interval in which a new observation will likely fall. To construct the PPI take the following steps\n\nSimulate a number of parameter values taken the uncertainty in the parameter estimates into account.\nhint: If the fitted mle object is called mle2.obj, then you can extract the variance-covariance matrix by using vcov(mle2.obj). You can extract the mean parameter estimates by using coef(mle2.obj). Now you are ready to simulate 1000 combinations of parameter values through z = mvrnorm(1000,mu=coef(mle2.obj),Sigma=vcov(mle2.obj)). mvrnorm is a function to randomly draw values from a multivariate normal distribution.\nPredict the mean response based on the simulated parameter values and the values of \\(x\\)\nhint: make a for-loop and predict for each simulated pair of parameter values the mean for a given x. Thus mu = z[i,1]*(1-exp(-z[i,2]*x))\nDraw from a normal distribution with a mean that was predicted in the previous step and the sd that you simulated in step a.\nhint: pred = rnorm(length(mu),mean=mu,sd=z[i,3]). Store pred in a matrix with each simulated dataset in a seperate row.\nCalculate for each value of \\(x\\) the 2.5% and the 97.5% quantiles\nhint: If the predictions are stored in a matrix mat, you can use apply(mat,2,quantile,0.975) to get the upper limit.\n\n\n\n\n\nThe solution is given below in the big chunk of code\n\n\n\nshapes1= read.csv(\"shapes1.csv\")\nplot(shapes1)\n\nnll.mle = function(a,b,sd){\n  # this calculates the mean y for a given value of x: the deterministic function\n  mu = a*(1-exp(-b*shapes1$x))\n  # this calculates the likelihood of the function given the probability\n  # distribution, the data and mu and sd\n  nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T))\n  return(nll)\n}\n\nlibrary(bbmle)\n\n# Try 4 different starting points\nmle2.1 = vector(\"list\", 4)\nstart_a = c(5,10,20,30)\nstart_b = c(0.001,0.005,0.01,0.1)\nfor(i in 1:4) {\n  mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method=\"Nelder-Mead\")\n}\n\n# Check the best fit (in this case it is 3rd starting point)\nfor(i in 1:4) {\n  print(logLik(mle2.1[[i]]))\n}\n\n# Extract the best fit for the rest of the analysis\nbest_mle2.1 = mle2.1[[3]]\nsummary(best_mle2.1)\nlogLik(best_mle2.1)\nconfint(best_mle2.1)\ncoef(best_mle2.1)\n\nplot(shapes1)\ncurve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)\ncurve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)\n\n# likelihood surface\na1 = seq(0,40,length.out = 100)\nb1.1 = seq(0.001,0.03,length.out=100)\nb1.2 = seq(0.1,10,length.out=100)\n\nnll.grid = expand.grid(a1,b1.1)\nnll.grid$NLL = NA\nno = 0\n# Construct first contour\nfor (i in 1:length(a1)){\n  for (j in 1:length(b1.1)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.1[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)\n  }\n}\nlibrary(reshape2)\nz1.1 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])\n\n# Construct second contour\nno = 0\nfor (i in 1:length(a1)){\n  for (j in 1:length(b1.2)){\n    no = no + 1\n    nll.grid[no,1] = a1[i]\n    nll.grid[no,2] = b1.2[j]\n    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)\n  }\n}\nz1.2 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])\n\n# Plot the two contours\npar(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)\ncontour(a1,b1.2,z1.2,nlevels = 20, xaxt = \"n\", yaxt = \"n\", ylim = c(0,9))\naxis(2, seq(1,9,2))\npoints(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)\npoints(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)\npoints(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)\npoints(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)\ncontour(a1,b1.2,z1.2,levels=120,col=2,add=T)\n\npar(mar = c(3.5,4,0.5,1))\ncontour(a1,b1.1,z1.1,nlevels = 20)\npoints(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)\npoints(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)\npoints(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)\npoints(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)\ncontour(a1,b1.1,z1.1,levels=120,col=2,add=T)\n\n\n\n# profile\nnll.mle1 = function(a,sd){\n  # this calculates the mean y for a given value of x: the deterministic function\n  mu = a*(1-exp(-b*x))\n  # this calculates the likelihood of the function given the probability\n  # distribution, the data and mu and sd\n  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))\n  return(nll)\n}\n\nnll = numeric(length(b1.1))\nfor (i in 1:length(b1.1)){\n  b = b1.1[i]\n  mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method=\"Nelder-Mead\")\n  nll[i] = -logLik(mle.21)\n}\npar(mfrow = c(1,1))\nplot(nll~ b1.1,type=\"l\",xlim=c(0.008,0.012), ylim = c(117,125))\nwhich.min(nll)\n\n# cutoff\n-logLik(best_mle2.1) + qchisq(0.95, 1)/2\nwhich(nll < 119.852)\nb1.1[c(23,35)]\n\nplot(nll~ b1.1,type=\"l\",xlim=c(0.0070,0.012),ylim=c(116,125))\nabline(v=c(0.00744,0.01096),lty=2)\nabline(v=0.008968,lty=1,lwd=2)\nabline(v=c(0.00738,0.01103),lty=2,col=\"red\")\n\nse.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])\nb + c(-1,1)*qnorm(0.975) * se.mu\nconfint(best_mle2.1)\n\nabline(v=c(0.007177,0.0107589),col=\"blue\")"
  },
  {
    "objectID": "Lab6/solution.html#bayesian-parameter-estimation-negative-binomial",
    "href": "Lab6/solution.html#bayesian-parameter-estimation-negative-binomial",
    "title": "Lab 6 & 7 Fitting models to data, optimisation and bayesian statistics",
    "section": "6.3 Bayesian parameter estimation: negative binomial",
    "text": "6.3 Bayesian parameter estimation: negative binomial\nIn this section we will practice parameter estimation using the Bayesian method on the same negative binomial example as before (Section 6.1). The purpose of this exercise is to gain intuition of how Markov Chain Monte Carlo (MCMC) algorithms work and better understand the differences (and similarities) between maximum likelihood and Bayesian parameter estimation. The MCMC algorithm implemented below can be useful for relatively simple models such as the ones covered in this course. For more complex data analysis we recommend to use dedicated R packages that implement more powerful (and automated) algorithms. A list of such packages can be found in the task view on Bayesian Inference (https://cran.r-project.org/web/views/Bayesian.html).\n\n6.3.1 From Bayes rule to log posterior\nThe aim of Bayesian analysis is to estimate the parameters of a model conditional on observed data (\\(P(\\theta | D)\\), known as posterior distribution) given the likelihood (\\(L(\\theta|D) = P(D|\\theta)\\)) and prior distributions of the parameters (\\(P(\\theta)\\)), according to Bayes rule:\n\\[\nP(\\theta | D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}\n\\]\nDetails on Bayes rule are given in section 4.3 and 6.2.2 of the book. Note that the only unknown in the right hand side of Bayes rule is \\(P(D)\\). However, we know that \\(P(D) = \\int P(D|\\theta)P(\\theta)d\\theta\\). Therefore, in order to calculate the posterior distribution, we could calculate this integral. Any integration method would work, but integration will not be feasible for a large number of parameters. In practice, a more popular approach is to generate samples from the posterior distribution, while avoiding the integral. This is achieved by so-called Markov Chain Monte Carlo (MCMC) algorithms. These algorithms will provide a random sample from the posterior distribution given a formulation of the problem as:\n\\[\n\\log(P(\\theta | D)) \\propto \\log(P(D|\\theta)) + \\log(P(\\theta)) = \\mathcal{L} + \\log(P(\\theta))\n\\]\nWhere \\(\\mathcal{L}\\) is the positive log-likelihood and \\(\\propto\\) means “proportional to”. These algorithms work with logarithms for the same reason as in maximum likelihood estimation (i.e., to avoid numerical instability due to very large or very small numbers that would result from multiplication).\nThe first step of Bayesian parameter estimation is to build a function that calculates the log-posterior density for every parameter value. We will use the example of the negative binomial from section 6.1. This example fits a negative binomial distribution parameterized by its mean (mu) and size (k) both of which have to be positive.\nIn a Bayesian approach, we need to assign prior probabilities to each of the parameters, which means choosing a distribution and its parameters, based on prior knowledge. Of course, without a context, it is not possible to specify meaningful prior distributions (and this is arguably the hardest step in any Bayesian analysis), but for the sake of this exercise let’s assume that we can represent our prior beliefs with Normal distributions centered around 0 and with a standard deviation of 2 (in practice only half of these prior distributions are being used as mu and k are positive, but that is fine). This essentially means that were are 99% certain that mu and k will be lower than 4.6, prior to seeing any data.\nWe have to construct a function that can return the sum of the log-likelihood and log-prior densities for a given combination of mu and k in order to use MCMC (remember, this is not the exact log-posterior because of the unknown normalizing constant):\n\nLPfun1 = function(p, dat = x) {\n  # Mean and size of the negative binomial (use exp to force them to be positive)\n  mu = exp(p[1])\n  k  = exp(p[2])\n  # Logarithm of the prior distributions on mu and k\n  # (0 and 2 are parameters chosen by the user, they represent prior beliefs)\n  lp_mu = dnorm(mu, 0, 2, log = TRUE)\n  lp_k = dnorm(k, 0, 2, log = TRUE)\n  log_prior = lp_mu + lp_k\n  # Log-likelihood of the data under the model\n  LL = sum(dnbinom(dat,mu=mu,size=k,log=TRUE))\n  # Sum of the log-likelihood and the log-prior\n  LL + log_prior\n}\n\nThe main difference between LPfun1 and NLLfun1 created in section 6.1 is that the new function includes the log-prior densities of mu and k (lp_mu and lp_k, respectively) and that it returns the sum of log-likelihood + log-prior densities.\n\n\n6.3.2 Sampling from posterior: Metropolis-Hastings algorithm\nBelow is a simple version of the Metropolis-Hastings algorithm (section 7.3.1 of the book), with a multivariate Normal proposal distribution (you need to install package mvtnorm first!). Note that this function is written in a generic fashion, that is, it will work with any user-defined function that is assigned to the first argument (model) and any data required by said function is passed through the ... argument (this is the strategy is used in many R functions, including optim).\nThe inputs of the MH function (see below for code) are:\n\nmodel: Function that calculates the non-normalized log-posterior (i.e. LPfun1).\ninit: Initial values for the parameters. The closer to the “true” values the faster the MCMC algorithm will converge to the posterior distribution.\nsigma: Variance-covariance matrix of the proposal distribution used to calculate jumps in parameter space.\nniter: Number of iterations the algorithm will run for.\nburn: Fraction of iterations that will be used as burn-in (check section 7.3.2). These iterations will not be used for analysis but are required for convergence of the MCMC algorithm.\nseed: Seed for pseudo-random number generator that allows reproducing results.\n\nThe algorithm keeps track of all the parameter values it visits and stores them in the variable chain. Each iteration, it proposes new values for each parameter (proposal) sampled from a multivariate Normal distribution centered at the current values. The probability of accepting the proposal is equal to the exponent of the difference in log posterior densities (paccept, see Equation 7.3.2 in the book, taking into account that the proposal distribution is symmetric). If the proposal is accepted, then it is added to the chain and becomes the new current values (i.e., the algorithm “moves” to that location). After the run is finished, the values in chain are split between the burn-in samples and after burn-in. The variable acceptance calculates the fraction of jumps that were accepted (do not confuse with the probability of accepting an individual jump!). The higher this number is, the more efficient the algorithm is in exploring the posterior distribution.\n\nlibrary(mvtnorm)\n\nWarning: package 'mvtnorm' was built under R version 4.3.1\n\nMH = function(model, init, Sigma = diag(init/10), niter = 3e4, burn = 0.5,\n                      seed = 1134, ...) {\n  # To make results reproducible you should set a seed (change among chains!!!)\n  set.seed(seed)\n  # Pre-allocate chain of values\n  chain = matrix(NA, ncol = length(init), nrow = niter)\n  # Chain starts at init\n  current = init\n  lp_current = model(current, ...)\n  # Iterate niter times and update chain\n  for(iter in 1:niter) {\n    # Generate proposal values from multivariate Normal distribution\n    proposal = rmvnorm(1, mean = current, sigma = Sigma)\n    # Calculate probability of acceptance (proposal distribution is symmetric)\n    lp_proposal = model(proposal, ...)\n    paccept = min(1, exp(lp_proposal - lp_current))\n    # Accept the proposal... or not!\n    # If accept, update the current and lp_current values\n    accept = runif(1) < paccept\n    if(accept) {\n      chain[iter,] = proposal\n      lp_current = lp_proposal\n      current = chain[iter,]\n    } else {\n      chain[iter,] = current\n    }\n  }\n  # Calculate the length of burn-in\n  nburn = floor(niter*burn)\n  # Calculate final acceptance probability after burn-in (fraction of proposals accepted)\n  acceptance = 1 - mean(duplicated(chain[-(1:nburn),]))\n  # Package the results\n  list(burnin = chain[1:nburn,], sample = chain[-(1:nburn),],\n       acceptance = acceptance, nburn = nburn)\n}\n\nSo let’s tackle the negative binomial problem with the algorithm above. First, let’s regenerate the data that was used in the previous section:\n\nset.seed(1001)\nmu.true=1\nk.true=0.4\nx = rnbinom(50,mu=mu.true,size=k.true)\n\nNow we can run MH with some values. I want to make the point that choosing a good proposal distribution matters for an efficient MCMC algorithm. So let’s start with a variance-covariance matrix that is not reasonable (because it is too wide):\n\nSigma = diag(c(10,10))\n\nNow we can run MH combined with the LPfun1 function and some initial values:\n\ninit = log(c(1,1))\nbay1 = MH(LPfun1, init, Sigma, burn = 0.3, dat = x)\n\nThe first result you want to check is the acceptance probability to see how succesful proposals were:\n\nbay1$acceptance\n\n[1] 0.01957143\n\n\nThis is terrible! 98% of the proposed values were rejected so it would take really long to get a representative sample from the posterior distribution. The next step is usually to take a look at the traces of the values sampled by the MCMC (noticed that the sampling was done on the log transformation of the parameters as they are positive):\n\npar(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)\nplot(bay1$sample[,1], t = \"l\", ylab = \"Trace of log(mu)\")\nplot(bay1$sample[,2], t = \"l\", ylab = \"Trace of log(k)\")\n\n\n\n\n\n\n\n\nThe low probability of acceptance means that the traces look like “squiggly lines”, getting stuck at different values for hundreds of iterations (i.e., horizontal sections in the traces). This slows down the effective sampling and can introduce biases in the estimates (unless the algorithm runs for very long).\nNote that more modern MCMC algorithms (that R packages specialized on Bayesian statistics will use internally) will automatically tune the proposal distribution or even use alternatives methods to propose values that are more robust. However a poor man’s tuned MCMC may suffice for this introduction (and for simple models with few parameters) and it works as follows:\n\nCalculate the value that maximizes the posterior distribution using optim (a.k.a Maximum A Posteriori estimate or MAP for short).\nEstimate the variance-covariance matrix of the posterior distribution using the Hessian matrix returned by optim (analogous to what we do for maximum likelihood estimation).\nRun MH using the above results as the values for init and sigma, respectively.\n\nThe reason why this works better is because points 1 and 2 will often give a good first approximation of the posterior distribution, especially for large data (in which case the posterior distribution approaches a Normal distribution). This means that MH will be sampling from a distribution similar to the target distribution and hence a higher proportion of proposals will be accepted (intuitively, fewer values that are far in the tails of the posterior distribution will be proposed). This approach can be implemented as:\n\nmapfit = optim(fn = LPfun1, par = log(c(1,1)),\n               hessian = TRUE, method = \"BFGS\",\n               control = list(fnscale = -1), dat = x)\nSigma = solve(-mapfit$hessian)\ninit = mapfit$par\nbay2 = MH(LPfun1, init, Sigma, burn = 0.3)\n\nNotice that we should use control = list(fnscale = -1) because we want to maximize the posterior probability, not minimize it. That is also the reason why I add a negative sign in front of the Hessian as in solve(-mapfit$hessian) (in previous examples we were minimizing the negative log likelihood and we did not include the negative sign in front of the Hessian).\nWe can see that the matrix Sigma obtained from the Hessian around the MAP estimate is different from the one assumed in the first MH run (specifically, the variances are much smaller):\n\nSigma\n\n            [,1]        [,2]\n[1,] 0.078897091 0.002920995\n[2,] 0.002920995 0.122861528\n\n\nThese lower variances mean that the Markov chain does not wonder far into the tails of the posterior distribution but rather remains in the area of high probability. Thus, the new run has a higher acceptance probability:\n\nbay2$acceptance\n\n[1] 0.5599524\n\n\nNow this is the good. 56% of the time the candidates will be accepted, ensuring that the chain samples efficiently from the posterior. The traces will approach white noise (these are often called “fuzzy caterpillars” in the community):\n\npar(mfrow = c(2,1), mar = c(4,4,0.5,0.5), las = 1)\nplot(bay2$sample[,1], t = \"l\", ylab = \"Trace of log(mu)\")\nplot(bay2$sample[,2], t = \"l\", ylab = \"Trace of log(k)\")\n\n\n\n\n\n\n\n\nAt this point we would normally calculate more diagnostics to build up more confidence on the results of the MCMC chains, but we will keep it simple in this introduction. The object bay2$sample contains a random sample from the posterior from which we can calculate several properties. First, remember that we took the logarithm of the parameters to avoid negative values, so we need to undo this transformation in the result:\n\nbay2sample = exp(bay2$sample)\n\nWe can visualize the estimates for each parameter using density (more common) or hist (easier to interpret):\n\npar(mfrow = c(1,2), mar = c(4,4,1.5,1))\nhist(bay2sample[,1], main = \"Density of mu\", freq = F, xlim = c(0,4))\nhist(bay2sample[,2], main = \"Density of k\", freq = F, xlim = c(0,1))\n\n\n\n\nOne striking feature is that the distributions are not symmetric, they have a longer tail to the right. This is typical of positive parameters that are close to 0. A consequence of this is that the mean, median and mode of the distributions will differ (though in in this case not so much). Let’s compare all the estimates we have so far for the negative binomial fitted to these data:\n\nmap = exp(mapfit$par)\nmeanp = colMeans(bay2sample)\nmedianp = c(median(bay2sample[,1]), median(bay2sample[,2]))\ncbind(map, meanp, medianp,\n      mom = c(mu.mom, k.mom),\n      mle = opt1$par,\n      true = c(mu.true, k.true))\n\n         map     meanp  medianp       mom       mle true\nmu 1.2209093 1.2897290 1.222248 1.2600000 1.2602356  1.0\nk  0.2876172 0.2926038 0.274538 0.3778531 0.2884793  0.4\n\n\nFor this model, data and priors, all estimates are quite similar to each other across different methods of estimation. The reason is because there is sufficient data (50 points for 2 parameters is quite some data…) such that the priors have a negligible effect.\nThe 95% credible intervals (analogous to 95% confidence intervals) can be calculated with the quantile function applied directly to the sample from the posterior:\n\nt(apply(bay2sample, 2, quantile, probs = c(0.025, 0.975)))\n\n          2.5%     97.5%\n[1,] 0.7078096 2.2138543\n[2,] 0.1348619 0.5519341"
  },
  {
    "objectID": "Lab9/no_solution.html",
    "href": "Lab9/no_solution.html",
    "title": "Lab 9 + GLS and Mixed effect Models",
    "section": "",
    "text": "You will learn how to:\n\nEstimate the parameters of a model with the variance varying as a function of a covariate (gls)\nApply mixed effect models to data with nested structures"
  },
  {
    "objectID": "Lab9/solution.html",
    "href": "Lab9/solution.html",
    "title": "Lab 9 + GLS and Mixed effect Models",
    "section": "",
    "text": "You will learn how to:\n\nEstimate the parameters of a model with the variance varying as a function of a covariate (gls)\nApply mixed effect models to data with nested structures"
  }
]
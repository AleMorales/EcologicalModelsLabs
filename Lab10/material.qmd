# Learning goals

In this practical you will learn

- The concept of maximum marginal likelihood

- Estimate models with process and measurement errors

- Estimate multilevel non linear models with Gaussian quadrature

- The Bayesian approach for the two models above

- Specify informative priors using general knowledge

# Introduction

In this practical we will cover solve a couple of examples to illustrate
how to solve multilevel models are addressed.

- The first example illustrates situations where it is important to separate
process error (aka stochasticity) and  measurement error, which leads to two
nested stochastic models.

- The second example illustrate situations where multiple individuals are measured
and parameters (*traits*) vary across individuals (representing true ecological
variation, not error), which also leads to two (or more) nested stochastic models.

These are more advanced models compared to the rest of the course and the book
only introduces them as teasers, you are not meant to master them. Therefore,
these practicals are really more tutorials with small exercises in
each section for you to practice and mastering these type of models would
require additional learning beyond this course.

Here is how the rest of the practical is organized:

- First I show how to solve these two problems using maximum marginal likelihood,
explicitly going through every step and introducing some simplifications.

- In the second part I show how to solve these problems using a Bayesian approach. Remember
that this is optional.

Just like in the previous chapters, there are *canned* procedures that can tackle
subsets of the problems you may face and these procedures are much easier to use
in practice. Check the relevant supplement.

# Maximum marginal likelihood approach

The maximum likelihood approach to estimate parameters in multilevel models is
based on the concept of Expectation-Maximization. The general idea is as follows:

1. Organize the random variables in a multilevel hierarchy (the output of one
level is the input to the next). We sort the levels from 1 (bottom) to $n_L$ (top).

2. Calculate a *marginal* likelihood that integrates the full model over all
levels random variables that are not observed. This result in a likelihood with
only one stochastic model in it.

3. Calculate the parameters of the model by maximizing the marginal likelihood
from step 2.

4. Calculate the *conditional* likelihood for the level $N_L - 1$ by marginalizing
over all the level below and fixing parameters to the values estimated in step 3.
Maximizing the conditional likelihood will give you an estimate to the values of
the latent random variable at level $N_L - 1$.

5. If your model has more than two levels, keep repeating step 4 going one level
at a time (walking down the hierarchy).

The examples below will only deal with two levels so step 5 will not be relevant.
Please go through the examples below and come back to the steps above to better
understand the procedure.

## Model with measurement error

Let's assume that the growth rates of trees within a forest follow a Gamma
distribution and the measurements of this growth rate contain errors that follow
a Normal distribution. This is the problem described in section 10.5.2 of the
book. This is a model with two levels:

$$
\begin{align*}
\text{Level}~2~:~X_{true} &\sim Gamma(a, s) \\
\text{Level}~1~:~X_{obs~~}  &\sim Normal(X_{true}, \sigma)
\end{align*}
$$

Where $X_{true}$ are the true growth rates of the trees and $X_{obs}$ are the
measured growth rates. Notice that the level 1 uses as input the output of level
2 ($X_{true}$), which determines the order.

In this exercise we will work with synthetic data generated by stochastic
simulation (see Chapter 5 of the book):

```r
set.seed(1001)
x_true <- rgamma(1000, shape = 3, scale = 10) # True growth rates
x_obs  <- rnorm(1000, mean = x_true, sd = 10) # Observed growth rates with error
hist(x_obs, ylim = c(0,300))
hist(x_true, add = TRUE, col = rgb(1,0,0,0.3))
```

According to the general procedure described below, we can estimate $a$ and $s$
from level 2 by creating a marginal likelihood (integrating over level 1) and
maximizing it (steps 2 - 3). This is shown in the next section. Once that
estimation is done, we can estimate each of value of $X_{true}$ by maximizing the
conditional likelihood (step 4).

### Estimation of parameters

The marginal likelihood is defined as:

$$
L_m \left(X_{obs,i} | a, s , \sigma \right) = \prod_{i=1}^{n} \int{\text{Gamma}\left(X_{true,i} | a, s\right) \text{Normal}\left(X_{obs,i} | X_{true,i} , \sigma \right) dX_{true,i}
$$

That is, for each observation we integrate over all the possible true values (that
is the *latent* random variable we do not observe) to obtained a likelihood
function that only contains observations and parameters. Because the measurement
error is normal, we can reparameterized as:

$$
L_m \left(X_{obs,i} | a, s , \sigma \right) = \prod_{i=1}^{n} \int{\text{Gamma}\left(X_{obs,i} - \epsilon_i | a, s\right) \text{Normal}\left(\epsilon_i |0 , \sigma \right) d\epsilon_i}
$$

Where we replace $X_{true} = X_{obs} - \epsilon$. This form is how most multilevel
models are expressed. In R the product of the two distributions is:

```r
prodfun <- function(eps, a, s, sigma, x) {
 dgamma(x - eps, shape = a, scale = s)*dnorm(eps, mean = 0, sd =  sigma)
}
prodfun(eps = 1, a = 3, s = 10, sigma = 1, x = x_obs[1])
```

We can now integrate `prodfun` using the function `integrate` (note: this only
works for one latent random variable, more advanced methods are needed when more
latent variables are used):

```r
integrate(f = prodfun, lower = -Inf, upper = Inf,
          a = 3, s = 10, sigma = 1, x = x_obs[1],
          rel.tol = 1e-6, abs.tol = 1e-6)$value
```
We can now build a function to compute the negative log marginal likelihood by
applying this integral to each observations:

```r
NLML <- function(x, a, s, sigma) {
  # Marginal likelihood of each observation
  ML <- sapply(x, function(x) integrate(f = prodfun, lower = -Inf, upper = Inf,
                                        rel.tol = 1e-6, abs.tol = 1e-6,
               a = a, s = s, sigma = sigma, x = x)$value)
  # Negative log marginal likelihood
  NLML <- -sum(log(ML))
  NLML
}
NLML(x_obs, a = 3, s = 10, sigma = 1)
```
Notice that this takes a bit longer than usual because we are doing 1000 numerical
integrations. We can minimize the function `NLML` with `mle2`:

```r
library(bbmle)
fit <- mle2(minuslogl = NLML,
            start = list(a = 3, s = 10, sigma = 1),
            lower = c(0,0,0),
            data = list(x = x_obs), method = "L-BFGS-B")
fit
```

We can now compare the true and estimated models for growth and error to see how
well the estimation worked:

```r
pars = coef(fit)
plot(density(x_obs), ylim = c(0,0.045), xlab = "Growth rate",
     ylab = "Probability density", main = "", las = 1, col = 3)
curve(dgamma(x, shape = 3, scale = 10), add = TRUE)
curve(dgamma(x, shape = pars["a"], scale = pars["s"]), add = TRUE, col = 2)
curve(dnorm(x, mean = 0, sd = 10), add = TRUE, lty = 2)
curve(dnorm(x, mean = 0, sd = pars["sigma"]), add = TRUE, col = 2, lty = 2)
legend("topright", c("Obs", "True growth", "Est growth", "True error", "Est error"),
       col = c(3, 1, 2, 1, 2), lty = c(1, 1, 1, 2, 2))
```

We can quickly estimate the confidence intervals using the quadratic approximation:

```r
confint(fit, method = "quad")
```

Notice that the true values (`a = 3`, `s = 10` and `sigma = 10`) are within the
confidence intervals, so the estimation has worked. We could also try
building the likelihood profiles but that would take quite a while
since this would require many more optimizations so we will skip this part.

:::::: {.callout-important title="Exercise" collapse="true"}

**Add an exercise with measurement errors**

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

**Add an exercise with measurement errors**

::::

:::::

::::::


## Model for nested data

To keep it within the same theme, in this example
we will look at a simple dataset
of tree growth, but this time using real data. The data describes the growth in
height of several individuals of Loblolly pine (*Pinus taeda*). We can load the
data as follows:

```r
library(ggplot2)
data(Loblolly)
summary(Loblolly)
# eyeballing; hm = 60, b = 0.25, c = 12 (check  Figure 3.9 in the book)
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) + geom_point() +
  stat_function(fun = function(x) 60/(1 + exp(-0.25*(x - 12))), color = "black")
```

Each tree is identified by the column `Seed` and six measurements of height over
the age of the tree are reported. The growth curve of each tree will be modeled
using a logistic curve:

$$
h(t) =  \frac{h_m}{1 + e^{-b\left(t - c \right)}
$$

where $h$ is the height of the tree, $h_m$ is the maximum height, $b$ control
the steepness of the curve and $c$ is the age at which the half maximum height
is reached (see Chapter 3 of  the book, I will refer to these parameters as
traits from here on) and $t$ is
the age of the tree. We want to know the average values for
the three traits as well as how much they vary across
individuals (variation in traits across individuals is often very relevant in
ecology). In a mixed model, $h_m$, $b$ and $c$ are assumed to vary across individuals
assuming particular distributions (we will assume normal distributions which is
the standard in mixed models).

As a first approach, we can try to estimate the curve for each tree independently.
This is a good way to
get a first estimate of how much the traits vary across individuals as well as
the observation error. As we will see in the examples below, non-linear mixed models
as simple as these ones already need quite some constraints in order to work.

### Stepwise procedure

The stepwise procedure is as follows:

1. Fit the model to each individual separately, using maximum likelihood.

2. Treat the maximum likelihood estimates from each individuals as if they were
observations and analyze them with a separate model.

Let's setup a function to fit the logistic growth curve to each tree:

```r
logistic <- function(b, c, h_m, t) {
  h_m/(1 + exp(-b*(t - c)))
}
mle_fun <- function(b, c, h_m, sigma, t, h) {
  hmod = logistic(b, c, h_m, t)
  -sum(dnorm(h, hmod, sigma, log = TRUE))
}
```

We can test the function for the first tree:

```r
seeds = unique(Loblolly$Seed)
tree1 = subset(Loblolly, Seed == seeds[1])
mle_fun(c = 12, b = 0.25, h_m = 60, sigma = 1, t = tree1$age, h = tree1$height)
```

Let's estimate the parameters for this first tree:

```r
library(bbmle)
fit1 = mle2(mle_fun, start = list(c = 12, b = 0.25, h_m = 60, sigma = 1),
     data = list(t = tree1$age, h = tree1$height))
fit1
```

Let's repeat it for all trees:

```r
traits = matrix(NA, ncol = 4, nrow = length(seeds))
colnames(traits) = c("b", "c", "h_m", "sigma")
for(i in 1:length(seeds)) {
  tree = subset(Loblolly, Seed == seeds[i])
  fit = mle2(mle_fun, start = list(c= 12, b = 0.25, h_m = 60, sigma = 1),
             lower = c(c = 0, b = 1e-4, h_m = 10, sigma = 1e-4),
             upper = c(c = 30, b = 1, h_m = 80, sigma = 20),
            data = list(t = tree$age, h = tree$height), method = "L-BFGS-B")
  traits[i,] = coef(fit)
}
traits
```

Let's look at the individual fits by adding the predicitons to the data:

```r
Loblolly = transform(Loblolly,
                     hm1  = rep(traits[,"h_m"], each = 6),
                     c1  = rep(traits[,"c"], each = 6),
                     b1  = rep(traits[,"b"], each = 6))
# Prediction from stepwise model
Loblolly = transform(Loblolly, pred_height1 = logistic(b1, c1, hm1, age))
# Plot the data and predictions for each indicidual
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height1))
```

We can now look at the (co-)variation of the traits:

```r
library(GGally)
ggpairs(data = traits[,1:3])
```

Notice that b and c are correlated. The standard averages and standard
deviations can be used as initial estimates for the mixed model later:

```r
means = colMeans(traits)
sds   = apply(traits, 2, sd)
cbind(means, sds)
```

In the next sections (and to keep it simple) we will fit a multilevel model where
only $h_m$ is allow to vary across trees.

### Estimating at population level

To obtain the mean estimates of $a$, $b$ and $h_m$ we need to construct a
marginal likelihood to integrate over the distribution of values across
individuals. If we only assume a random effect for $h_m$ we can specify the
model as follows:

$$
\begin{align*}
h_{ij} &\sim \text{Normal} \left(\frac{h_{mi}{1 + e^{-b\left(t_j - c \right)}, \sigma \right) \\
h_{mi}  &\sim \text{Normal}(\mu_{hm}, \sigma_{hm})
\end{align*}
$$

where $h_{ij}$ is the height of tree $i$ at age $j$, and $mu_hm$
is the population mean for $h_m$, $\sigma$ represents the observation error and
$\sigma_{hm}$ represents the variation of $h_{mi}$ across individuals. In some of
the literature, the distribution of $h_{ij}$ is referred to as "individual level"
and the distribution of $h_{mi}$ is known as "population level" (and this type
of models are known as *multilevel* or *hierarchical* models).

We can  decompose the values of $h_m$ for each individual into the average and
the  deviation with respect to the average ($\epsilon$). In some of the literature
(where these models are know as *mixed effect* models) the values of $\epsilon$
are know as random effects:

$$
\begin{align*}
h_{ij} &\sim \text{Normal} \left(\frac{h_{mi}{1 + e^{-b\left(t_j - c \right)}, \sigma \right) \\
h_{mi} &= \mu_{hm} - \epsilon_{hmi} \\
\epsilon_{hmi}  &\sim \text{Normal}(0, \sigma_{hm}) \\
\end{align*}
$$

To create the marginal likelihood we now need to integrate over the Normal
distributions of $h_m$. Let's first build the function. We do it a bit different
from before, because we have multiple observations for one tree (before we only
had one). This also means we need to be careful with implementation: (i) the
function `integrate` will pass a vector of `eps` values to `prodfun` and we have
multiple values of `t` and `h`. Therefore, we must use sapply:

```r
prodfun <- function(eps_hm, b, c, mu_hm, sigma, sigma_hm, t, h) {
 sapply(eps_hm, function(x) exp(dnorm(x, mean = 0, sd =  sigma_hm, log = TRUE) + # Population level
     sum(dnorm(h, logistic(b, c, mu_hm - x, t), sigma, log = TRUE)))) # Individual level
}
# Evaluate for first tree using as initial values what we obtain from the stepwise approach
prodfun(eps_hm = 0, c = 11.8, b = 0.23, mu_hm = 61,  sigma_hm = 2.3, sigma = 2,
        t = Loblolly$age[1:6], h = Loblolly$height[1:6])
```

The integration for one tree would be:

```r
integrate(f = prodfun, lower = -6*2.3, upper = 6*2.3,
          c = 11.8, b = 0.23, mu_hm = 61, sigma_hm = 2.3, sigma = 2,
          t = Loblolly$age[1:6], h = Loblolly$height[1:6],
          rel.tol = 1e-12, abs.tol = 1e-12)$value
```

We can now define a function to compute the negative log marginal likelihood by
solving the integral for each observation, log transforming and adding them up.
Notice that now we are not applying the integration to each observation but to
each group of observations that belongs to a tree:

```r
NLML <- function(b, c, mu_hm, sigma_hm, sigma, t, h) { # data
  # Every 6 observations is a tree
  id = seq(1, length(t), by = 6)
  ML = sapply(id, function(id)
                   integrate(f = prodfun,lower = -Inf, upper = Inf,
                        c = c, b = b, mu_hm = mu_hm,
                        sigma = sigma, sigma_hm = sigma_hm,
                        t = t[id:(id + 5)], h = h[id:(id + 5)],
                   rel.tol = 1e-12, abs.tol = 1e-12)$value)
  NLML <- -sum(log(ML))
  NLML
}
NLML(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2,
     t = Loblolly$age, h = Loblolly$height)
```

And now we can pass this big boy to bbmle. As usual, I used constrained optimization
to avoid negative values or getting too close to zero (and I check later if I
hit the boundary):

```r
par_0 = c(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2)

fit <- mle2(minuslogl = NLML, start = as.list(par_0),
            data = list(t = Loblolly$age, h = Loblolly$height),
            method = "L-BFGS-B", lower = c(b = 0.01, c = 1, mu_hm = 10,
                                           # error in lower sigma_hm = 0.01, sigma = 0.01),
            control = list(parscale = abs(par_0)))
summary(fit)
```

As usual we can extract the maximum likelihood estimates and the confidence
intervals:

```r
pars = coef(fit)
ci = confint(fit, method = "quad")
print(pars)
print(ci)
```

Notice that the estimates are close to what we estimated before with the
stepwise approach but not exactly the same

```r
cbind(means, sds)
```

### Estimating at individual level

The values of traits for each individual trait can be estimated in the same way
that we estimated the true growth rate of trees, by maximizing the conditional
likelihood for each tree separately. Let's build the negative log conditional
likelihood of the data of one tree conditional on knowing the population averages
and variances:

```r
NCLL <- function(eps_hm, b, c, mu_hm, sigma_hm, sigma, t, h) {
   -dnorm(eps_hm, mean = 0, sd =  sigma_hm, log = T) - # Population level
     sum(dnorm(h, logistic(b, c, mu_hm - eps_hm, t), sigma, log = T)) # Individual level
}
```

Let's the plot NCLL for the first tree

```r
tree1 = subset(Loblolly, Seed == seeds[1])
t = tree1$age
h = tree1$height
eps <- seq(-3,3, by = 0.01)*pars["sigma"]
NCLL1 <- sapply(eps, function(e)
                  NCLL(e, c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = t, h = h))
plot(eps, NCLL1)
```

We can then optimize this function to obtain the estimated deviation between
the maximum height of the first tree and the average of the population and
compare with the value estimated from the stepwise procedure:

```r
eps_1 = optimize(NCLL, c(-3,3)*pars["sigma_hm"],c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = tree1$age, h = tree1$height)$minimum
hm1 = pars["mu_hm"] - eps_1
cat("Multivelel: ", hm1, "Stepwise: ", traits[1,"h_m"])
```

Let's do the estimation for all the trees:

```r
eps = numeric(length(seeds))
for(i in 1:length(seeds)) {
  treei = subset(Loblolly, Seed == seeds[i])
  t = treei$age
  h = treei$height
  eps[i] = optimize(NCLL, c(-3,3)*pars["sigma_hm"], c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = t, h = h)$minimum
}
hm = pars["mu_hm"] - eps
plot(traits[,"h_m"], hm, ylim = c(52,66), xlim = c(52,66))
abline(a = 0, b = 1)
abline(lm(hm~I(traits[,"h_m"])), lty = 2)
```

We can see that the individual estimates of `hm` from the multilevel model and
the stepwise approach as correlated (on average they are practically the same)
but the multilevel model estimates higher values for the smaller trees and lower
values for the higher trees. That is, the estimates for individual trees are
*pulled* towards the population mean. This is a common effect of using multilevel
models known as *shrinkage*. As long as your model for the variation of `hm`
across individuals is reasonable, the estimates with shrinkages are actually
better than in the stepwise approach (in the sense that they will be closer to
the truth on average).


Let't calculate the predictions for each tree

```r
Loblolly = transform(Loblolly,
                     hm2 = rep(hm, each = 6))
Loblolly = transform(Loblolly, pred_height2 = logistic(pars["b"], pars["c"], hm2, age))

# Compare individual predictions for stepwise and multilevel model
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height2))
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height1))
```

We can visually see that the multilevel model makes predictions that vary less
across trees. Part of it is because of the shrinkage effect, but also because
we ignored the variation across trees of `a` and `b`. We could write the code
to evaluate those two too, but then we have to use for advanced methods of
integration and it gets very tedious.

:::::: {.callout-important title="Exercise" collapse="true"}

**Add an exercise with non-linear mixed model**

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

**Add an exercise with non-linear mixed model**

::::

:::::

::::::

# (OPTIONAL) Bayesian approach

In the Bayesian approach we can estimate all the parameters and latent random
variables by applying Markov Chain Monte Carlo. In fact, no changes are needed
to the estimation procedure at all, and a single pass of MCMC gives use all the
information we need. If you use a probabilistic language (like Stan,
JAGS or NIMBLE) it is also straightforward to add levels to the model. Given
reasonable priors, it can be much easier to estimate non-linear multilevel
models in a Bayesian way than the procedure describe in the first half. In fact,
fitting multilevel models (and other models with latent random variables) is the
most common reason why ecologist switch to Bayesian methods.

## Model with measurement error

We can estimate this model using Stan as shown in previous tutorials. Remember
that the stan model needs to be defined in its own file with the extension `.stan`
or add your code in a Quarto document with the label `stan`. If you are getting
lost in the code, please check the supplement on Stan (also compare with the
BUGS code in the book).

```stan
data {
  int N; # Number of observations/true values
  vector[N] x_obs; # The observed growth rate
  vector[6] hp; # Hyperparameters of the prior distributiobs
}

parameters {
  vector<lower = 0>[N] x_true; # Estimated true growth rates
  real<lower = 0> a; # Shape parameter of Gamma
  real<lower = 0> s; # Scale parameter of Gamma
  real<lower = 0> sigma; # Scale parameter of Normal
}

model {
  # Priors (different from Bolker)
  a ~ normal(hp[1], hp[2]); # Shape (95% <  26), 10, 10
  s ~ normal(hp[3], hp[4]); # rate (95% < 40), 15, 15
  sigma ~ normal(hp[5], hp[6]); # sigma (95% < 40), 10, 10
  # True growth rate
  x_true ~ gamma(a, 1/s); # gamma(shape, rate = 1/scale)
  # Observed growth rate with error
  x_obs ~ normal(x_true, sigma);
}
```


I made the following changes to the code with respect to Bolker:

- All parameters are given (truncated) Normal distributions as priors because
it is easier to reason about prior means and standard deviations.

- I do not hard-code the hyperparameters, so that you can rerun sampling with
different priors (for prior sensitivity analysis).

- The normal distribution is parameterized with the standard deviation rather
than precision.

- There is no need to specify initial values for chains since the priors are
not excessively wide (so random samples from them are good starting values).

In all cases we are using Normal prior distribution that are *weakly informative*
meaning that we only incorporate knowledge of the scale of the parameters. They
will be automatically truncated as parameters are all positive. Note that the
Bayesian approach will estimate a true value for each observation in addition to
the three parameters of the model (so technically we will get 1003 estimates!):
The details below are as usual (see Stan supplement). Notice that I increase
`adapt_delta` to `0.95` because this was a harder fit and I want a total of 40
thousand samples:

```r
library(rstan)
ncores = parallel::detectCores()
nchains = min(8, ncores)
niter = 1000 + round(40e3/nchains, 0)
options(mc.cores = ncores)
bayesian_fit <- sampling(error_growth_model,
                         cores = nchains, chains = nchains,
                         iter = niter, warmup = 1000,
                         data = list(N = length(x_obs), x_obs = x_obs,
                                     hp = c(10, 10, 15, 15, 10, 10)),
                         control = list(adapt_delta = 0.95))
```

This ran in about a minute on my computer. Let's look at the summaries for the
population parameters (`a`, `s` and `sigma`):

```r
print(bayesian_fit, pars=c("a", "s", "sigma"), probs=c(.025,.5,.975))
```

We are getting perfect Rhat values (Gelman-Rubin diagnostics) and the effective
sample size is in the thousands, which means that our estimations of the posterior
distributions from these samples would be very accurate (in fact a bit overkill
if you use my originally settings).

Both the median and mean are quite similar suggesting a posterior
distribution that is fairly symmetric (in fact with this sample size it should be
fairly normal and the
priors should have a very small effect). We can compare these estimates to the
maximum likelihood estimates from before:

```r
pars
exp(confint(fit, method = "quad"))
```

We are getting very similar values to the maximum likelihood approach, as
expected from the large sample sizes (so again, the priors did not have much effect).
If you check table 10.1 in the book (section 10.5.2) the intervals computed from
the posterior distribution are very close to the confidence intervals from the
likelihood profile.

The Bayesian procedure also gives us the estimates `x_true` directly. We can
convert the samples from the posterior into a matrix:

```r
posterior <- as.matrix(bayesian_fit);
dim(posterior)
posterior[1:4,1:4]
```

We can compare the different estimates of `x` for the first observation using a
kernel density plot:

```r
plot(density(posterior[,1]), xlab = "Growth rate", main = "", las = 1)
abline(v = c(x_obs[1], x_true[1], x_obs[1] - est_eps[1]), col = 1:3)
abline(v = quantile(posterior[,1], prob = c(0.025,0.975)), lty = 2, col = 4)
legend("topright", c("Obs", "True", "Max Lik", "CI 95%"), col = c(1:3,4,4),
       bty= "n", lty = c(1,1,1,2,2))
```

We can see that the Bayesian estimate of the true growth rate for the first
observation has a maximum around the same value as the point estimate we obtained
using maximum likelihood (and therefore in between observed and true growth
rates). However, we actually have quite a bit of uncertainty in this estimate as
reflected by the 95% credible interval.

Of course, given that we know the true values of x in this simulation, we can
test how often these 95% intervals include the true values (for a perfect
estimation we should cover the true value 95% of the times). Let's compute the
lower and upper bounds of the intervals for each observation:

```r
lower_ci = quantile(posterior[,1:1000], 2, prob = 0.025)
upper_ci = quantile(posterior[,1:1000], 2, prob = 0.975)
inside   = (x_true >= lower_ci) & (x_true <= upper_ci)
coverage = sum(inside)/1e3
coverage
```

That is pretty much spot on! Getting exactly 95% is very difficult (especially
with only 1000 values, we would need more for the coverage estimate to
stabilize). The fact that it errors on the side of caution (i.e., simulated coverage
is slightly higher than 95%) is also a good thing (we generally prefer to be
pessimistic rather than optimistic). So even though our uncertainty about the true
growth rate of individual trees remains high given how big the measurement error
is, achieving a practically perfect coverage is the best we can do.

:::::: {.callout-important title="Exercise" collapse="true"}

**Add Bayesian version of mixed model exercise**

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

**Add Bayesian version of mixed model exercise**

::::

:::::

::::::


## Model for nested data

Let's implement the model in Stan. This time we are going to include all traits
as varying across replicates but we will ignore correlations as that makes the
model much more complex (but it would be possible). The predictions of heights
is as before:

$$
h_{ij} \sim \text{Normal} \left(\frac{h_{mi}{1 + e^{-b_i\left(t_j - c_i \right)}, \sigma \right)
$$
Where the suffix $i$ refers to the tree and $j$ to the time point. The variation
in heights across the population is assumed to follow a normal distribution (note
that this can technically produce negative values but we will ignore this for
simplicity):

$$
\begin{align*}
h_{mi} &= \mu_{hm} + \sigma_{hm} z_{hmi} \\
z_{hmi}  &\sim \text{Normal}(0, 1) \\
\end{align*}
$$

Notice that we have further decomposed the Normal distribution based on the fact
that $\text{Normal}(\mu, \sigma) = \mu + \sigma \text{Normal}(0,1)$ which tends
to make MCMC sampling much faster. We repeat the same decomposition for the rest
of the trees:

$$
\begin{align*}
b_{i} &= \mu_{b} + \sigma_{b} z_{bi} \\
z_{bi}  &\sim \text{Normal}(0, 1) \\
c_{i} &= \mu_{c} + \sigma_{c} z_{ci} \\
z_{ci}  &\sim \text{Normal}(0, 1) \\
\end{align*}
$$

The model in Stan would look at follows

```stan
data {
  int N; # Number of observations
  int Ntree; # Number of trees
  vector[N] h; # The observed growth rate
  vector[N] t; # The age associated to each observation
  int tree[N]; # Id of the tree (not seed, but 1 - 14)
  vector[14] hp; # Hyperparameters
}

parameters {
  # Population-level parameters
  real<lower = 0> mu_hm; # Mean maximum height
  real<lower = 0> mu_b;  # Mean parameter b
  real<lower = 0> mu_c;  # Mean parameter c
  real<lower = 0> sigma_hm; # Variation in maximum height
  real<lower = 0> sigma_b;  # Variation in parameter b
  real<lower = 0> sigma_c;  # Variation in parameter c
  real<lower = 0> sigma;    # Measurement/observation error
  vector[Ntree] z_hm; # Standardized deviations of maximum height in population
  vector[Ntree] z_b;  # Standardized deviations of b in population
  vector[Ntree] z_c;  # Standardized deviations of c in population
}

# In this block we can do intermediate calculations (makes it run faster)
transformed parameters{
  # Individual-level parameters
  vector[Ntree] hm; # Maximum height of each tree
  vector[Ntree] b; # Trait b of each tree
  vector[Ntree] c; # Trait c of each tree
  # Observations
  vector[N] h_mod;
  # Compute the trait value of each tree
  hm = mu_hm + sigma_hm*z_hm;
  b  = mu_b  + sigma_b*z_b;
  c  = mu_c  + sigma_c*z_c;
  # Compute the height of each tree at each timepoint
  for(i in 1:N) {
    int id = tree[i]; # Check which tree we are dealing with
    h_mod[i] = hm[id]/(1 + exp(-b[id]*(t[i] - c[id]))); # Logistic model
  }
}

# Here we only list the distributions since we already have done the calculations
# of growth in the transformed parameters block
model {
  // Population-level parameters (priors)
  mu_hm ~ normal(hp[1], hp[2]);
  mu_b  ~ normal(hp[3], hp[4]);
  mu_c  ~ normal(hp[5], hp[6]);
  sigma_hm ~ normal(hp[7], hp[8]);
  sigma_b  ~ normal(hp[9], hp[10]);
  sigma_c  ~ normal(hp[11], hp[12]);
  sigma ~ normal(hp[13], hp[14]);
  // Standardized deviations from population mean (actual tree-level traits above)
  z_hm ~ normal(0, 1);
  z_b ~ normal(0, 1);
  z_c ~ normal(0, 1);
  // Observations within tree
  h ~ normal(h_mod, sigma);
}
```

For this example, I came up with more informative priors based on general I could
find online on the growth of Loblolly pine. I put the whole reasoning to come up
with the priors at the end of the lab as an example of how one could come up with
reasonable priors and check that they produce sensible predictions. So please go
there if you want to check it.

Once we have chosen some prior distributions we can sample from the posterior
distribution:

```r
library(rstan)

# Parallelize
ncores = parallel::detectCores()
nchains = min(8, ncores)
niter = 1000 + round(40e3/nchains, 0)
options(mc.cores = ncores)

# See section below on prior elicitation
priors = c(65, 10,          # mean and sd of mu_hm
           0.25, 0.25/4,    # mean and sd of mu_b
           10, 10/4,        # mean and sd of mu_c
           65/5, 65/10,     # mean and sd of sigma_hm
           0.25/5, 0.25/10, # mean and sd of sigma_b
           10/5, 10/10,     # mean and sd of sigma_c
           1, 1)            # mean and sd of sigma

bayesian_fit <- sampling(logistic_growth_model,
                         cores = nchains, chains = nchains,
                         iter = niter, warmup = 1000,
                         data = list(N = nrow(Loblolly),
                                     Ntree = length(unique(Loblolly$Seed)),
                                     h = Loblolly$height,
                                     t = Loblolly$age,
                                     tree = as.numeric(Loblolly$Seed),
                                     hp = priors),
                         control = list(adapt_delta = 0.95))

print(bayesian_fit, pars=c("mu_hm", "mu_b", "mu_c",
                           "sigma_hm", "sigma_b", "sigma_c", "sigma"),
      probs=c(.025,.5,.975))
```

Compare this to the estimates we obtained from the stepwise approach:

```r
means = colMeans(traits)
sds   = apply(traits, 2, sd)
cbind(means, sds)
```

The Bayesian estimate of the standard deviations are a bit smaller than the ones
obtained in the stepwise approach, but they are in the right order of magnitude
and the 95% credible interval includes the estimates from the stepwise approach.
Let's look at the posterior distributions of this standard deviations:

```r
posterior = as.matrix(bayesian_fit)
par(mfrow = c(1,3))
plot(density(posterior[,"sigma_hm"], from = 0), main = "", xlab = "sigma_hm")
plot(density(posterior[,"sigma_b"], from = 0), main = "", xlab = "sigma_b")
plot(density(posterior[,"sigma_c"], from = 0), main = "", xlab = "sigma_c")
```

Note how the posterior distributions of standard deviations of b and c are quite
small but also highly asymmetric. This is typical of constrained variables that
are highly uncertain. Methods based on maximum marginal likelihood will struggle
estimating optimal values `sigma_b` and `sigma_c`.

The samples for `hm`, `b` and `c` of each can be retrieved directly from
posterior. For  example, for the first tree:

```r
par(mfrow = c(1,3))
plot(density(posterior[,"hm[1]"]), main = "", xlab = "hm")
plot(density(posterior[,"b[1]"]), main = "",  xlab = "b")
plot(density(posterior[,"c[1]"]), main = "",  xlab = "c")
```

We can also print the summary information directly from othe original fitted object:

```r
print(bayesian_fit, pars=c("hm"), probs=c(.025,.5,.975))
```

Similarly, we can also extract the predicted values for each observation of
height (I only show it for one observation, otherwise it becomes too long):

```r
print(bayesian_fit, pars=c("h_mod[1]"), probs=c(.025,.5,.975))
```

:::::: {.callout-important title="Exercise" collapse="true"}

**Add Bayesian version of mixed model exercise**

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

**Add Bayesian version of mixed model exercise**

::::

:::::

::::::


## Informative prior elicitation

Let's define some reasonable priors. Let's start with the priors for the means
of the populations. The parameter `hm` is the maximum height of the trees. We
are dealing with a pine tree that growth is humid warm areas of North America
and typical has heights of 50 to 80 feet with record heights
(https://edis.ifas.ufl.edu/publication/ST478). Since it is often between 50
and 80, we can specify our prior to have a mean of 65 feet and a standard deviation
of 10:

```r
library(truncnorm)
set.seed(1234)
prior_mu_hm = rtruncnorm(1000, mean = 65, sd = 10, a = 0)
sum(prior_mu_hm > 50 & prior_mu_hm < 80)/1e3 # 87% of heights within this range
hist(prior_mu_hm)
```

We know that parameter `b` is related to growth rates (it is 4 times the maximum
growth rate normalized by maximum height, see Chapter 3 of the book). Searching the literature
(actually using AI with sources...) tells use that average growth rate in this
species is about 2 feet/year. How does `b` related to this? First let's
compute the derivative of the logistic (which would tell you the growth rate).
I will let the computer compute the derivative for me...

```r
library(Deriv) # Package to compute symbolic derivatives
deriv_logistic = Deriv(logistic, "t")
curve(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = x), 0, 35, ylab = "Growth rate")
```

We can now calculate the average growth rate by averaging the first 30 years of
growth (after that it does not seem to grow much):

```r
avg_growth_rate = mean(deriv_logistic(b = 0.25, c = 12, h_m = 65, t = 1:30))
```

Ok, that is exactly the value we got from literature (I guess our data is very
average, but you will not always be so lucky), meaning that an average
growth rate of 2 feet/year corresponds to `b = 0.25`. We don't have information
on uncertainty, so we can take a conservative estimate of say, quarter of the mean:

```r
prior_mu_b = rtruncnorm(1000, mean = 0.25, sd = 0.25/4, a = 0)
hist(prior_mu_b)
```

This would lead to maximum growth rates of 0.5 - 7 feet/year. Finally the
parameter `c` that tells us about the age at which half of maximum height is
reached. Again, checking our AI-powered Google and some of the sources within,
we see that most of the growth happens in the first 20 years and after that
growth rate decrease significantly as the tree matures. Since the logistic curve
is symmetric, this would imply a value of `c = 10` and we can assume again a
standard deviation of a quarter of that:

```r
prior_mu_c = rtruncnorm(1000, mean = 10, sd = 10/4, a = 0)
hist(prior_mu_c)
```

To double check that our priors make sense, we can simulate 1000 growth curves
based on the parameters we just obtained
```r
# Simulation prior population means
t = tree1$age
prior_mu = sapply(t, function(x)
                  logistic(b = prior_mu_b, c = prior_mu_c, h_m = prior_mu_hm, x))
head(prior_mu) # Each column is a different age
```

Let's summarise all these trajectories:

```r
mean_prior_mu = colMeans(prior_mu)
med_prior_mu = apply(prior_mu, 2, quantile, prob = 0.5)
lower_prior_mu = apply(prior_mu, 2, quantile, prob = 0.025)
upper_prior_mu = apply(prior_mu, 2, quantile, prob = 0.975)
plot(t, mean_prior_mu, ylim = range(prior_mu), t = "l")
lines(t, med_prior_mu, col = 2)
lines(t, lower_prior_mu, col = 3)
lines(t, upper_prior_mu, col = 3)
```

We can see that this covers a wide range of trajectories, but it looks very
reasonable. Of course later we can test what happens if our priors are wider
but if you have information do use it and come up with sensible priors rather
than covering an unreasonable range "just to be sure".

More complicated is coming up with reasonable priors for the variances. The
reason is that the variation across trees is going to depend on the context. Is
this a wild population or a more artificial plantation? If it is a wild population,
is it a mixed forest or a very homogeneous system? Unfortunately, most of the
published research focuses on averages, so even if you are an expert, it can be
hard to justify prior distributions for the variances. Given that we have 14
replicates we can allow to be rather vague and calculate these variances as a
function of the better defined means above. For example, we could expect a
standard deviation of 20% around the mean for each trait and half the value for
our uncertainty of what the exact value should be:

```r
prior_sigma_hm = rtruncnorm(1000, mean = 65/5, sd = 65/10, a = 0)
prior_sigma_b = rtruncnorm(1000, mean = 0.25/5, sd = 0.25/10, a = 0)
prior_sigma_c = rtruncnorm(1000, mean = 10/5, sd = 10/10, a = 0)
```

We can now simulate individual trees by combining the priors for means and
standard deviations.

```r
prior_hm = rtruncnorm(1000, mean = prior_mu_hm, sd = prior_sigma_hm, a = 0)
prior_b  = rtruncnorm(1000, mean = prior_mu_b, sd = prior_sigma_b, a = 0)
prior_c  = rtruncnorm(1000, mean = prior_mu_c, sd = prior_sigma_c, a = 0)
```

The resulting population should be more variable than the averages above:

```r
t = 1:25
prior_h = sapply(t, function(x)
                  logistic(b = prior_b, c = prior_c, h_m = prior_hm, x))
mean_prior_h = colMeans(prior_h)
med_prior_h = apply(prior_h, 2, quantile, prob = 0.5)
lower_prior_h = apply(prior_h, 2, quantile, prob = 0.025)
upper_prior_h = apply(prior_h, 2, quantile, prob = 0.975)
plot(t, mean_prior_h, ylim = range(prior_h), t = "l", ylab = "Tree height")
lines(t, med_prior_h, col = 2)
lines(t, lower_prior_h, col = 3)
lines(t, upper_prior_h, col = 3)
```

The last variable which prior we need to specify is the measurement error. The
measurement errors that we should expect depend on how the measurements were taken
which of course we do not know. Assuming a visual estimation using trigonometry,
the reported errors seem to be 1 - 4% of the tree height. This means a prior we
believe an error of 0.2 - 3 m based on the prior predicted heights. We can
achieve this as follows:

```r
prior_sigma = rtruncnorm(1000, mean = 1, sd = 1, a = 0)
hist(prior_sigma)
```

# Learning goals

In this practical you will learn

- The concept of maximum marginal likelihood

- Estimate models with process and measurement errors

- Estimate multilevel non linear models with Gaussian quadrature

- The Bayesian approach for the two models above

- Specify informative priors using general knowledge

# Introduction

In this practical we will cover solve a couple of examples to illustrate
how to solve multilevel models are addressed.

- The first example illustrates situations where it is important to separate
process error (aka stochasticity) and  measurement error, which leads to two
nested stochastic models.

- The second example illustrate situations where multiple individuals are measured
and parameters (*traits*) vary across individuals (representing true ecological
variation, not error), which also leads to two (or more) nested stochastic models.

These are more advanced models compared to the rest of the course and the book
only introduces them as teasers, you are not meant to master them. Therefore,
these practicals are really more tutorials with small exercises in
each section for you to practice and mastering these type of models would
require additional learning beyond this course.

Here is how the rest of the practical is organized:

- First I show how to solve these two problems using maximum marginal likelihood,
explicitly going through every step and introducing some simplifications.

- In the second part I show how to solve these problems using a Bayesian approach. Remember
that this is optional.

Just like in the previous chapters, there are *canned* procedures that can tackle
subsets of the problems you may face and these procedures are much easier to use
in practice. Check the relevant supplement.

# Maximum marginal likelihood approach

The maximum likelihood approach to estimate parameters in multilevel models is
based on the concept of Expectation-Maximization. The general idea is as follows:

1. Organize the random variables in a multilevel hierarchy (the output of one
level is the input to the next). We sort the levels from 1 (bottom) to $n_L$ (top).

2. Calculate a *marginal* likelihood that integrates the full model over all
levels random variables that are not observed. This result in a likelihood with
only one stochastic model in it.

3. Calculate the parameters of the model by maximizing the marginal likelihood
from step 2.

4. Calculate the *conditional* likelihood for the level $N_L - 1$ by marginalizing
over all the level below and fixing parameters to the values estimated in step 3.
Maximizing the conditional likelihood will give you an estimate to the values of
the latent random variable at level $N_L - 1$.

5. If your model has more than two levels, keep repeating step 4 going one level
at a time (walking down the hierarchy).

The examples below will only deal with two levels so step 5 will not be relevant.
Please go through the examples below and come back to the steps above to better
understand the procedure.

## Model with measurement error

Let's assume that the growth rates of trees within a forest follow a Gamma
distribution and the measurements of this growth rate contain errors that follow
a Normal distribution. This is the problem described in section 10.5.2 of the
book. This is a model with two levels:

$$
\begin{align*}
\text{Level}~2~:~X_{true} &\sim Gamma(a, s) \\
\text{Level}~1~:~X_{obs~~}  &\sim Normal(X_{true}, \sigma)
\end{align*}
$$

Where $X_{true}$ are the true growth rates of the trees and $X_{obs}$ are the
measured growth rates. Notice that the level 1 uses as input the output of level
2 ($X_{true}$), which determines the order.

In this exercise we will work with synthetic data generated by stochastic
simulation (see Chapter 5 of the book):

```r
set.seed(1001)
x_true <- rgamma(1000, shape = 3, scale = 10) # True growth rates
x_obs  <- rnorm(1000, mean = x_true, sd = 10) # Observed growth rates with error
hist(x_obs, ylim = c(0,300))
hist(x_true, add = TRUE, col = rgb(1,0,0,0.3))
```

According to the general procedure described below, we can estimate $a$ and $s$
from level 2 by creating a marginal likelihood (integrating over level 1) and
maximizing it (steps 2 - 3). This is shown in the next section. Once that
estimation is done, we can estimate each of value of $X_{true}$ by maximizing the
conditional likelihood (step 4).

### Estimation of parameters

The marginal likelihood is defined as:

$$
L_m \left(X_{obs,i} | a, s , \sigma \right) = \prod_{i=1}^{n} \int{\text{Gamma}\left(X_{true,i} | a, s\right) \text{Normal}\left(X_{obs,i} | X_{true,i} , \sigma \right) dX_{true,i}}
$$

That is, for each observation we integrate over all the possible true values (that
is the *latent* random variable we do not observe) to obtained a likelihood
function that only contains observations and parameters. Because the measurement
error is normal, we can reparameterized as:

$$
L_m \left(X_{obs,i} | a, s , \sigma \right) = \prod_{i=1}^{n} \int{\text{Gamma}\left(X_{obs,i} - \epsilon_i | a, s\right) \text{Normal}\left(\epsilon_i |0 , \sigma \right) d\epsilon_i}
$$

Where we replace $X_{true} = X_{obs} - \epsilon$. This form is how most multilevel
models are expressed. In R the product of the two distributions is:

```r
prodfun <- function(eps, a, s, sigma, x) {
 dgamma(x - eps, shape = a, scale = s)*dnorm(eps, mean = 0, sd =  sigma)
}
prodfun(eps = 1, a = 3, s = 10, sigma = 1, x = x_obs[1])
```

We can now integrate `prodfun` using the function `integrate` (note: this only
works for one latent random variable, more advanced methods are needed when more
latent variables are used):

```r
integrate(f = prodfun, lower = -Inf, upper = Inf,
          a = 3, s = 10, sigma = 1, x = x_obs[1],
          rel.tol = 1e-6, abs.tol = 1e-6)$value
```
We can now build a function to compute the negative log marginal likelihood by
applying this integral to each observations:

```r
NLML <- function(x, a, s, sigma) {
  # Marginal likelihood of each observation
  ML <- sapply(x, function(x) integrate(f = prodfun, lower = -Inf, upper = Inf,
                                        rel.tol = 1e-6, abs.tol = 1e-6,
               a = a, s = s, sigma = sigma, x = x)$value)
  # Negative log marginal likelihood
  NLML <- -sum(log(ML))
  NLML
}
NLML(x_obs, a = 3, s = 10, sigma = 1)
```
Notice that this takes a bit longer than usual because we are doing 1000 numerical
integrations. We can minimize the function `NLML` with `mle2`:

```r
library(bbmle)
fit <- mle2(minuslogl = NLML,
            start = list(a = 3, s = 10, sigma = 1),
            lower = c(0,0,0),
            data = list(x = x_obs), method = "L-BFGS-B")
fit
```

We can now compare the true and estimated models for growth and error to see how
well the estimation worked:

```r
pars = coef(fit)
plot(density(x_obs), ylim = c(0,0.045), xlab = "Growth rate",
     ylab = "Probability density", main = "", las = 1, col = 3)
curve(dgamma(x, shape = 3, scale = 10), add = TRUE)
curve(dgamma(x, shape = pars["a"], scale = pars["s"]), add = TRUE, col = 2)
curve(dnorm(x, mean = 0, sd = 10), add = TRUE, lty = 2)
curve(dnorm(x, mean = 0, sd = pars["sigma"]), add = TRUE, col = 2, lty = 2)
legend("topright", c("Obs", "True growth", "Est growth", "True error", "Est error"),
       col = c(3, 1, 2, 1, 2), lty = c(1, 1, 1, 2, 2))
```

We can quickly estimate the confidence intervals using the quadratic approximation:

```r
confint(fit, method = "quad")
```

Notice that the true values (`a = 3`, `s = 10` and `sigma = 10`) are within the
confidence intervals, so the estimation has worked. We could also try
building the likelihood profiles but that would take quite a while
since this would require many more optimizations so we will skip this part.


## Model for nested data

To keep it within the same theme, in this example
we will look at a simple dataset
of tree growth, but this time using real data. The data describes the growth in
height of several individuals of Loblolly pine (*Pinus taeda*). We can load the
data as follows:

```r
library(ggplot2)
data(Loblolly)
summary(Loblolly)
# eyeballing; hm = 60, b = 0.25, c = 12 (check  Figure 3.9 in the book)
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) + geom_point() +
  stat_function(fun = function(x) 60/(1 + exp(-0.25*(x - 12))), color = "black")
```

Each tree is identified by the column `Seed` and six measurements of height over
the age of the tree are reported. The growth curve of each tree will be modeled
using a logistic curve:

$$
h(t) =  \frac{h_m}{1 + e^{-b\left(t - c \right)}}
$$

where $h$ is the height of the tree, $h_m$ is the maximum height, $b$ control
the steepness of the curve and $c$ is the age at which the half maximum height
is reached (see Chapter 3 of  the book, I will refer to these parameters as
traits from here on) and $t$ is
the age of the tree. We want to know the average values for
the three traits as well as how much they vary across
individuals (variation in traits across individuals is often very relevant in
ecology). In a mixed model, $h_m$, $b$ and $c$ are assumed to vary across individuals
assuming particular distributions (we will assume normal distributions which is
the standard in mixed models).

As a first approach, we can try to estimate the curve for each tree independently.
This is a good way to
get a first estimate of how much the traits vary across individuals as well as
the observation error. As we will see in the examples below, non-linear mixed models
as simple as these ones already need quite some constraints in order to work.

### Stepwise procedure

The stepwise procedure is as follows:

1. Fit the model to each individual separately, using maximum likelihood.

2. Treat the maximum likelihood estimates from each individuals as if they were
observations and analyze them with a separate model.

Let's setup a function to fit the logistic growth curve to each tree:

```r
logistic <- function(b, c, h_m, t) {
  h_m/(1 + exp(-b*(t - c)))
}
mle_fun <- function(b, c, h_m, sigma, t, h) {
  hmod = logistic(b, c, h_m, t)
  -sum(dnorm(h, hmod, sigma, log = TRUE))
}
```

We can test the function for the first tree:

```r
seeds = unique(Loblolly$Seed)
tree1 = subset(Loblolly, Seed == seeds[1])
mle_fun(c = 12, b = 0.25, h_m = 60, sigma = 1, t = tree1$age, h = tree1$height)
```

Let's estimate the parameters for this first tree:

```r
library(bbmle)
fit1 = mle2(mle_fun, start = list(c = 12, b = 0.25, h_m = 60, sigma = 1),
     data = list(t = tree1$age, h = tree1$height))
fit1
```

Let's repeat it for all trees:

```r
traits = matrix(NA, ncol = 4, nrow = length(seeds))
colnames(traits) = c("b", "c", "h_m", "sigma")
for(i in 1:length(seeds)) {
  tree = subset(Loblolly, Seed == seeds[i])
  fit = mle2(mle_fun, start = list(c= 12, b = 0.25, h_m = 60, sigma = 1),
             lower = c(c = 0, b = 1e-4, h_m = 10, sigma = 1e-4),
             upper = c(c = 30, b = 1, h_m = 80, sigma = 20),
            data = list(t = tree$age, h = tree$height), method = "L-BFGS-B")
  traits[i,] = coef(fit)
}
traits
```

Let's look at the individual fits by adding the predicitons to the data:

```r
Loblolly = transform(Loblolly,
                     hm1  = rep(traits[,"h_m"], each = 6),
                     c1  = rep(traits[,"c"], each = 6),
                     b1  = rep(traits[,"b"], each = 6))
# Prediction from stepwise model
Loblolly = transform(Loblolly, pred_height1 = logistic(b1, c1, hm1, age))
# Plot the data and predictions for each indicidual
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height1))
```

We can now look at the (co-)variation of the traits:

```r
library(GGally)
ggpairs(data = traits[,1:3])
```

Notice that b and c are correlated. The standard averages and standard
deviations can be used as initial estimates for the mixed model later:

```r
means = colMeans(traits)
sds   = apply(traits, 2, sd)
cbind(means, sds)
```

In the next sections (and to keep it simple) we will fit a multilevel model where
only $h_m$ is allow to vary across trees.

### Estimating at population level

To obtain the mean estimates of $a$, $b$ and $h_m$ we need to construct a
marginal likelihood to integrate over the distribution of values across
individuals. If we only assume a random effect for $h_m$ we can specify the
model as follows:

$$
\begin{align*}
h_{ij} &\sim \text{Normal} \left(\frac{h_{mi}}{1 + e^{-b\left(t_j - c \right)}}, \sigma \right) \\
h_{mi}  &\sim \text{Normal}(\mu_{hm}, \sigma_{hm})
\end{align*}
$$

where $h_{ij}$ is the height of tree $i$ at age $j$, and $mu_hm$
is the population mean for $h_m$, $\sigma$ represents the observation error and
$\sigma_{hm}$ represents the variation of $h_{mi}$ across individuals. In some of
the literature, the distribution of $h_{ij}$ is referred to as "individual level"
and the distribution of $h_{mi}$ is known as "population level" (and this type
of models are known as *multilevel* or *hierarchical* models).

We can  decompose the values of $h_m$ for each individual into the average and
the  deviation with respect to the average ($\epsilon$). In some of the literature
(where these models are know as *mixed effect* models) the values of $\epsilon$
are know as random effects:

$$
\begin{align*}
h_{ij} &\sim \text{Normal} \left(\frac{h_{mi}}{1 + e^{-b\left(t_j - c \right)}}, \sigma \right) \\
h_{mi} &= \mu_{hm} - \epsilon_{hmi} \\
\epsilon_{hmi}  &\sim \text{Normal}(0, \sigma_{hm}) \\
\end{align*}
$$

To create the marginal likelihood we now need to integrate over the Normal
distributions of $h_m$. Let's first build the function. We do it a bit different
from before, because we have multiple observations for one tree (before we only
had one). This also means we need to be careful with implementation: (i) the
function `integrate` will pass a vector of `eps` values to `prodfun` and we have
multiple values of `t` and `h`. Therefore, we must use sapply:

```r
prodfun <- function(eps_hm, b, c, mu_hm, sigma, sigma_hm, t, h) {
 sapply(eps_hm, function(x) exp(dnorm(x, mean = 0, sd =  sigma_hm, log = TRUE) + # Population level
     sum(dnorm(h, logistic(b, c, mu_hm - x, t), sigma, log = TRUE)))) # Individual level
}
# Evaluate for first tree using as initial values what we obtain from the stepwise approach
prodfun(eps_hm = 0, c = 11.8, b = 0.23, mu_hm = 61,  sigma_hm = 2.3, sigma = 2,
        t = Loblolly$age[1:6], h = Loblolly$height[1:6])
```

The integration for one tree would be:

```r
integrate(f = prodfun, lower = -6*2.3, upper = 6*2.3,
          c = 11.8, b = 0.23, mu_hm = 61, sigma_hm = 2.3, sigma = 2,
          t = Loblolly$age[1:6], h = Loblolly$height[1:6],
          rel.tol = 1e-12, abs.tol = 1e-12)$value
```

We can now define a function to compute the negative log marginal likelihood by
solving the integral for each observation, log transforming and adding them up.
Notice that now we are not applying the integration to each observation but to
each group of observations that belongs to a tree:

```r
NLML <- function(b, c, mu_hm, sigma_hm, sigma, t, h) { # data
  # Every 6 observations is a tree
  id = seq(1, length(t), by = 6)
  ML = sapply(id, function(id)
                   integrate(f = prodfun,lower = -Inf, upper = Inf,
                        c = c, b = b, mu_hm = mu_hm,
                        sigma = sigma, sigma_hm = sigma_hm,
                        t = t[id:(id + 5)], h = h[id:(id + 5)],
                   rel.tol = 1e-12, abs.tol = 1e-12)$value)
  NLML <- -sum(log(ML))
  NLML
}
NLML(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2,
     t = Loblolly$age, h = Loblolly$height)
```

And now we can pass this big boy to bbmle. As usual, I used constrained optimization
to avoid negative values or getting too close to zero (and I check later if I
hit the boundary):

```r
par_0 = c(b = 0.23, c = 11.8, mu_hm = 61, sigma_hm = 2.3, sigma = 2)

fit <- mle2(minuslogl = NLML, start = as.list(par_0),
            data = list(t = Loblolly$age, h = Loblolly$height),
            method = "L-BFGS-B", lower = c(b = 0.01, c = 1, mu_hm = 10,
                                           # error in lower sigma_hm = 0.01, sigma = 0.01),
            control = list(parscale = abs(par_0)))
summary(fit)
```

As usual we can extract the maximum likelihood estimates and the confidence
intervals:

```r
pars = coef(fit)
ci = confint(fit, method = "quad")
print(pars)
print(ci)
```

Notice that the estimates are close to what we estimated before with the
stepwise approach but not exactly the same

```r
cbind(means, sds)
```

### Estimating at individual level

The values of traits for each individual trait can be estimated in the same way
that we estimated the true growth rate of trees, by maximizing the conditional
likelihood for each tree separately. Let's build the negative log conditional
likelihood of the data of one tree conditional on knowing the population averages
and variances:

```r
NCLL <- function(eps_hm, b, c, mu_hm, sigma_hm, sigma, t, h) {
   -dnorm(eps_hm, mean = 0, sd =  sigma_hm, log = T) - # Population level
     sum(dnorm(h, logistic(b, c, mu_hm - eps_hm, t), sigma, log = T)) # Individual level
}
```

Let's the plot NCLL for the first tree

```r
tree1 = subset(Loblolly, Seed == seeds[1])
t = tree1$age
h = tree1$height
eps <- seq(-3,3, by = 0.01)*pars["sigma"]
NCLL1 <- sapply(eps, function(e)
                  NCLL(e, c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = t, h = h))
plot(eps, NCLL1)
```

We can then optimize this function to obtain the estimated deviation between
the maximum height of the first tree and the average of the population and
compare with the value estimated from the stepwise procedure:

```r
eps_1 = optimize(NCLL, c(-3,3)*pars["sigma_hm"],c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = tree1$age, h = tree1$height)$minimum
hm1 = pars["mu_hm"] - eps_1
cat("Multivelel: ", hm1, "Stepwise: ", traits[1,"h_m"])
```

Let's do the estimation for all the trees:

```r
eps = numeric(length(seeds))
for(i in 1:length(seeds)) {
  treei = subset(Loblolly, Seed == seeds[i])
  t = treei$age
  h = treei$height
  eps[i] = optimize(NCLL, c(-3,3)*pars["sigma_hm"], c = pars["c"], b = pars["b"],
                       mu_hm = pars["mu_hm"], sigma_hm = pars["sigma_hm"],
                       sigma = pars["sigma"], t = t, h = h)$minimum
}
hm = pars["mu_hm"] - eps
plot(traits[,"h_m"], hm, ylim = c(52,66), xlim = c(52,66))
abline(a = 0, b = 1)
abline(lm(hm~I(traits[,"h_m"])), lty = 2)
```

We can see that the individual estimates of `hm` from the multilevel model and
the stepwise approach as correlated (on average they are practically the same)
but the multilevel model estimates higher values for the smaller trees and lower
values for the higher trees. That is, the estimates for individual trees are
*pulled* towards the population mean. This is a common effect of using multilevel
models known as *shrinkage*. As long as your model for the variation of `hm`
across individuals is reasonable, the estimates with shrinkages are actually
better than in the stepwise approach (in the sense that they will be closer to
the truth on average).


Let's calculate the predictions for each tree:s

```r
Loblolly = transform(Loblolly,
                     hm2 = rep(hm, each = 6))
Loblolly = transform(Loblolly, pred_height2 = logistic(pars["b"], pars["c"], hm2, age))

# Compare individual predictions for stepwise and multilevel model
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height2))
ggplot(data = Loblolly, aes(x = age, y = height, color = Seed)) +
  geom_point() +
  geom_line(mapping = aes(y = pred_height1))
```

We can visually see that the multilevel model makes predictions that vary less
across trees. Part of it is because of the shrinkage effect, but also because
we ignored the variation across trees of `a` and `b`. We could write the code
to evaluate those two too, but then we have to use for advanced methods of
integration and it gets very tedious.



# Learning goals

You will learn how to:

1. Allow for variance to vary across groups

2. Allow for variance to vary as a continuous function

# Variance across groups

In previous labs we have been modelling the mean of the population or process
under study. In this lab we are going to look into models of the variance or
parameters related to variance (called scale parameters). For many distributions,
the mean and variance are not independent and it might not be possible to specify
them directly with the usual parameterizations. When it doubt, check Chapter 4
of the book (I will give you some hints on possible ways to decouple mean and
scale parameters).

In this first section we will work with allometry data relating the length and
biomass (as ash-free dry weight or `AFD`) of clams on a beach in Argentina (this
data was used in the book by Zuur et al. (2009)). I have given you the data
in the file `Clams.txt`:

```r
library(ggplot2)
clams = read.table("./Lab9/Clams.txt", header = TRUE)
ggplot(clams, aes(x = LENGTH, y = AFD, color = as.factor(MONTH))) + geom_point()
```

This is an allometric relationship, so I will try a power law with a log-normal.
I will also parameterize the log-normal distribution as a function of the actual
mean $\mu$ and coefficient of variation $cv$. We can calculate the `meanlog` and
`sdlog` from `mu` and `cv` as

```r
# Derived from the moments of log-normal (check Wikipedia)
mu = 0.2
cv = 1
meanlog = log(mu/sqrt(1 + cv^2))
sdlog   = sqrt(log(1 + cv^2))
# Test it
test = rlnorm(10000, meanlog = meanlog, sdlog = sdlog)
(mean(test) - mu)/mu
(mean(test)/sd(test) - cv)/cv
```


:::::: {.callout-important title="Exercise" collapse="true"}

1. Fit a power law of the form $a x ^b$ to the clam data with a fixed `cv`. Make
sure to use proper lower and upper bounds on all parameters. Make sure that `mu`
remains positive or the code will crash. Also, since `a` is very small you
probably want to work with `log(a)` or something like that and undo the
transformation inside the `nll` function.

2. The scatter seems to vary across months. Refit the model but vary `cv` per
month. Interpret the result: is there evidence that `cv` would vary across months?
Can we justify using this second model?


::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

1. First eyeball possible parameter values:

```r
with(clams, plot(LENGTH, AFD))
curve(5e-4*x^1.7, add = TRUE)
```

Calculate MLE

```r
library(bbmle)
nll = function(la, b, cv) {
  a = 10^la # Avoid scale issues
  mu = a*clams$LENGTH^b
  if(any(mu < 0)) browser()
  meanlog = log(mu/sqrt(1 + cv^2))
  sdlog   = sqrt(log(1 + cv^2))
  -sum(dlnorm(clams$AFD, meanlog, sdlog, log = TRUE))
}
par0 = c(la = log10(5e-4), b = 1.7, cv = 0.1)
fit = mle2(minuslogl = nll, start = as.list(par0), method = "L-BFGS-B",
           lower = c(la = -6, b = 0, cv = 1e-2),
           upper = c(la = 0, b = 10, cv = 1),
           control = list(parscale = abs(par0), trace = TRUE))
summary(fit)
```

Look at results

```r
mle_a = 10^coef(fit)[1]
with(clams, plot(LENGTH, AFD))
curve(mle_a*x^coef(fit)[2], add = TRUE, col = 2)
```

2. New NLL where a different cv is computed per month

```r
library(bbmle)
month_index = as.integer(as.factor(clams$MONTH))
nll2 = function(la, b, cv1, cv2, cv3, cv4, cv5, cv6) {
  # Avoid scale issues
  a = 10^la
  # Select the right cv
  cv = c(cv1, cv2, cv3, cv4, cv5, cv6)[month_index]
  mu = a*clams$LENGTH^b
  if(any(mu < 0)) browser()
  meanlog = log(mu/sqrt(1 + cv^2))
  sdlog   = sqrt(log(1 + cv^2))
  -sum(dlnorm(clams$AFD, meanlog, sdlog, log = TRUE))
}
par0 = c(la = log10(5e-4), b = 1.7, cv1 = 0.1, cv2 = 0.1, cv3 = 0.1,
         cv4 = 0.1, cv5 = 0.1, cv6 = 0.1)
fit2 = mle2(minuslogl = nll2, start = as.list(par0), method = "L-BFGS-B",
           lower = c(la = -6, b = 0, cv1 = 0.01, cv2 = 0.01, cv3 = 0.01,
                     cv4 = 0.01, cv5 = 0.01, cv6 = 0.01),
           upper = c(la = 0, b = 10, cv1 = 1, cv2 = 1, cv3 = 1,
                     cv4 = 1, cv5 = 1, cv6 = 1),
           control = list(parscale = abs(par0), trace = TRUE))
summary(fit2)
```

There is substantial variation in the estimated `cv`s that cannot be explain by
uncertainty, suggesting that they might be different across months (at least
when we hold `a` and `b` constant).

Because the models are nested and we have a lot of data, we could use a likelihood
ratio test to compare them or AIC:

```r
anova(fit, fit2)
AIC(fit, fit2)
```

Both indices confirm that the second model is much better.

::::

:::::

::::::

# Variance along continuous variables

In the book, Bolker analyzes a dataset on cone production by fir trees as a
function of their diameter:

```r
library(emdbook)
# Remove missing data
cleanFIR = FirDBHFec[!is.na(FirDBHFec$TOTCONES) & !is.na(FirDBHFec$DBH),]
with(cleanFIR, plot(TOTCONES~DBH))
```

We are dealing with count data. We can first assess whether there is over-
dispersion in these counts. We can assess this by comparing the variances
to the mean cone production. For this we need to bin the data into intervals
of diameter at breast height:


```r
# Create bins
DBHpoints = c(6,8,10,12,14,18)
# Allocate vetor to store vars calculate for first bin
vars = numeric(length(DBHpoints))
means = numeric(length(DBHpoints))
temp = subset(cleanFIR, DBH <= DBHpoints[1])$TOTCONES
vars[1]  = var(temp)
means[1] = mean(temp)
# Repeat for all bins
for(i in 2:length(DBHpoints)) {
  temp = subset(cleanFIR, DBH <= DBHpoints[i] & DBH >= DBHpoints[i - 1])$TOTCONES
  vars[i] = var(temp)
  means[i] = mean(temp)
}
```

We can now plot the results

```r
plot(means, vars, xlab = "means TOTCONES", ylab = "vars TOTCONES",
     ylim = range(means, vars), xlim = range(means, vars))
abline(a = 0, b = 1)
```

We can clearly see that the variance is much larger than the mean cone production
so we might want to use a negative binomial defined by a mean (`mu`) and a
dispersion parameter `size`. The question is whether assuming a constant `size`
is good enough to capture the overdispersion or whether we want `size` to vary
as a function of DBH. A method of moments estimate of `size` is `size = mu^2/(var - mu)`:


```r
plot(DBHpoints, means^2/(vars - means), xlab = "DBH", ylab = "size TOTCONES")
```

We can see that the `size` parameter is increasing with diameter, indicating that
as the number of cone production increases, the overdispersion actually decreases,
so we may want to try modelling this increase. The data suggest that a non-linear
increase appears to be most adequate. For example, a scaled and shifted logistic
seems to fit quite well:

```r
plot(DBHpoints, means^2/(vars - means), xlab = "DBH", ylab = "size TOTCONES")
curve(1 + 5.5/(1 + exp(-0.7*(x  - 12))), add = TRUE)
```


:::::: {.callout-important title="Exercise" collapse="true"}

1. Fit a non-linear model to the relationship between `TOTCONES` and `DBH` with
a negative binomial error distribution where the `size` parameter increases with
diameter at breast height following a shifted and scaled logistic function.

2. Simulate some observations from the fitted model and compare to original
data visually

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

I am going to fit a power law for the mean trend. Let's eyeball its coefficients
from the raw data:

```r
with(cleanFIR, plot(TOTCONES~DBH))
curve(0.5*x^2, add = TRUE)
```

We can now fit the model using the power law for `mean` and logistic for `size`
using the eye-balled parameters from above:

```r
nll = function(a, b, c, d, e, f) {
  mu = a*cleanFIR$DBH^b
  size = c + d/(1 + exp(-e*(cleanFIR$DBH - f)))
  NLL = -sum(dnbinom(as.integer(cleanFIR$TOTCONES), mu = mu, size = size, log = TRUE))
  NLL
}
par0 = c(a = 0.5, b = 2, c = 1, d = 5.5, e = 0.7, f  = 12)
nll(a = 0.5, b = 2, c = 1, d = 5.5, e = 0.7, f  = 12)
fit = mle2(minuslogl = nll, start = as.list(par0), method = "L-BFGS-B",
           lower = c(a = 0.1, b = 1, c = 0.1, d = 2, e = 0.2, f = 2),
           upper = c(a = 2,   b = 4, c = 1.5, d = 8, e = 5.0, f = 20),
           control = list(parscale = abs(par0)))
summary(fit)
```

2. We can simulate results using fitted model by computing the `mean` and `size`
and sampling from NBinom rather than computing the density of observations:

```r
set.seedd(101)
p = coef(fit)
mu = p[1]*cleanFIR$DBH^p[2]
size = p[3] + p[4]/(1 + exp(-p[5]*(cleanFIR$DBH - p[6])))
sample = rnbinom(nrow(cleanFIR), mu = mu, size = size)
plot(TOTCONES~DBH, data = cleanFIR)
points(cleanFIR$DBH, sample, col = 2)
```

That looks quite reasonable!

::::

:::::

::::::

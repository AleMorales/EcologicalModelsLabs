# Learning goals

This lab has two goals:

- Practice with probability distributions on different types of data.

- Make you familiar with the technicalities of stochastic distributions in `R`.

In particular, how to generate values from probability distributions and how to make
your own probability distribution. Under time constraints, make sure that you make
at least the exercises in the first three sections.


# Choosing probability distributions

In this exercise we revisit the *shapes* dataset from the previous lab. Remember that
we had six different datasets each describing different ecological phenomenon. The
first step to model your data was to choose a deterministic function that describes the
mean effect of the predictor variable (`x`) on the response variable (`y`). The second step
involves the choice of the stochastic distribution which describes how the data varies
around the mean and that is what we will focus on today.

:::::: {.callout-important title="Exercise" collapse="true"}

Reload the six datasets and choose for each dataset one or two candidate probability
distributions. To guide you in making the choice consider whether you have counts
(integer) or continuous (real) values. Also, if you have continuous values, are they
constraint? (e.g., only positive, or only between zero and one).

For simplicity, you may work with the distributions in table 4.1 in the book. But
if you are curious, you can check this Wikipedia page that lists a large number
of distributions: https://en.wikipedia.org/wiki/List_of_probability_distributions

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

1. Dataset 1

`y` represents real numbers and both positive and negative numbers occur. This
implies that we should choose a continuous probability distribution with support
over all real numbers. The normal seems a good candidate distribution because this
one runs from -$\inf$ to +$\inf$. The other continuous distributions in the book
(Table 4.1) would not be such good candidates because they do not allow negative
values. If we expect many extreme values we may choose distributions with *thicker*
tails. The book does not offer any examples for unbounded continuous variables but
from Wikipedia we may choose the [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution)
or the [noncentral t-distribution](https://en.wikipedia.org/wiki/Noncentral_t-distribution)


2. Dataset 2

`y` represents real positive numbers. Indeed, the data represents a functional response
(predation rate), and that cannot be negative. From Table 4.1, possible distributions would
be Gamma, Lognormal or Exponential. The Exponential is less flexible (mode always at zero)
and the Gamma distribution is not defined for `y = 0`, so the Lognormal is probably
a better choice. In addition, if you assume a Lognormal, that is equivalent to
assuming `log(y)` follows a Normal distribution (this is one reason why people often
log transform).

Looking at Wikipedia we find some famous distributions such as F or Chi-squared.
In practice we do not usually use these distributions for building models but rather
they show up in hypothesis testing (but there is no hard reason why you would not use
them for modeling).


3. Dataset 3

`y` seems represents counts of pine cones. Given that it contains counts we can
pick a distribution from the family of discrete distributions. The Poisson and the
Negative Binomial could be good candidates to describe this type of data. Alternatives
from online sources would include Beta-Binomial. Sometimes we may also use continuous
distributions (for positive reals) for count data when the count values get very large
and it those cases you may get similar results.

4. Dataset 4

`y` represents population size over time and from looking at the data, they seems to
indeed represent counts. Given that it contains counts we can pick a distribution from
the family of discrete distributions. The Poisson and the Negative Binomial could also
be good candidates to describe this type of data.

5. Dataset 5

No information is given on y. The data clearly seems to represent counts. Thus,
the same reasoning applies here as to the two previous datasets.

6. Dataset 6

The data (`y`) represents species occurrences at different coordinates (presence/absence).
For binary processes we use the Bernoulli distribution (as in coin flipping exercises). But
we can also count the number of presences in a particular range and in that case we would
use the Binomial distribution.

::::

:::::

::::::


# Random distributions in `R`

`R` knows about lots of probability distributions (and there are packages that define
additional distributions). For each distribution, it defines four functions that can
do different things related to the distribution. Taken the Binomial distribution
for random variable `x` as example it can:

- Generate random numbers drawn from the distribution. For example `rbinom(n,size,prob)`
gives `n` values of `x` with parameters `size` (total number of draws) and `p` (probability
of success on each draw). Every distribution will have an equivalent function (`rnorm` for
the Normal, `rpois` for the Poisson, etc.).

- Compute the probability (density or mass) function. For example `dbinom(n, size prob)`
will give the probability mass that $x = n$.

- Compute the cumulative probability function. For example `pbinom(q, size prob)`
will give the cumulative probability that $x \leq q$, that is $\int_0^q P(x)dx$.

- Compute the quantile function. For example, `qbinom(p, size, prob)` gives the
$x$ value such as that $\int_0^q P(x)dx = p$.

Notice that the quantile and cumulative functions are related. In one case, I fix
a cumulative probability value and I calculate a value of $x$ (quantile) and in the
other case I fix the value of $x$ and I calculate the cumulative probability.

We can generate random draws with the same or different parameter values. For
example:

```r
rbinom(10,size=8,prob=0.5)
rbinom(3,size=8,prob=c(0.2,0.4,0.6))
```
In the first case I generate 10 random values from the same distribution. In the
second case I generate three random values from three different distributions (they
different in the argument `prob`).

The code below shows the result of drawing a large number of values from a binomial
distribution with $N=12$ (i.e., `size = 12`) and $p=0.5$ (i.e., `prob = 0.5`) and
plotting the results as a `factor` in order to create bars (**never use histograms on discrete variables**):

```r
set.seed(1)
plot(factor(rbinom(5000,size=12,prob=0.5)), xlab="# of successes",
     ylab="# of trials out of 5000")
```

The four functions described above exist for each of the distributions `R` has
built-in: e.g. for the normal distribution they're `rnorm()`, `pnorm()`, `dnorm()`, `qnorm()`.
Each distribution has its own set of parameters (so e.g. `pnorm()` is `pnorm(x,mean=0,sd=1)`).
To see which distributions are available in the `base` package; check `?distributions`.
For additional distributions check the packages listed in https://cran.r-project.org/web/views/Distributions.html

The code below plots the four functions for the Normal distribution. We generate some
arrows to indicate what is an input to the function and what is an output.

```r
par(mfrow = c(2,2), mar = c(2.5,4,2.5,1), mgp = c(3,1,0), las = 1, xaxs = "i", yaxs = "i")

# Parameters and variables
mu = 10
sd = 3
x = 7.5
P = 0.4

# dnorm
curve(dnorm(x,mu,sd),0,20, ylab = "p(x)", main = "dnorm", ylim = c(0,0.15))
arrows(x,0,x,dnorm(x,mu,sd), length = 0.1, col = 2)
arrows(x,dnorm(x,mu,sd), 0, dnorm(x,mu,sd), length = 0.1, col = 2)

# rnorm
curve(dnorm(x,mu,sd),0,20, ylab = "p(x)", main = "rnorm", ylim = c(0,0.15))
set.seed(0)
sample = rnorm(20, mu, sd)
points(sample, rep(0.005, 20), cex = 1.5)

# pnorm
curve(pnorm(x,mu,sd),0,20, ylab = "P(x)", main = "pnorm", ylim = c(0,1))
arrows(x,0,x,pnorm(x,mu,sd), length = 0.1, col = 2)
arrows(x,pnorm(x,mu,sd), 0, pnorm(x,mu,sd), length = 0.1, col = 2)

# qnorm
curve(pnorm(x,mu,sd),0,20, ylab = "P(x)", main = "qnorm", ylim = c(0,1))
arrows(0,P, qnorm(P,mu,sd),P, length = 0.1, col = 2)
arrows(qnorm(P,mu,sd),P, qnorm(P,mu,sd), 0, length = 0.1, col = 2)

par(mfrow = c(1,1))
```

:::::: {.callout-important title="Exercise" collapse="true"}

For the binomial distribution with 10 trials and a success probability
of 0.2:

1. Pick 8 random values and sort them into increasing order
(if you `set.seed(1001)` beforehand, you should get $X=0$
(twice), $X=2$ (4 times), and $X=4$ and $X=5$ (once each)).

2. Calculate the probabilities of getting 3, 4, or 5
successes first by hand (See Chapter4) and check it with the computer.

3. Calculate the probability of getting 5 or more successes.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

1.

```r
set.seed(1001)
rbinom(8,prob=0.2,size=10)
```
2.

```r
dbinom(3:5,size=10,prob=0.2)
```
3.

We can calculate it as the complement of the cumulative probability:

```r
1 - pbinom(4,size=10,prob=0.2)
```

It is also possible to set `lower.tail = FALSE` which does this internally:

```r
pbinom(4,size=10,prob=0.2, lower.tail = FALSE)
```

::::

:::::

::::::


You can use the `R` functions to test your understanding of a distribution and make
sure that random draws match up with the theoretical distributions as they should.
This procedure is particularly valuable when you're developing new probability distributions
by combining simpler ones, e.g. by zero-inflating or compounding distributions.

The results of a large number of random draws should have the correct moments (mean
and variance), and a histogram of those random draws (with `freq=FALSE` or `prob=TRUE`)
should match up with the theoretical distribution. For example, draws from a binomial
distribution with $p=0.2$ and $N=20$ should have a mean of approximately $Np=4$
and a variance of $Np(1-p)=3.2$:

```r
set.seed(1001)
N=20; p=0.2
x = rbinom(10000,prob=p,size=N)
c(mean(x),var(x))
```

The mean is very close, the variance is a little bit farther off. Just for the heck
of it, we can use the `replicate()` function to re-do this command many times and see how close we get:

```r
var_dist = replicate(1000,var(rbinom(10000,prob=p,size=N)))
```

Looking at the summary statistics and at the 2.5% and 97.5% quantiles of the
distribution of variances:

```r
summary(var_dist)
quantile(var_dist,c(0.025,0.975))
hist(var_dist)
```

Even though there's some variation (of the variance) around the theoretical value,
we seem to be doing the right thing since the 95% confidence limits include the theoretical
value (and the average of this sampling distribution is very close to the true value).
Chapter 5 and the last exercise in Lab 6 go deeper into how we can use simulations
to check estimates.

I the figure below I show the barplot of 10000 draws along with the theoretical values.

```{r, echo = FALSE}
N=20; p=0.2
x = rbinom(10000,prob=p,size=N)
tx = table(factor(x,levels=0:12))/10000
b1 = barplot(tx,ylim=c(0,0.23),ylab="Probability")
points(b1,dbinom(0:12,prob=p,size=N),pch=16)
```


The steps in `R` to produce this figure are:

**1.** Generate 10,000 random deviates:

```r
x = rbinom(10000,prob=p,size=N)
```

**2.** Tabulate the values, and divide by the number of samples to get their frequencies:

```r
tx = table(factor(x,levels=0:12))/10000
```

Note that the `levels` command is necessary in this case because the probability of $x=12$
with $p=0.2$ and $N=12$ is actually so low ($\approx 4\times 10^{-9}$) that it's very
unlikely that a sample of 10,000 won't include any samples with 12 successes.

**3.** Draw a barplot of the values, extending the $y$-limits a bit to make room for
the theoretical values and saving the $x$ locations at which the bars are drawn:

```r
b1 = barplot(tx,ylim=c(0,0.23),ylab="Probability")
```

**4.** Add the theoretical values, plotting them at the same $x$-locations as the centers
of the bars (these were stored inside `b1`):

```r
points(b1,dbinom(0:12,prob=p,size=N),pch=16)
```

Note that `barplot()` doesn't put the bars at $x$ locations (because the x
axis is assume discrete, there is no concept of coordinates, imagine if these
data were countries or species). For that reason, we need to store the actual
coordinates where each bar was drawn by storing these in the variable `b1` to
make sure that the theoretical values end up in the right place.

:::::: {.callout-important title="Exercise" collapse="true"}

Pick 10,000 negative binomial deviates with $\mu=2$, $k=0.5$ (using `rnbinom()`). In
`rbnbinom()` $\mu$ = `mu` and $k$ = `size`. Pick one of the ways above to draw
the distribution. Check that the mean and variance agree reasonably well with the
theoretical values. Add points representing the theoretical distribution to the plot.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

We can generate the sample as follows:

```r
mu = 2
k = 0.5
x = rnbinom(10000, mu = mu, size = k)
tx = table(factor(x, levels = 0:max(x)))/10000
b1 = barplot(tx, ylab = "Probability")
points(b1, dnbinom(0:max(x), mu = mu, size = k), pch = 1)
mean(x)
var(x)
mu
mu * (1 + mu/k)
```

There is an alternative parameterization probability and size:

```r
p = 1/(1 + mu/k)
n = k
b1 = barplot(tx, ylab = "Probability")
points(b1, dnbinom(0:max(x), mu = mu, size = k), pch = 1)
points(b1, dnbinom(0:max(x), prob = p, size = k), pch = 2)
```
::::

:::::

::::::

Doing the equivalent plot for continuous distributions is actually somewhat easier,
since you don't have to deal with the complications of a discrete distribution: the
histogram method will automatically create bins for the data and tabulate the
frequency and derive estimates of probability from that (`hist(...,prob=TRUE)`).
You could then add the theoretical density function on top (e.g.:
`curve(dgamma(x, shape = 2, scale = 1), add = TRUE)`).

# Averaging distributions

Suppose we have a (tiny) data set; we can organize it in two different ways, in
standard long format or in tabular form:

```r
dat = c(5,6,5,7,5,8); dat
tabdat = table(dat); tabdat
```

To get some basic estimates of the probabilities from the data, we can just scale the
calculated frequency by the total sample size:

```r
prob=tabdat/sum(tabdat); prob
```

In the long format, we can take the mean with `mean(dat)` or, replicating the
formula $\sum x_i/N$ exactly, `sum(dat)/length(dat)`.

In the tabular format, we can calculate the mean with the formula $\sum P(x) x$,
which in `R` would be `sum(prob*5:8)` or more generally

```r
vals = as.numeric(names(prob))
sum(prob*vals)
```

`names` extracts the names of the vector and `as.numeric` transform characters to numbers.
You could also get the values by `as.numeric(levels(prob))`, or by `sort(unique(dat))`.

Going back the other way, from a table to raw values, we can use
the `rep()` function to repeat values an appropriate number of times.
In its simplest form, `rep(x,n)` just creates a vector repeats `x` (which
may be either a single value or a vector) `n` times, but **if n is a vector as well**
then each element of `x` is repeated the corresponding number of times: for example,

```r
rep(c(1,2,3),c(2,1,5))
```

gives two copies of 1, one copy of 2, and five copies of 3.

Therefore,

```r
rep(vals,tabdat)
```

will recover our original data (although not in the original order) by repeating each
element of `vals` the correct number of times.

## Jensen's inequality

Jensen's inequality states the following: Suppose you have a number of values, $x$,
with a mean $\bar{x}$, and a non-linear function $f(x)$. Then the mean of $f(x)$ is
not equal to $f(\bar{x})$.

Jensen's inequality can be important in a number of cases. The first one is mentioned in
Chapter 4 (page 104) on how variability can change the mean behaviour of a system (damselfish).

Another example where Jensen's inequality kicks in is when transforming your data. Data
transformations are commonly applied to get normally distributed errors. Because in
statistical models you are often interested in the mean effect of a given treatment.

Note that quantiles are not afffected by Jensen's inequality as long as the transformation
is monotonic. Thus, if you were to model the median effect of a given treatment you
could back transform that.

:::::: {.callout-important title="Exercise" collapse="true"}

Find out what the effect of Jensen's inequality is on a series of log-tranformed
datapoints with respect to the estimated mean.

Use the following pseudo-code:

1. Generate 10 random deviates from a uniform distribution (choose the range of 0 to 10).

2. Calculate the mean of those 10 deviates.

3. Plot the function $\log(x)$ with `curve()` on the range from 0-10, and plot random sample onto it.

4. Calculate the mean of the log-transformed values and transform this mean back the
normal scale, and compare to the mean calculated at 1.

5. Plot the means with `abline(h=...)` if you want to draw a horizontal line or `abline(v=...)`
to draw a vertical line.

6. Explain differences between the two means.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

1.
```r
rf = runif(10,min=0,max=10)
```

2.
```r
mean(rf)
```

3.
```r
plot(log(rf)~ rf)
curve(log(x),add=T)
```
4.
```r
exp(mean(log(rf)))` versus `mean(rf)
```

5.
```r
segments(x0=0,y0=log(mean(rf)),x1=mean(rf), y1=log(mean(rf)),lty=1)
segments(x0=mean(rf),y0=0,x1=mean(rf), y1=log(mean(rf)),lty=1)
segments(x0=0,y0=mean(log(rf)),x1=exp(mean(log(rf))), y1=mean(log(rf)),lty=2)
```

A dotted line for the mean of the log transformed values

```r
segments(x0=exp(mean(log(rf))),y0=mean(log(rf)),
            x1=exp(mean(log(rf))),
            y1=min(log(rf)),lty=2)
```

By doing a log transformation first, the higher values are "compressed" and weigh
less into the mean.

::::

:::::

::::::


This exercise shows that it is usually a good idea to leave variables untransformed
when estimating the properties from this data.

# The method of moments: reparameterizing distributions

In the chapter, I showed how to use the *method of moments* to estimate the parameters
of a distribution by setting the sample mean and variance ($\bar x$, $s^2$) equal to
the theoretical mean and variance of a distribution and solving for the parameters.
For the negative binomial, in particular, I found $\mu=\bar x$ and $k=(\bar x)/(s^2/\bar x -1)$.

You can also define your own functions that use your own parameterizations: call them `my_function`
rather than just replacing the standard `R` functions.

For example, defining

```r
my_dnbinom = function(x,mean,var,...) {
  mu = mean
  k = mean/(var/mean-1)
  dnbinom(x,mu=mu,size=k,...)
}

my_rnbinom = function(n,mean,var,...) {
  mu = mean
  k = mean/(var/mean-1)
  rnbinom(n,mu=mu,size=k,...)
}
```

(the `...` in the function takes any other arguments you give to `my_dnbinom` and
just passes them through, unchanged, to `dnbinom`).

Defining your own functions can be handy if you need to work on a regular basis with
a distribution that uses a different parameterization than the one built into the
standard `R` function.

You can use the kinds of histograms shown above to test your results (remembering
that the method of moments estimates may be slightly biased especially for small samples
--- but they shouldn't cause errors as large as those caused by typos in code or
mistakes in the formulae).

```r
x = my_rnbinom(100000,mean=1,var=4)
mean(x)
var(x)
```

```r
tx = table(factor(x,levels=0:max(x)))/100000
b1 = barplot(tx,ylab="Probability")
points(b1,my_dnbinom(0:max(x),mean=1,var=4),pch=16)
abline(v=1)
```

:::::: {.callout-important title="Exercise" collapse="true"}

Morris (1997) gives a definition of the beta function that is different from the
standard statistical parameterization. The standard parameterization is

$$
\mbox{Beta}(x|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1}(1-x)^{b-1}
$$

whereas Morris uses

$$
\mbox{Beta}(x|P,\theta) = \frac{\Gamma(\theta)}{\Gamma(\theta P)\Gamma(\theta (1-P))} x^{\theta P-1} (1-x)^{\theta(1-P)-1}.
$$

1. Find expressions for $P$ and $\theta$ in terms of $a$ and $b$ and viceversa (use pen and paper).

2. Explain why you might prefer Morris's parameterization.

3. Define a new set of functions that generate random numbers from the beta distribution
(`my_rbeta`) and calculate the density function (`my_dbeta`) in terms of $P$ and $\theta$.

4. Generate a histogram from this distribution and draw a vertical line showing the
mean of the distribution. Vertical lines can be drawn by using `abline(v=...)`

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

1. Based just on the expressions in the normalization constant $\Gamma(a+b)/(\Gamma(a)\Gamma(b))$
for the standard parameterization, $\Gamma(\theta)/(\Gamma(\theta P)\Gamma(\theta(1-P))))$ gives
$\theta=a+b$, $P=a/(a+b)$ or conversely $a = \theta P$, $b=\theta(1-P)$.

2. In this parameterization, P is the mean proportion/number of successes/etc. and
$\theta$ governs the width of the distribution

3.
```r
my_rbeta = function(n, theta, P) {
    rbeta(n, shape1 = theta * P, shape2 = theta * (1 - P))
}

my_dbeta = function(x, theta, P) {
    dbeta(x, shape1 = theta * P, shape2 = theta * (1 - P))
}
```

4.

```r
x = my_rbeta(1000, theta = 10, P = 0.2)
hist(x, breaks = 50, prob = TRUE, col = "gray")
curve(my_dbeta(x, theta = 10, P = 0.2), add = TRUE, lwd = 2)
abline(v = 0.2, lwd = 2, lty = 3)
abline(v = mean(x), lty = 2)
```

::::

:::::

::::::


# Creating new distributions

## Zero-inflated distributions

The general formula for the probability distribution of a zero-inflated distribution,
with an underlying distribution $P(x)$ and a zero-inflation probability of $p_z$, is:

$$
\begin{eqnarray*}
\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\
\mbox{Prob}(x>0) & = & (1-p_z) P(x)
\end{eqnarray*}
$$

So, for example, we could define a probability distribution for a zero-inflated
negative binomial as follows:

```r
dzinbinom = function(x,mu,size,zprob) {
  ifelse(x==0,
         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),
         (1-zprob)*dnbinom(x,mu=mu,size=size))
}
```

The name, `dzinbinom`, follows the `R` convention for a probability distribution
function: a `d` followed by the abbreviated name of the distribution, in this case
`zinbinom` for "**z**ero-**i**nflated **n**egative **binom**ial").

The `ifelse()` command checks every element of `x` to see whether it is zero or not
and fills in the appropriate value depending on the answer.

The sampling function for our zero-inflated distribution would look like this:

```r
rzinbinom = function(n,mu,size,zprob) {
  ifelse(runif(n)<zprob,
         0,
         rnbinom(n,mu=mu,size=size))
}
```

The command `runif(n)` picks `n` random values between 0 and 1; the `ifelse` command
compares them with the value of `zprob`.  If an individual value is less than `zprob`
(which happens with probability `zprob`=$p_z$), then the corresponding random number
is zero; otherwise it is a value picked out of the appropriate negative binomial
distribution.

:::::: {.callout-important title="Exercise" collapse="true"}

Check graphically that these functions actually work. For instance, you could compare
the results with a negative binomial function with the same mean and variance as the
data.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

```r
rzinbinom = function(n,mu,size,zprob) {
    ifelse(runif(n)<zprob, 0,rnbinom(n,mu=mu,size=size))
}
a = rzinbinom(1000,mu=4,size=1,zprob=0.2)
mean.a = mean(a)
var.a = var(a)
size = 1/(((var.a - mean.a))/mean.a^2)
a1 = rnbinom(1000,mu=mean.a,size=size)
x = as.numeric(names(table(a)))
plot(as.numeric(table(a))~ x,type="h")
x = as.numeric(names(table(a1)))
points(as.numeric(table(a1))~ x,type="p")
```
::::

:::::

::::::

# Compounding distributions

The key to compounding distributions in `R` is that the functions that generate random
deviates can all take a vector of different parameters rather than a single parameter.
For example, if you were simulating the number of hatchlings surviving (with individual
probability 0.8) from a series of 8 clutches, all of size 10, you would say

```r
rbinom(8,size=10,prob=0.8)
```

but if you had a series of clutches of different sizes, you could still pick all the
random values at the same time:

```r
clutch_size = c(10,9,9,12,10,10,8,11)
rbinom(8,size=clutch_size,prob=0.8)
```

Taking this a step farther, the clutch size itself could be a random variable:

```r
clutch_size = rpois(8,lambda=10)
rbinom(8,size=clutch_size,prob=0.8)
```

We've just generated a Poisson-binomial random deviate...

As a second example, I'll follow Clark *et al.* in constructing a distribution that
is a compounding of normal distributions, with 1/variance of each sample drawn from
a amma distribution.

First pick the variances as the reciprocals of 10,000 values from a gamma distribution
with shape 5 (setting the scale equal to 1/5 so the mean will be 1):

```r
var_vals=1/rgamma(10000,shape=5,scale=1/5)
```

Take the square root, since `dnorm` uses the standard deviation and not the variance
as a parameter:

```r
sd_vals = sqrt(var_vals)
```

Generate 10,000 normal deviates using this range of standard deviations:

```r
x = rnorm(10000,mean=0,sd=sd_vals)
```

Figure 4 shows a histogram of the following commands:

```r
hist(x,prob=TRUE,breaks=100,col="gray")
curve(dt(x,df=11),add=TRUE,lwd=2)
```

```{r,echo=FALSE, fig=TRUE, fig.cap="Clark model: inverse gamma compounded with normal, equivalent to the Student $t$ distribution"}
var_vals=1/rgamma(10000,shape=5,scale=1/5)
sd_vals = sqrt(var_vals)
x = rnorm(10000,mean=0,sd=sd_vals)
hist(x,prob=TRUE,breaks=100,col="gray")
curve(dt(x,df=11),add=TRUE,lwd=2)
```

The superimposed curve is a $t$ distribution with 11 degrees of freedom; it turns out that
if the underlying gamma distribution has shape parameter $p$, the resulting $t$ distribution
has $df=2p+1$. Figuring out the analytical form of the compounded probability distribution
or density function, or its equivalence to some existing distribution, is the hard part;
for the most part, though, you can find these answers in the ecological and statistical
literature if you search hard enough.

:::::: {.callout-important title="Exercise" collapse="true"}

Generate 10,000 values from a gamma-Poisson compounded distribution with parameters
`shape = 0.5`, `scale = 4/0.5 = 8` and demonstrate that it's equivalent to a
negative binomial with the appropriate `mean` and `shape` parameters.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

```r
mu = 4
k = 0.5
x = rpois(10000, rgamma(10000, shape = k, scale = mu/k))
plot(table(x)/10000)
points(0:max(x), dnbinom(0:max(x), mu = mu, size = k), cex = 0.75)
```
::::

:::::

::::::

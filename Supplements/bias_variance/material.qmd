# Learning goals

You will learn how to:

1. Reducing bias using restricted maximum likelihood (REML)

2. Reducing variance by using the mode of the posterior (MAP)

# Reducing bias in estimates: Restricted maximum likelihood (REML)

REML is a technique that originated in linear models but can be extended to 
non-linear models and has to do with the concept of bias. Bias of an estimator
means that if you were to take many samples from the same population and fit the
same model to each sample, the distribution of parameter estimates across
samples would be centered around the true values.

## Bias in estimates of linear models

In a linear model the maximum likelihood estimates of intercepts and slopes are 
unbiased, but the maximum likelihood estimate of the variance ($\sigma^2$) is
biased. This motivated the development of REML to compute unbiased estimates
of the parameters related to the variance in linear models. If you are fitting
linear models, you probably want to use one of the canned methods.

```r
# Assume a very vanilla linear model but with std dev increasing exponentially
x = seq(0,1, 0.1)
# Generate many many samples
N = 1e4
Y = t(sapply(x, function(x) rnorm(N, 2 + 10*x, exp(x))))
```

We will use the canned method `gls` from the package `nlme` to fit the model
with normal maximum likelihood:

```r
library(nlme)
y = Y[,1]
fit = gls(y~x, weights = varExp(form = ~x), method = "ML")
fit
```

We can extract the intercept slope and exponential coefficient as follows:

```r
c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])
```
We can now do it for all the samples

```r
library(future.apply)
plan(multisession)
coefs = t(future_sapply(1:N, function(i) {
            y = Y[,i]
            fit = gls(y~x, weights = varExp(form = ~x), method = "ML")
             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])
            }))
head(coefs)
```

We can now calculate the means of each column and compare to the true values:

```r
bias = colMeans(coefs) - c(2, 10, 1)
bias
```

This makes more relative to the true values:

```r
bias/c(2, 10, 1)
```

Notice how the estimates of the coefficient relating $\sigma$ to $x$ is being 
overestimated by 15% when we compare the average of the sampling distribution to
the true value. We do not see this bias in the intercept and slope. 

If we now redo the fits using REML we will get also unbiased estimates for the
coefficient related to the variance:

```r
coefs_REML = t(future_sapply(1:N, function(i) {
            y = Y[,i]
            fit = gls(y~x, weights = varExp(form = ~x), method = "REML")
             c(coef(fit), sigma_coef = fit$modelStruct[[1]][1])
            }))
```

The estimates are now unbiased

```r
bias_REML = colMeans(coefs_REML) - c(2, 10, 1)
bias_REML/c(2, 10, 1)
```

Many statisticians seem to be obsessed with the bias in estimates, but remember
that any estimate that you make from a single sample will have an error. For
example, we can quantify the Mean Squared Error (MSE) for the ML estimate:

```r
MSE = c(a = mean((coefs[,1] - 2)^2),
        b = mean((coefs[,2] - 10)^2),
        c = mean((coefs[,3] - 1)^2))
MSE
```

The MSE is actually equal to the bias squared plus the variance of the estimator.
This means that we can compute the contribution of bias and variance to the error:

```r
vars = c(a = var(coefs[,1]), b = var(coefs[,2]), c = var(coefs[,3]))
cbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)
```
We can see that in the parameters `a` and `b` all the error is due to variance
in the parameter (since they are unbiased) and in `c` only 11% of the error is
due to the bias.

This means that using REML only reduces the error of the estimation by a small 
amount. Let's compare the new MSE with REML:

```r
MSE_REML = c(a = mean((coefs_REML[,1] - 2)^2),
             b = mean((coefs_REML[,2] - 10)^2),
             c = mean((coefs_REML[,3] - 1)^2))
cbind(MSE, MSE_REML, Reduction = (MSE - MSE_REML)/MSE)
```

So, using REML decreased the error in the estimation by `c` by 33% (incidentally
the error in `a` and `b` also decreased, that will not always happen but can).
This is a pretty good result, especially if you care about the variation in 
your data (i.e., if it is of ecological relevance and not just noise). So, if
you can use REML in a linear model, it pays off to use it.

Note that methods for model comparison that rely on (pure) maximum likelihood will
not apply to models estimated via REML. This includes AIC, BIC and likelihood
ratio tests. So, model comparison should always be done with models estimated
via maximum likelihood, not REML (in practice this means you end up with two
versions of your model).

## Bias in estimates of generalized linear models

**Add example for glm**

## Bias in estimates of non-linear models

Let's do a similar analysis for the log-normal model with exponentially 
decaying `cv` that we fitted in Lab 9 I am going to assume some true parameter
values (similar to what we got after fitting to the data):

```r
# True parameters
a = 0.61
b = 2.1
c = 4.0
d = 0.13
# Use the cleanFIR dataset for DBH
mu = a*cleanFIR$DBH^b
cv = c*exp(-d*cleanFIR$DBH)
meanlog = log(mu/sqrt(1 + cv^2))
sdlog   = sqrt(log(1 + cv^2))
N = nrow(cleanFIR)
Y = rlnorm(N, meanlog, sdlog)
plot(cleanFIR$DBH, Y)
```

Let's simulate 1000 datasets:

```r
library(future.apply)
plan(multisession)
Y = future_sapply(1:1e3, function(x) rlnorm(N, meanlog, sdlog), future.seed=TRUE)
```

:::::: {.callout-important title="Exercise" collapse="true"}

1. Fit the model to each of the simulated datasets (check how you did it 
before). Make sure that the bounds are sufficiently wide so that none of the 
estimates lie on the bound. 

2. Check the bias in the parameter estimates. How does it differ from  the 
linear model?

3. Compare the bias to the mean absolute error of the parameter estimates, how 
relevant is the bias?

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

I first define a version of the NLL function that takes the simulated data as
input.

```r
nll = function(a, b, c, d, Y) {
  mu = a*cleanFIR$DBH^b
  cv = c*exp(-d*cleanFIR$DBH)
  meanlog = log(mu/sqrt(1 + cv^2))
  sdlog   = sqrt(log(1 + cv^2))
  NLL = -sum(dlnorm(Y, meanlog, sdlog, log = TRUE))
  NLL
}
```

I test the function to make sure it all works

```r
par0 = c(a = 0.5, b = 2, c = 3, d = 0.15)
fit = mle2(minuslogl = nll, start = as.list(par0), data = data.frame(Y = Y[,1]),
                method = "L-BFGS-B",
               lower = c(a = 0.01, b = 1, c = 1, d = 0),
               upper = c(a = 4, b = 4, c = 20, d = 1),
               control = list(parscale = abs(par0)))
```

Now that we are certain this is going to work, let's do it 1000 times and extract
the coefficients

```r
coefs = t(future_apply(Y, 2, function(Y) {
    fit = mle2(minuslogl = nll, start = as.list(par0), data = data.frame(Y = Y),
                method = "L-BFGS-B",
               lower = c(a = 0.01, b = 1, c = 1, d = 0),
               upper = c(a = 4, b = 4, c = 20, d = 1),
               control = list(parscale = abs(par0)))
    coef(fit)
}))
```

Now we can check the bias in the estimates (relative to the magnitude of each
parameter)

```r
bias = colMeans(coefs) - c(0.61, 2.1, 4, 0.13)
bias/c(0.61, 2.1, 4, 0.13)
```

We can see that the estimate of `c` and `d`, related to the coefficient of
variation are biased, but so is `a`, which is related to the mean. This differs
from the linear model where the parameters related to the mean were unbiased. In
fact the bias in `c` and `d` are smaller than the bias in `a`. 

Let's compute the MSE and how bias and variance contribute to it

```r
MSE = c(a = mean((coefs[,1] - 0.61)^2),
        b = mean((coefs[,2] - 2.1)^2),
        c = mean((coefs[,3] - 4)^2),
        d = mean((coefs[,4] - 0.13)^2))
vars = c(a = var(coefs[,1]), b = var(coefs[,2]), 
         c = var(coefs[,3]), d = var(coefs[,4]))
cbind(MSE = MSE, bias_MSE = bias^2/MSE, var_MSE = vars/MSE)
```

What this tells us is that the variance in the estimates of all parameters is
so large, that the bias becomes pretty much irrelevant to the overall error. SO,
not only do we have biases in parameters related to mean (unlike in the linear
model) but these biases matter very little given the high variance of the
estimates. For that reason, even trying REML (which is much hard in a non-linear
non-Normal model) its not worth it. 
::::

:::::

::::::



# Reducing variance in estimates: Maximum a posteriori (MAP)

We saw in the previous exercise that using REML we could minimize the bias of
parameter estimates in linear models and reduce significantly the error of 
estimation in parameters related to variance. In the case of non-linear models,
any parameter estimate can be biased but reducing it would not significantly
improve the error of estimation.

Instead, we should focus on reducing the error of estimation directly by reducing
the variance of the estimates at the expense of increase the bias even more. This
is one example of a concept called *bias-variance tradeoff*. Remember that the
mean squared error of an estimator can be decomposed as:

$$
\text{MSE} = \text{bias}^2 + \text{var}
$$

This shows that if your errors of estimation are dominated by the variance 
component, you are better off focusing your efforts in reducing the variance 
rather than the bias. In fact, the techniques to do so often increase the bias
but this is compensated by an even larger decrease in variance. This tradeoff
will show again in the next chapter on multilevel models, but we can also apply
it in models are not multilevel. I would show how the mode of a posterior
distribution (with good priors) achieves this goal of reducing MSE. A similar
approach is used in machine learning (called *regularized regression*)

To calculate the mode of the posterior distribution we can use Laplace's approximation
as described in Lab chapter 6. We are actually going to do it in R this time 
because we can do approximation in R (Stan is really needed for the Hamiltonian
Monte Carlo). I modify the NL function we used before by adding the negative log prior
densities for each parameter:

```r
nlpd = function(a, b, c, d, Y) {
  # This is the same as before
  mu = a*cleanFIR$DBH^b
  cv = c*exp(-d*cleanFIR$DBH)
  meanlog = log(mu/sqrt(1 + cv^2))
  sdlog   = sqrt(log(1 + cv^2))
  NLL = -sum(dlnorm(Y, meanlog, sdlog, log = TRUE))
  # I add negative log prior densities
  NLPD = -dnorm(a, hp[1], hp[2], log = TRUE) -
          dnorm(b, hp[3], hp[4], log = TRUE) -
          dnorm(c, hp[5], hp[6], log = TRUE) -
          dnorm(d, hp[7], hp[8], log = TRUE)
  NLL + NLPD       
}
```
For simplicity I assigned a normal prior to each parameter and I will estimate
these prior models based on the order of magnitude of each parameter (these are
known as *weakly informative priors*):

```r
hp = c(1, 1, # Order of magnitude of a
       2, 2, # Order of magnitude of b
       4, 4, # Order of magnitude of c
       0.2, 0.2 # Order of magnitude of d
)
```

We can now evaluate our model of negative log posterior density:

```r
par0 = c(a = 0.5, b = 2, c = 3, d = 0.15)
fit = mle2(minuslogl = nlpd, start = as.list(par0), data = data.frame(Y = Y[,1]),
                method = "L-BFGS-B",
               lower = c(a = 0.01, b = 1, c = 1, d = 0),
               upper = c(a = 4, b = 4, c = 20, d = 1),
               control = list(parscale = abs(par0)))
summary(fit)
```


:::::: {.callout-important title="Exercise" collapse="true"}

1. Use the procedure described above on all the simulated datasets (`Y`) 

2. Check the bias, variance and MSE of parameter estimates and compare to the
maximum likelihood estimates in the previous exercise.

::::: {.content-hidden unless-meta="show_solution"}

:::: {.callout-tip title="Solution" collapse="true"}

This is the same loop as before with the only modification that we use `nlpd`

```r
coefs_MAP = t(future_apply(Y, 2, function(Y) {
    fit = mle2(minuslogl = nlpd, start = as.list(par0), data = data.frame(Y = Y),
                method = "L-BFGS-B",
               lower = c(a = 0.01, b = 1, c = 1, d = 0),
               upper = c(a = 4, b = 4, c = 20, d = 1),
               control = list(parscale = abs(par0)))
    coef(fit)
}))
```

Now we can check the bias in the estimates and compare them to the previous
exercise

```r
bias_MAP = colMeans(coefs_MAP) - c(0.61, 2.1, 4, 0.13)
cbind(ML = bias, MAP = bias_MAP, MAP_ML = abs(bias_MAP/bias))
```

We can see that the MAP estimates increase the bias only slightly, except for
estimate `b`.

Let's compute the MSE and compare to previous estimates

```r
MSE_MAP = c(a = mean((coefs_MAP[,1] - 0.61)^2),
            b = mean((coefs_MAP[,2] - 2.1)^2),
            c = mean((coefs_MAP[,3] - 4)^2),
            d = mean((coefs_MAP[,4] - 0.13)^2))
cbind(MAP = MSE_MAP, ML = MSE, MAP_ML = MSE_MAP/MSE)

```

We can see reductions in the error of 20% - 38% depending on the parameter. In
the case of parameters `a`, `c` and `d` we managed to reduce the variance without
changing much the bias. In the case of parameter `b` we did increase the bias in
relative terms, but since the contribution of bias to the error was negligible,
we still reduce the variance.
:::::

::::::

# Conclusions

- The error of a point estimate of a parameter is determined by its bias and
variance. There can be a trade-off between the two, such that increase the bias
in an estimate but simultaneously decreasing the variance may lead to a lower
overall error.

- In linear models, estimates of intercepts and slopes are unbiased, estimates of
variance-related parameters are not. Restricted Maximum Likelihood (REML) is a
technique that removes this bias and reduces the error of the estimates. This is
relevant to modelling variance as a function of groups or covariates and also for
multilevel models. The relevant canned methods in packages (`nlme`, `lme4`, etc) 
provide ways to apply REML to these models.

- In non-linear models (and in generlized linear models), any parameter estimate 
may be biased and variance-related parameters do not have to be more biased that 
mean-related parameters. In addition, the bias in the estimate may only be a 
small fraction of the total error. You can apply REML to simple non-linear (mixed) 
models with Normal errors using the package `nlme` or `TMB` (coding in C++). You
can apply REML to `glm` models using `nlme`. If you are dealing with GLMM, you 
can also use `lme4` or `glmmTMB`.

- In all models, the overall error in the parameter estimates may be reduced by
using alternatives to maximum likelihood where the estimates do not only depend 
on the data. This includes mode or mean of posterior distributions with (weakly) 
informative priors and regularized regressions such lasso and ridge. For the
Bayesian estimator you can use `Stan` or the package `brms` or `stanarm` for 
more canned methods.






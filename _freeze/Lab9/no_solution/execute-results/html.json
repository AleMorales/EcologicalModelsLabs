{
  "hash": "cd939fa561105cac7c9399e2c1614b47",
  "result": {
    "markdown": "---\ntitle: \"Lab 9 + GLS and Mixed effect Models\"\nauthor: \"Bob Douma\"\ndate: \"3 December 2018\"\ncache: true\nparams:\n  solution: false\n# output:\n#   pdf_document: default\n#   html_document:\n#     df_print: paged\n---\n\n\n\n\n\n## Learning goals\n\nYou will learn how to:\n\n1. Estimate the parameters of a model with the variance varying as a function of a covariate (`gls`)\n\n2. Apply mixed effect models to data with nested structures\n\n#  Fitting models containing  categorical predictors\n\n# Fitting models with the heterogenous variance\nIt is common to observe that the variance of your data is not constant along your predictor $x$. In case of the Poisson distribution, the relationship between the mean and the variance is fixed through $\\lambda$. For other probability distributions with $\\geq2$ parameters the variance can be modelled (somewhat) independent of the mean. A classic example is the normal distribution with $mu$ equal to the mean of the distribution and $\\sigma$ to the square root of the variance. Traditionally, when the variance increases with $x$ log transformations are applied. However, the variance can also be modelled explicitly. The purpose of this exercise is to give you insight how this can done (by showing a simple but wrong approach), followed by the correct, but canned approach `gls` in `R`.\n\nTo illustrate the principle, we generate data from scratch first with constant variance:\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-1_f74ec431f02dc088f3aea683eedffda4'}\n\n```{.r .cell-code}\nx = seq(1,100,length.out = 100)\ny = 2 + 2 * x + rnorm(100,0,20)\nplot(y~x)\n```\n\n::: {.cell-output-display}\n![](no_solution_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNow we let the variance increase linearly with $x$. Note that the `rnorm` takes the standard deviation, hence the $\\sqrt(20x)$\n\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-2_e4e9de8b7fe59e0190c4e9fc732f821b'}\n\n```{.r .cell-code}\nx = seq(1,100,length.out = 100)\ny = 2 + 2 * x + rnorm(100,0,sqrt(20*x^1))\nplot(y~x)\n```\n\n::: {.cell-output-display}\n![](no_solution_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndat = data.frame(x,y)\n```\n:::\n\n\n1. Estimate the paramaters of the model through applying the Bolker approach. Make two models, one with constant variance and another one with the variance as a function of $x$.\n\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-3_142b5f8ca2c5f53434226c5086fe48bc'}\n\n:::\n\n\n2. Compare the AIC of both models. Which model is preferred?\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-4_8a46756a47a85905bfc780580a43e437'}\n\n:::\n\n\n3. Now we will apply the function `gls` (from the package `nlme`), see below the R-script. Do the values of the `gls.2` correspond to the values that were obtained through the Bolker approach? To understand the differences, look careful at 1) the residual standard error of the gls (how does this compare to the variance?), 2) the equation of `varPower` (see Zuur), 3) and how the data was generated. To fit the models you can type:\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-5_d69f3b16efb90440fb579faee9905228'}\n\n```{.r .cell-code}\nlibrary(nlme)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'nlme' was built under R version 4.3.1\n```\n:::\n\n```{.r .cell-code}\ngls.1 = gls(y~x, data=dat,method=\"ML\")\nsummary(gls.1)\n# to specify the variance as a function of x we can use different functions\n# (see chapter 4 of Zuur for details) or see ?varClasses\ngls.2 = gls(y~x, weights=varPower(form=~x), data=dat,method=\"ML\")\nsummary(gls.2)\nAIC(gls.1,gls.2)\n```\n:::\n\n\n\n4. The variance estimates are biased because they are estimated with _maximum likelihood_ and not _restricted maximum likelihood_ (`method=REML` is the default in `gls`). With restricted maximumlikelihood you account for the fact that you need estimate the mean from the data. Since the esimated mean will be slightly different than the true mean, the estimated variance is also slightly different from the true variance (i.e. $\\sigma^2=\\sum_{i=1}^{N}(y_i-\\mu)^2$). Thus the mean itself is an estimate from the data (i.e. $\\bar{y}$) and not the population parameter itself (i.e. $\\mu$). Thus you should fit models with `REML` when applying AIC. Which model fits the data better?\n\n\n# Fitting models to nested data.\nAn important assumption of the models we have discussed so far (linear models, generalized linear models and generalized least squares models) is that the observations are independent of each other. This assumptions is often violated in ecological data. The solution for the lack of independence is to model the data with so called `mixed effect models`. Mixed effect models contain a fixed part and a random part (see Zuur for more information).\nTo understand how mixed effect models work, we will generate data ourselves and backfit the coefficients with a mixed effect model:\n\n$$Y_{i,j} = \\alpha+\\beta x + b_{i}+\\epsilon_{j}$$\nwith $b_{i} \\sim N(0,\\sigma_{site})$\n\nwith $\\epsilon_{j} \\sim N(0,\\sigma_{res})$\n\n\nWe can simulate data as follows:\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-6_87c868504ca3fbe1f65426799e3dc78d'}\n\n```{.r .cell-code}\nx = runif(200,0,10) # this is the covariate\nsigma_site = 1\nsite = rnorm(10,0,sigma_site) # we construct 10 different sites\nid = as.factor(rep(c(1:10),20)) # site id\nsigma_res = 2\ny = 2+ 2*x + site[id] + rnorm(200,0,sigma_res)\ndat= data.frame(x,y,id)\n\n# let's make a plot\nplot(y ~x,col=id)\n```\n\n::: {.cell-output-display}\n![](no_solution_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNow let's fit a mixed effect model with a random intercept across sites:\n\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-7_041ec6c937d8554400da92e70ce1e1d1'}\n\n```{.r .cell-code}\nlibrary(nlme)\n\nlme.1 = lme(y ~ x, random=~1|id,data=dat)\nsummary(lme.1)\n```\n:::\n\n\n5. Check the coefficients of the mixed effect model object `lme.1`. Do the values of the random effects make sense? And do the values of the fixed  part make sense?\n\nLet's plot the fitted lines for each site:\n\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-8_c8f6d665e2e0a489d211f646a77c2f59'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndat$fit.site = fitted(lme.1,level=1)\ndat$fit.main = fitted(lme.1,level=0)\n\nggplot(data=dat) +\n  geom_point(aes(x=x,y=y,colour=id))+\n  geom_line(aes(x=x,y=fit.site,colour=id))+\n  geom_line(aes(x=x,y=fit.main),size=2)\n```\n:::\n\n\n6. Simulate data when allowing for random slopes across sites. Choose a $\\sigma$ for the slopes of 1. Fit a mixed effects model with simulated data now allowing for random variation across slopes. Check the book of Zuur for the syntax. If you get convergence problems, run `ctrl <- lmeControl(opt='optim')`, and add `control=ctrl` to the lme function, i.e. `lme(y~x..,control=ctrl)`.\n\n::: {.cell hash='no_solution_cache/html/unnamed-chunk-9_848bd438d665c63bd83115cb89e39811'}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}